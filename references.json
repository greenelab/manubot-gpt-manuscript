[
  {
    "id": "bYOaJHMe",
    "URL": "https://arxiv.org/abs/2005.14165",
    "number": "2005.14165",
    "title": "Language Models are Few-Shot Learners",
    "issued": {
      "date-parts": [
        [
          2020,
          7,
          24
        ]
      ]
    },
    "author": [
      {
        "given": "Tom B.",
        "family": "Brown"
      },
      {
        "given": "Benjamin",
        "family": "Mann"
      },
      {
        "given": "Nick",
        "family": "Ryder"
      },
      {
        "given": "Melanie",
        "family": "Subbiah"
      },
      {
        "given": "Jared",
        "family": "Kaplan"
      },
      {
        "given": "Prafulla",
        "family": "Dhariwal"
      },
      {
        "given": "Arvind",
        "family": "Neelakantan"
      },
      {
        "given": "Pranav",
        "family": "Shyam"
      },
      {
        "given": "Girish",
        "family": "Sastry"
      },
      {
        "given": "Amanda",
        "family": "Askell"
      },
      {
        "given": "Sandhini",
        "family": "Agarwal"
      },
      {
        "given": "Ariel",
        "family": "Herbert-Voss"
      },
      {
        "given": "Gretchen",
        "family": "Krueger"
      },
      {
        "given": "Tom",
        "family": "Henighan"
      },
      {
        "given": "Rewon",
        "family": "Child"
      },
      {
        "given": "Aditya",
        "family": "Ramesh"
      },
      {
        "given": "Daniel M.",
        "family": "Ziegler"
      },
      {
        "given": "Jeffrey",
        "family": "Wu"
      },
      {
        "given": "Clemens",
        "family": "Winter"
      },
      {
        "given": "Christopher",
        "family": "Hesse"
      },
      {
        "given": "Mark",
        "family": "Chen"
      },
      {
        "given": "Eric",
        "family": "Sigler"
      },
      {
        "given": "Mateusz",
        "family": "Litwin"
      },
      {
        "given": "Scott",
        "family": "Gray"
      },
      {
        "given": "Benjamin",
        "family": "Chess"
      },
      {
        "given": "Jack",
        "family": "Clark"
      },
      {
        "given": "Christopher",
        "family": "Berner"
      },
      {
        "given": "Sam",
        "family": "McCandlish"
      },
      {
        "given": "Alec",
        "family": "Radford"
      },
      {
        "given": "Ilya",
        "family": "Sutskever"
      },
      {
        "given": "Dario",
        "family": "Amodei"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: arxiv:2005.14165"
  },
  {
    "id": "xq1uEbPa",
    "URL": "https://arxiv.org/abs/2102.02503",
    "number": "2102.02503",
    "title": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models",
    "issued": {
      "date-parts": [
        [
          2021,
          2,
          5
        ]
      ]
    },
    "author": [
      {
        "given": "Alex",
        "family": "Tamkin"
      },
      {
        "given": "Miles",
        "family": "Brundage"
      },
      {
        "given": "Jack",
        "family": "Clark"
      },
      {
        "given": "Deep",
        "family": "Ganguli"
      }
    ],
    "container-title": "arXiv",
    "publisher": "arXiv",
    "type": "report",
    "abstract": "  On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above. ",
    "note": "license: http://arxiv.org/licenses/nonexclusive-distrib/1.0/\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: arxiv:2102.02503"
  },
  {
    "publisher": "Elsevier BV",
    "issue": "8",
    "DOI": "10.1016/s0167-7799(02)01985-6",
    "type": "article-journal",
    "page": "357-358",
    "source": "Crossref",
    "title": "The history of the peer-review process",
    "volume": "20",
    "author": [
      {
        "given": "Ray",
        "family": "Spier"
      }
    ],
    "container-title": "Trends in Biotechnology",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2002,
          8
        ]
      ]
    },
    "URL": "https://doi.org/d26d8b",
    "container-title-short": "Trends in Biotechnology",
    "PMID": "12127284",
    "id": "1HMhNrQq1",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: doi:10.1016/s0167-7799(02)01985-6"
  },
  {
    "publisher": "Springer Science and Business Media LLC",
    "issue": "7694",
    "DOI": "10.1038/d41586-018-02404-4",
    "type": "article-journal",
    "page": "129-130",
    "source": "Crossref",
    "title": "How to write a first-class paper",
    "volume": "555",
    "author": [
      {
        "given": "Virginia",
        "family": "Gewin"
      }
    ],
    "container-title": "Nature",
    "language": "en",
    "issued": {
      "date-parts": [
        [
          2018,
          2,
          28
        ]
      ]
    },
    "URL": "https://doi.org/ggh63n",
    "container-title-short": "Nature",
    "id": "19YWsShi0",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: doi:10.1038/d41586-018-02404-4"
  },
  {
    "publisher": "Public Library of Science (PLoS)",
    "issue": "6",
    "DOI": "10.1371/journal.pcbi.1007128",
    "type": "article-journal",
    "page": "e1007128",
    "source": "Crossref",
    "title": "Open collaborative writing with Manubot",
    "volume": "15",
    "author": [
      {
        "given": "Daniel S.",
        "family": "Himmelstein"
      },
      {
        "given": "Vincent",
        "family": "Rubinetti"
      },
      {
        "given": "David R.",
        "family": "Slochower"
      },
      {
        "given": "Dongbo",
        "family": "Hu"
      },
      {
        "given": "Venkat S.",
        "family": "Malladi"
      },
      {
        "given": "Casey S.",
        "family": "Greene"
      },
      {
        "given": "Anthony",
        "family": "Gitter"
      }
    ],
    "container-title": "PLOS Computational Biology",
    "language": "en",
    "editor": [
      {
        "given": "Dina",
        "family": "Schneidman-Duhovny"
      }
    ],
    "issued": {
      "date-parts": [
        [
          2019,
          6,
          24
        ]
      ]
    },
    "URL": "https://doi.org/c7np",
    "container-title-short": "PLoS Comput Biol",
    "PMCID": "PMC6611653",
    "PMID": "31233491",
    "id": "YuJbg3zO",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: doi:10.1371/journal.pcbi.1007128"
  },
  {
    "id": "F3iZfGUC",
    "type": "book",
    "call-number": "PN4784.T3 K7 1976",
    "edition": "2d ed",
    "event-place": "Metuchen, N.J",
    "ISBN": "9780810808447",
    "number-of-pages": "336",
    "publisher": "Scarecrow Press",
    "publisher-place": "Metuchen, N.J",
    "source": "Library of Congress ISBN",
    "title": "A history of scientific & technical periodicals: the origins and development of the scientific and technical press, 1665-1790",
    "title-short": "A history of scientific & technical periodicals",
    "author": [
      {
        "family": "Kronick",
        "given": "David A."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1976"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: isbn:9780810808447"
  },
  {
    "title": "An Open-Publishing Response to the COVID-19 Infodemic.",
    "container-title": "ArXiv",
    "container-title-short": "ArXiv",
    "ISSN": "2331-8422",
    "issued": {
      "date-parts": [
        [
          2021,
          9,
          17
        ]
      ]
    },
    "author": [
      {
        "given": "Halie M",
        "family": "Rando"
      },
      {
        "given": "Simina M",
        "family": "Boca"
      },
      {
        "given": "Lucy D'Agostino",
        "family": "McGowan"
      },
      {
        "given": "Daniel S",
        "family": "Himmelstein"
      },
      {
        "given": "Michael P",
        "family": "Robson"
      },
      {
        "given": "Vincent",
        "family": "Rubinetti"
      },
      {
        "given": "Ryan",
        "family": "Velazquez"
      },
      {},
      {
        "given": "Casey S",
        "family": "Greene"
      },
      {
        "given": "Anthony",
        "family": "Gitter"
      }
    ],
    "PMID": "34545336",
    "PMCID": "PMC8452106",
    "abstract": "The COVID-19 pandemic catalyzed the rapid dissemination of papers and preprints investigating the disease and its associated virus, SARS-CoV-2. The multifaceted nature of COVID-19 demands a multidisciplinary approach, but the urgency of the crisis combined with the need for social distancing measures present unique challenges to collaborative science. We applied a massive online open publishing approach to this problem using Manubot. Through GitHub, collaborators summarized and critiqued COVID-19 literature, creating a review manuscript. Manubot automatically compiled citation information for referenced preprints, journal publications, websites, and clinical trials. Continuous integration workflows retrieved up-to-date data from online sources nightly, regenerating some of the manuscript's figures and statistics. Manubot rendered the manuscript into PDF, HTML, LaTeX, and DOCX outputs, immediately updating the version available online upon the integration of new content. Through this effort, we organized over 50 scientists from a range of backgrounds who evaluated over 1,500 sources and developed seven literature reviews. While many efforts from the computational community have focused on mining COVID-19 literature, our project illustrates the power of open publishing to organize both technical and non-technical scientists to aggregate and disseminate information in response to an evolving crisis.",
    "URL": "https://www.ncbi.nlm.nih.gov/pubmed/34545336",
    "type": "article-journal",
    "id": "10gsAq0o",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: pubmed:34545336"
  }
]
