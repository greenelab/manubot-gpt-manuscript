## Conclusions

In our study, we integrated AI-based revision models into the Manubot publishing platform.
Writing academic papers can be time-consuming and difficult to comprehend, so we aimed to leverage technology to help researchers convey their findings effectively.
Our AI-based revision process utilizes a prompt generator that creates tailored instructions for the language model based on the manuscript and section.
Authors can activate this workflow from the GitHub repository to suggest revisions that can be reviewed later.
We used GPT-3 models via the OpenAI API in this workflow, with default parameters that work well for various manuscripts and sections.
Users can also personalize the revision process by selecting specific sections or adjusting the model's behavior to their preferences and budget.
While the evaluation of the tool is subjective, we observed that most paragraphs were improved.
The AI model also identified certain paragraphs that were challenging to revise, which could also pose difficulties for human readers.


We created specific prompts for each section of the text to guide revisions using GPT-3.
Interestingly, the model was able to identify an error in a Methods section that had been missed by humans, regarding a symbol in an equation.
However, the abstracts were more difficult for the model to revise, sometimes removing important background information about the research problem.
To improve the AI-based revisions, we suggest refining prompts using few-shot learning or fine-tuning the model with an additional corpus of academic writing focused on challenging sections.
Fine-tuning with preprint-publication pairs may also help identify sections or phrases likely to be changed during peer review.
While our approach used GPT-3 to process each paragraph, it lacked a contextual thread between queries, which mainly affected the Results and Methods sections.
Using chatbots that retain context, such as OpenAI's ChatGPT, could enable the revision of individual paragraphs while considering previously processed text.
We plan to update our workflow to support this strategy since an official ChatGPT API recently became available.
Other open models, such as BLOOM, GLM, or OPT, provide similar capabilities but lack the user-friendly OpenAI API.
Despite some limitations, we found that the models were able to capture the main ideas and generate a revision that often communicated the intended meaning more clearly and concisely.
However, it is important to note that our assessment of performance in case studies was subjective, as there may be writing styles that are not widely shared among researchers.


The use of AI tools to assist scientific writing is a topic of debate.
Some are concerned about the originality and ownership of texts generated by these models.
For instance, *Nature* requires documentation for any use of these models in scientific writing.
The International Conference on Machine Learning (ICML) has also banned the submission of papers containing text generated from a large-scale language model (LLM), although editing tools for grammar and spelling correction are allowed.
Our work focuses on revising *existing* text written by a human author, similar to other tools like Grammarly.
We track all changes made by humans and AI in the version control system for transparency.
Despite the concerns, our work provides a foundation for a future where humans and machines collaborate to create academic manuscripts.
Writing scientific articles can be time-consuming and require significant effort to communicate findings effectively.
As machines improve scholarly text, humans can focus more on what to communicate rather than how to write it.
This could lead to a more productive and equitable future for research, where scientists can focus on ideas and experiments to uncover the underlying principles of ourselves and our environment.
