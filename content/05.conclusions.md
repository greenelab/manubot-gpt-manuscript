# Conclusions

We implemented AI-based models into publishing infrastructure.
While most manuscripts have been written by humans, the process is time consuming and academic writing can be difficult to parse.
We sought to develop a technology that academics could use to make their writing more understandable without changing the fundamental meaning.
This work lays the foundation for a future where academic manuscripts are constructed by a process that incorporates both human and machine authors.


We found that, in general, the tool generated reasonable suggestions for most sections of the manuscripts, with paragraph that were greatly improved and others that needed minimal human review.
We also found, though, that for some paragraph the revision was not good enough, and in some cases it even made the text worse.
This was particularly true for short paragraphs and some in the the Methods sections when mathematical expressions were present.

Abstracts:

  - remember that the models are stochastic (meaning that we can always run it again).
  - for the phenoplier abstract, the revision was not really great (the AI model removed too much background information).
    - the user of the tool, in these cases, might want to run it again to obtain another revision.
    - for abstract, maybe we need to improve the prompts to say something like "keep more background information".


Introductions:

  - really good for both manuscripts
  - in phenoplier, the models did not keep the format of citations for one paragraph, although this can be easily fixed by a human.
    - the other paragraph that did not converge was a large one; this could be an indication that is not easy to understand and it should be split into smaller paragraphs.
    - therefore, the fact that the tool cannot process a paragraph of text can also provide insight about the quality of the writing.
    - From Casey: Paragraphs that AI-based models struggle to revise may also reveal paragraphs that would challenge human readers.


Methods:
  - the model not only improved the text, but also fixed references to wrong mathematical symbols.


One way to fix the problem of context is to use chatbots like ChatGPT, which could enable the same paragraph-by-paragraph approach to revision that we presented here, but with higher quality revisions by considering the context (just as if it were reading it).
  - This approach (chatGPT, etc) has also the advantage of being independent of the context length of the models.
  - Here, mention this kind of models in general again, besides ChatGPT, like AnthropicAI's Claude model (a competitor of ChatGPT): https://twitter.com/goodside/status/1611556749726605312


Edits vs completion endpoints:
  - although they are based on the same Davinci models, the completion endpoint was better
  - however, edits is still in beta, and might the preferred endpoint in the future (however, the best one would be the chatbot interface when available)
  - Curie models were clearly inferior, but this could also be explained by the complexity of our prompts; maybe simplier prompts would work better.
  - We show that the Davinci models are capable of revising a complex manuscript, and that any potential issue could be more related to the complexity of the prompts than to the models themselves (prompt design).


- the models offers a wide variety of parameters to tune, for instance, "logit_bias" could be helpful to improve completion in the methods section (equations), which allows to provide a bias for specific tokens (maybe if we add "$$", the model would deal better with equations).
- other models in the future will certainly improve the AI-assisted revision of scientific manuscripts
- another alternative is fine-tuning of models: https://beta.openai.com/docs/guides/fine-tuning
    - maybe using biorxiv or another corpus to train on specific sections of papers

- although Manubot manuscript are written in Markdown format, our prompts are independent of this markup language, and should work with other tools like Latex.


About the stochastic nature of the models:
  - the stochastic nature of the models mimic a human author
  - humans do not revise a text always in the same way
