# Conclusions

We implemented AI-based revision models into the publishing infrastructure provided by Manubot.
While most manuscripts have been written by humans, the process is time consuming and academic writing can be difficult to parse.
We sought to develop a technology that academics could use to make their writing more understandable without changing the fundamental meaning.
For this, we implemented a workflow in GitHub Actions that can be manually triggered by human authors to revise a manuscript.
The tool employs the OpenAI API for access to the GPT-3 models used for the revision, and when done, it generates a pull request that can be reviewed by authors before merging into the manuscript.
We provide default parameters for GPT-3 models that we found to work well for our use cases across different sections and manuscripts.
The tool also provides users with different ways to customize the revision, from selecting specific sections to adjusting the model's behavior to fit their needs and budget.
We found that, in general, the tool generated acceptable suggestions for most sections of the manuscripts, with most paragraphs being significantly improved and others that needed minimal human review.
Even parts of the text that the tool struggle to revise proved to be useful for the authors to identify paragraphs that would challenge human readers.


We designed section-specific prompts to guide the AI-based revision of the text in a way that is consistent with the section.
To test this, we performed case studies with three manuscripts across all the standard sections of an article in the life sciences.
These sections represented a wide range of complexity levels for a revision task: from abstracts that need to briefly describe the research questions and proposed solutions, to the Methods section that focuses in very specific technical details, sometimes needing a particular format (such as equations).
Overall, the tool provided very good suggestions that significantly improved the sentence structure.
The models were able to capture the main ideas in the original text and generate a revision that was easier to read and follow, yet communicating the original meaning more clearly and concisely.
In paragraphs from the Methods section, the model even understood the mathematical symbols in an equation and fixed references to them.
We found that the most challenging section to revise was the abstract, which basically needs to describe the state-of-the-art in a field, the importance of the research questions, and briefly communicate how the proposed methods or analyses address them.
This could be addressed either by designing better prompts (using a "few-shot learning" approach [@doi:10.1145/3386252]), which would be the fastest way, or by fine-tuning the pre-trained models using a large corpus of abstracts, which would potentially produce higher-quality results.
We believe that the AI-based revision tool, although with some errors and minor issues, performed very well on the rest of the sections.
As showed in the use cases, the improvements suggested by the model surpassed the mistakes that it eventually made, which can be easily fixed by a human after reviewing the suggestions.
It is important to note, however, that our assessment of the quality of the revisions was necessarily subjective, as there could be, for example, writing styles that might not be widely shared across researchers.
We tried to overcome this by selecting manuscripts of our own to more objectively assess the revisions, but we acknowledge that this is an important limitation of our work.
Only time and a more extensive use of the tool will tell if an automated, AI-based revision approach for scientific literature will become a standard practice in the future.


Our AI-based tool uses a paragraph-by-paragraph approach to revision, meaning that the model is applied to each paragraph of the manuscript *independently*, without considering the context of the paragraph.
In addition to producing acceptable results, this divide-and-conquer approach has advantages, such as the ability to process long texts without strongly depending context length of the models.
It was clear from our results, however, that depriving the tool from processing the context of the paragraph had some drawbacks.
This was especially true in the Results and Methods sections, where the tool often added details that were present later in the text, or explained again an acronym that was previously defined.
One way to address this issue would be to use chatbots, such as OpenAI's ChatGPT [@url:https://openai.com/blog/chatgpt/], that could process the entire manuscript using the same paragraph-by-paragraph approach while still considering the previously processed text.
This would mimic the way a human reads a manuscript, inherit the benefits of processing one paragraph at a time, and potentially produce higher-quality revisions.


In this preliminary work we focused on OpenAI's models mainly because of the simplicy in accessing the API.
However, our tool could be adapted to others models, such as BLOOM [@arxiv:2211.05100], GLM [@arxiv:2210.02414], or Meta's OPT [@arxiv:2205.01068], which are open.
Among all models offered by OpenAI, we found that only Davinci, the most advanced and capable model, was able to successfully revise a scholarly manuscript.
These GPT-3 models offer a variety of parameters that can be used to control their behavior.
For example, the models are stochastic, meaning that different revisions can be generated from the same input paragraph.
This is a desirable property for our use case, as it allows the authors to generate multiple revisions of the same section and then choose the prefered one.
Our tool, however, allows to easily control this randomness of the model by adjusting parameters such as the sampling temperature or use other techniques such as top-p (nucleus) sampling.
Other parameters could be useful in improving the model outcomes, such as the `logit_bias` parameter, which allows to change the likelihood of specific tokens for a completion.
An example would be to ban of the use of tokens related to references to figures or tables when we know these are not present in the paragraph to be revised.
Another option in improving the revision outcomes would be to fine-tune the models using a specific corpus of text.
This would allow to train even section-specific models that could be used to improve the revision of a particular section without the need to design a complex prompt (such as our prompt for the Methods section).


This work lays the foundation for a future where academic manuscripts are constructed by a process that incorporates both human and machine authors.
We believe that our AI-based revision tool, based on large language models, is a step towards a more efficient and effective way of writing scientific manuscripts.
We acknowledge that using these models in scientific authoring is controversial.
Several questions arise concerning the novelty or ownership of the text generated by these models.
Based on these grounded concerns, for example, the program chairs of the upcoming International Conference on Machine Learning (ICML) released a statement prohibiting *"papers that include text generated from a large-scale language model (LLM)"* [@url:https://icml.cc/Conferences/2023/llm-policy], while editing tools such as for grammar and spelling correction are allowed.
Indeed, our work focuses on the revision of an *existing* text written by a human author, and in this way it is not different from other automatic tools such as Grammarly [@url:https://www.grammarly.com/], which is already widely used by the scientific community.
In our view, our AI-based revision tool is a step towards a more efficient and effective way of writing scientific manuscripts.
The writing process is a time-consuming task that requires a significant amount of effort in thinking *how* to communicate a result or finding that was already obtained.
The increasing ability of machines to understand and improve a scholarly text means that humans can focus on *what* to communicate to other peers.
We envision a future where AI-assisted academic authoring will improve not only the quality of scholarly writings, but also the research itself by allowing scientists to center on the most fascinating and challenging aspects of their work.
