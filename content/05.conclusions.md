## Conclusions

Our tool, the Manubot AI Editor, implements AI-based revision models into the Manubot publishing platform.
Writing academic papers can be time-consuming and challenging to read, so we sought to use technology to help researchers communicate their findings to the community.
Our AI-based revision workflow uses a prompt generator that creates manuscript- and section-specific instructions for the language model.
Authors can easily trigger this workflow from the GitHub repository to suggest revisions that can be later reviewed.
This workflow uses GPT-3 models through the OpenAI API, generating a pull request of revisions that authors can review.
We set default parameters for GPT-3 models that work well for our use cases across different sections and manuscripts.
Users can also customize the revision by selecting specific sections, adjusting the model's behavior to fit their needs and budget, and even providing custom prompts instead of using the default, section-specific ones, which can be useful for specific use cases that do not require a complex revision.
Although the evaluation of automatic text revision is challenging, we found that most paragraphs were improved, while in some cases the model removed important information or introduced errors.
The AI model also highlighted certain paragraphs that were difficult to revise, which could be challenging for human readers too.


We designed section-specific prompts to guide the revision of text using GPT-3.
Surprisingly, in one Methods section, the model detected an error when referencing a symbol in an equation that had been overlooked by humans.
However, abstracts were more challenging for the model to revise, where revisions often removed background information about the research problem.
There are opportunities to improve the AI-based revisions, such as further refining prompts using few-shot learning [@doi:10.1145/3386252] or fine-tuning the model using an additional corpus of academic writing focused on particularly challenging sections.
Fine-tuning using preprint-publication pairs [@doi:10.1371/journal.pbio.3001470] may help to identify sections or phrases likely to be changed during peer review.
Our approach used GPT-3 to process each paragraph of the text, but it lacked a contextual thread between queries, which mainly affected the Results and Methods sections.
Using chatbots that retain context, such as [OpenAI's ChatGPT](https://openai.com/blog/chatgpt), could enable the revision of individual paragraphs while considering previously processed text, and we plan to update our workflow to support this strategy.
Open models, such as BLOOM [@arxiv:2211.05100], Meta's Llama 2 [@arxiv:2307.09288], or Mistral 7B [@arxiv:2310.06825], are growing in popularity and capabilities, but lack the user-friendly OpenAI API.
We used a combination of human evaluation and automated tools available at the time to assess the outcomes of the AI-based revisions.
Recent frameworks such as [OpenAI Evals](https://github.com/openai/evals) or strategies such as LLM-as-a-Judge [@arxiv:2306.05685] could be used to evaluate the quality of the revisions in a more automated way.
Despite these limitations, we found that models captured the main ideas and generated a revision that often communicated the intended meaning more clearly and concisely.
While our study used OpenAI's GPT-3, the Manubot AI Editor supports both GPT 3.5 Turbo and GPT-4 models, which were made available after the completion of our research.


The use of AI-assisted tools for scientific authoring is controversial [@doi:10.1038/d41586-023-00056-7; @doi:10.1038/d41586-023-00107-z].
Questions arise concerning the originality and ownership of texts generated by these models.
For example, the *Nature* journal has established that any use of these models in scientific writing must be documented [@doi:10.1038/d41586-023-00191-1], and the International Conference on Machine Learning (ICML) has prohibited the submission of *"papers that include text generated from a large-scale language model (LLM)"* [@url:https://icml.cc/Conferences/2023/llm-policy], although editing tools for grammar and spelling correction are allowed.
Our work, however, focuses on revising *existing* text written by a human author.
Additionally, all changes made by humans and AI are tracked in the version control system, which allows for full transparency.
Despite the concerns, there are also significant opportunities.
Our work lays the foundation for a future in which humans and machines construct academic manuscripts.
Scientific articles need to adhere to a certain style, which can make the writing time-consuming and require a significant amount of effort to think about *how* to communicate a result or finding that has already been obtained.
As machines become increasingly capable of improving scholarly text, humans can focus more on *what* to communicate to others, rather than on *how* to write it.
This could lead to a more equitable and productive future for research, where scientists are only limited by their ideas and ability to conduct experiments to uncover the underlying organizing principles of ourselves and our environment.
