# Conclusions

We implemented AI-based models into publishing infrastructure.
While most manuscripts have been written by humans, the process is time consuming and academic writing can be difficult to parse.
We sought to develop a technology that academics could use to make their writing more understandable without changing the fundamental meaning.
This work lays the foundation for a future where academic manuscripts are constructed by a process that incorporates both human and machine authors.


SUMMARY BETWEEN MODELS:
- The edits endpoint, however, provides the most natural interface for the revision of manuscripts, and since it's based on the Davinci models, we believe that it might become the best option for this task in the future.
- Curie models were not very good, but this could also be explained by the complexity of our prompts; maybe simplier prompts would work better.

- WE SHOW that the Davinci models are capable of revising a complex manuscript, and that any potential issue could be more related to the complexity of the prompts than to the models themselves.

- the models offers a wide variety of parameters to tune, for instance, "logit_bias" could be helpful to improve completion in the methods section (equations), which allows to provide a bias for specific tokens
- other models in the future will certainly improve the AI-assisted revision of scientific manuscripts


- read the "best practices" part, which has suggestions to improve, like using "best_of":
https://beta.openai.com/docs/guides/completion/inserting-text
- another improvement: check "finish_reason" to make sure it is "stop" and not "lenght" (which means max_tokens was too small)

- although Manubot manuscript are written in Markdown format, our prompts are independent of this markup language, and should work with other tools like Latex.

- another alternative is fine-tuning of models: https://beta.openai.com/docs/guides/fine-tuning
    - maybe using biorxiv or another corpus to train on specific sections of papers