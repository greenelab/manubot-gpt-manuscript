# Methods

We implemented an AI-assisted revision infrastructure in Manubot [@doi:10.1371/journal.pcbi.1007128].
Manubot takes Markdown as input and produces HTML, PDF, or other pandoc-supported formats as output.
It includes a cite-by-persistent-identifier infrastructure to ensure the accuracy of citations.
Its workflows are implemented in continuous integration software (Appveyor, GitHub Actions, etc).
These workflows run automatically with each commit, ensuring the integrity and accuracy of the revisions.

We used the OpenAI API to access large language models, focusing on completion endpoints.
This API incurs a cost that depends on the length of the manuscript.
To reduce costs, we implemented our workflow in GitHub actions so that users can trigger it.
They can also select the model of their choice, allowing them to adjust the cost.
For example, with the most complex model, `text-davinci-003`, the cost per run is less than $0.50 for many manuscripts.

When the user triggers the action, our workflow parses the manuscript by section and paragraph.
Then, it passes the text to the model along with custom prompts.
After receiving the output, the workflow reformats it and generates a new pull request via the GitHub API.
This allows the user to review and modify the output before merging, if desired.
This workflow also allows text to be attributed either to the initial user or to the language model.
This may be important for legal reasons, in case future decisions change the copyright landscape around the outputs of generative models [@doi:10.1093/oso/9780198850448.001.0001].
