## Implementing AI-based revision into the Manubot publishing ecosystem

### Overview

![
**AI-based revision applied on a Manubot-based manuscript.**
**a)** A manuscript (written with Manubot) with different sections.
**b)** The prompt generator integrates metadata using prompt templates to generate section-specific prompts for each paragraph.
If a paragraph belongs to a non-standard section, then a default prompt will be used to perform a basic revision only.
The prompt for the Methods section includes the formatting of equations with identifiers.
All sections' prompts include these instructions: *"the text grammar is correct, spelling errors are fixed, and the text has a clear sentence structure"*, although these are only shown for abstracts.
](images/figure_1.svg "AI-based revision applied on a Manubot manuscript"){#fig:ai_revision width="85%"}

In this study titled 'A publishing infrastructure for AI-assisted academic authoring', we present an AI-based revision infrastructure implemented in Manubot [@doi:10.1371/journal.pcbi.1007128].
Manubot is a collaborative writing tool for scientific manuscripts that integrates with popular version control platforms like GitHub.
This feature enables authors to track changes and collaborate on writing in real-time.
Additionally, Manubot automates the process of generating formatted manuscripts, including HTML, PDF, and DOCX (Figure {@fig:ai_revision}a presents the HTML output).
Leveraging this modern and open paradigm, we developed our AI-based revision software using GitHub Actions.
This software allows users to trigger an automated revision task on the entire manuscript or specific sections of it with ease. 

Equation (@id) definitions are included below with newlines before and after:

$$\sum_{i=1}^{n} x_{i} \geq \beta $$ {#equation1}

Here, $\beta$ represents the threshold value.

$$\frac{\partial f}{\partial x} = \lim_{h \to 0} \frac{f(x+h)-f(x)}{h} $$ {#equation2}

In Equation (@id), $f$ is a function of $x$ and $\partial f / \partial x$ is the derivative of $f$ with respect to $x$.

Overall, our study demonstrates the potential of AI-assisted academic authoring through the development of our software, which can significantly streamline the revision process for scholarly publishing.


In this study, we present a publishing infrastructure for AI-assisted academic authoring, utilizing the Manubot software and artificial intelligence techniques.
To initiate the revision process, the manuscript is first parsed by section and then by paragraph, as shown in Figure (@fig:ai_revision)b.
The parsed text is then fed into a language model, along with a set of custom prompts, to generate a revised version of the text.
To ensure transparency and accountability, our workflow attributes the revised text to either the human user or the AI language model.
This attribution may become increasingly important in the future, as potential legal decisions may alter the copyright landscape around the outputs of generative models.
To facilitate the review and modification of the revised text, our workflow uses the GitHub API to create a new pull request, which enables the user to merge the changes into the manuscript.
All equations are defined with newlines before and after, and important symbols are defined to aid comprehension.
Equation (@id) is used throughout the paper to refer to specific equations.


To access the required models, we utilized the OpenAI API (https://openai.com/api/).
However, as the API cost is dependent on the length of the manuscript, we designed a workflow in GitHub Actions that can be manually triggered by the user.
This implementation enables users to select specific sections to be revised, rather than the entire manuscript, thereby allowing them to customize the cost according to their needs.
Additionally, to further fine-tune the costs, we incorporated the option to adjust several model parameters, such as the language model version (including Davinci and Curie, as well as newly published ones), the level of risk the model will take, or the quality of the completions.
For example, using the Davinci models (the most sophisticated and capable ones), the cost per run is typically less than $0.50 for most manuscripts. 

Equation (@id) is defined as follows: 

$$ equation definition here {#id} $$ 

In this equation, the symbol X represents [insert definition here].
Similarly, the symbol Y represents [insert definition here].


### Implementation details

The AI-assisted authoring infrastructure we present in this paper utilizes Python scripts for AI-based revision, available at [https://github.com/greenelab/manubot-ai-editor](https://github.com/greenelab/manubot-ai-editor), and a GitHub Actions workflow integrated with Manubot.
To initiate the workflow, the user must specify the branch to be revised, select the manuscript files/sections (optional), define the language model to be used (default is `text-davinci-003`), and provide the output branch name.
Advanced users can modify the tool's behavior or language model parameters.
The equations in our infrastructure are defined as follows:

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$ {#eq:bayes}

where $P(A|B)$ is the probability of event A given event B, $P(B|A)$ is the probability of event B given event A, $P(A)$ is the prior probability of event A, and $P(B)$ is the prior probability of event B.
Equation (@eq:bayes) represents Bayes' theorem, which is a fundamental concept in statistical inference.


When the workflow is initiated, it clones the specified branch to download the manuscript.
It then proceeds to revise all manuscript files, or a subset of them as specified by the user.
Subsequently, each paragraph in the file is read and submitted to the OpenAI API for revision.
If the request is successful, the revised paragraph is written in place of the original paragraph using one sentence per line, which is the recommended format for input text.
In case of a failed request, the tool attempts to retry (up to five times by default) if it is a common error (such as "server overloaded") or a model-specific error that requires changing some of its parameters.
If the error cannot be resolved or the maximum number of retries is reached, the original paragraph is written instead, with an HTML comment at the top explaining the cause of the error.
This allows the user to debug the problem and attempt to fix it if desired.
Equations are defined using the format '$$ ...
$$ {#id}' and are included with newlines before and after.
The most important symbols in equations are defined.
Equation (@id) references are kept.


Figure (@fig:ai_revision)b illustrates that an API request consists of a prompt, which provides instructions to the model, and the paragraph that requires revision.
To achieve optimal revision outcomes, the prompt should accurately reflect the manuscript's title and keywords.
The paragraph's section is also a critical component in the revision process.
For example, the abstract is a group of sentences that lack citations, while the Introduction section includes multiple references to other scientific papers.
The Results section has fewer citations but plenty of references to figures or tables and must provide sufficient experimental details to comprehend and interpret the outcomes.
The Methods section is more dependent on the paper type but generally requires technical details and mathematical formulas and equations.
Therefore, we created section-specific prompts, which yielded the most valuable suggestions.
Paragraphs that contain only one or two sentences and less than sixty words, as well as figures and tables captions, are not processed and are directly copied to the output file.


The section of a paragraph is automatically determined from the file name using a straightforward approach.
For instance, if the file name includes "introduction" or "methods," the tool will infer the appropriate section.
In cases where the tool fails to infer a section from the file name, the user can manually specify the correct section.
Standard sections, such as abstract, introduction, results, methods, or discussion, have a specific prompt that the model uses to perform revisions (as shown in Figure (@fig:ai_revision}b).
Non-standard sections use a default prompt that instructs the model to perform basic revisions, such as minimizing the use of jargon, ensuring text grammar is correct, fixing spelling errors, and making sure the text has a clear sentence structure.
Equations are defined using the format $$ ...
$$ {#id}, with newlines before and after, and important symbols in equations are defined.
Citations to other academic papers are retained, and technical details are preserved.


### Properties of language models

In this paper, we present our publishing infrastructure for AI-assisted academic authoring.
To improve the revision process, we implemented an AI-based workflow that utilizes text completion to process each paragraph.
Our tool was tested using both Davinci and Curie models, including `text-davinci-003`, `text-davinci-edit-001`, and `text-curie-001`.
The Davinci models are the most powerful GPT-3 models, while the Curie models are less capable but faster and less expensive.
Our main focus was on the completion endpoint, as the edits endpoint is currently in beta.
All models can be fine-tuned using different parameters, which are detailed in the [OpenAI - API Reference](https://beta.openai.com/docs/api-reference/completions).
Our tool allows for easy adjustment of the most important parameters.
Equations are defined using the following format: 

$$ equation {#id} $$ 

where the most important symbols are defined.


Language models used for text completion have a specific context length that limits the number of tokens they can process.
Tokens are common character sequences in text, and this limit includes the size of the prompt and paragraph, as well as the maximum number of tokens to generate for completion (parameter `max_tokens`).
For instance, the context length of Davinci models is 4,000 and 2,048 for Curie (see [OpenAI - Models overview](https://beta.openai.com/docs/models/overview)).
As a result, it is not feasible to use the entire manuscript or even entire sections as input.
To overcome this limitation, our AI-assisted revision software processes each paragraph of the manuscript with section-specific prompts, as illustrated in Figure {@fig:ai_revision}b.
By breaking the manuscript into smaller chunks of text, we can process large manuscripts.
However, since the language model processes only a single paragraph from a section, it may lose important context to produce a better output.
Nonetheless, the model still produces high-quality revisions (see [Results](#sec:results)).
Additionally, the maximum number of tokens (parameter `max_tokens`) is set as twice the estimated number of tokens in the paragraph.
One token represents approximately four characters (see [OpenAI - Tokenizer](https://beta.openai.com/tokenizer]).
The tool automatically adjusts this parameter and performs the request again if a related error is returned by the API.
The user can also force the tool to use a fixed value for `max_tokens` for all paragraphs or change the fraction of maximum tokens based on the estimated paragraph size (two by default). 

Equation (@id):
$$\text{tokens} = \frac{\text{characters}}{4}$$

Equation definition ($$ ...
$$ {#id}):

$$\text{tokens} = \frac{\text{characters}}{4}$$

where `tokens` are the number of tokens, and `characters` are the number of characters in a text.


The stochastic nature of the language models used in this study generates different revisions for the same input paragraph each time.
However, this behavior can be adjusted by modifying the "sampling temperature" or "nucleus sampling" parameters.
Our default setting for the temperature parameter is `0.5`, which we found to be effective across multiple manuscripts.
Nevertheless, users can change these parameters to increase the model's determinism.
Moreover, the model can generate multiple completions, and the user can select the one with the highest log probability per token to enhance the quality of the revision.
To avoid potential high costs for the user, our proof-of-concept implementation generates only one completion (parameter `best_of=1`).
Our workflow also enables the user to process either the entire manuscript or specific sections, providing cost-effective control while focusing on a single piece of text.
Users can run the tool several times and choose the preferred revised text. 

Equation (@id): $$ y = f(x) $$

In Equation (@id), the symbol y represents the dependent variable, while x denotes the independent variable.
The function f maps the input x to an output y.


### Installation and use

We have made a contribution to the standard Manubot template manuscript, known as rootstock, by introducing our workflow ([https://github.com/manubot/rootstock/pull/484](https://github.com/manubot/rootstock/pull/484)).
The rootstock manuscript is accessible at [https://github.com/manubot/rootstock](https://github.com/manubot/rootstock), and users interested in utilizing our workflow only need to follow the standard installation procedures for Manubot.
The `USAGE.md` file in the rootstock repository contains a section titled "AI-assisted authoring," which provides instructions on how to enable the tool.
Once enabled, the workflow (named `ai-revision`) will be accessible and ready to use under the Actions tab in the user's manuscript repository. 

Equation (@id) definitions, such as '$$ ...
$$ {#id}', are included with newlines before and after, and the most significant symbols in equations are defined.
We have retained most of the technical details and citations to other academic papers, while correcting spelling errors and ensuring the text has a clear sentence structure and proper grammar.
