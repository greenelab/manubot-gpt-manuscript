## Observations of AI-based revisions {#sec:results}

### Evaluation setup

We evaluated our AI-assisted revision workflow using three GPT-3 models from OpenAI: `text-davinci-003`, `text-davinci-edit-001`, and `text-curie-001`.
`Text-davinci-003` is a production-ready model for the completion endpoint, while `text-davinci-edit-001`, still in beta, provides a more natural interface for revising manuscripts by taking two inputs: instructions and the text to revise.
`Text-curie-001` is faster and cheaper than Davinci models, and is defined as "very capable" by its authors (see [OpenAI - GPT-3 models](https://beta.openai.com/docs/models/gpt-3)).


| Manuscript ID    | Title | Keywords |
|:-------|:----------------------|:----------|
| [CCC](https://github.com/greenelab/ccc-manuscript) | An efficient not-only-linear correlation coefficient based on machine learning                                   | correlation coefficient, nonlinear relationships, gene expression |
| [PhenoPLIER](https://github.com/greenelab/phenoplier_manuscript) | Projecting genetic associations through gene expression patterns highlights disease etiology and drug mechanisms | genetic studies, functional genomics, gene co-expression, therapeutic targets, drug repurposing, clustering of complex traits |
| [Manubot-AI](https://github.com/greenelab/manubot-gpt-manuscript) | A publishing infrastructure for AI-assisted academic authoring | manubot, artificial intelligence, scholarly publishing, software |

Table: **Manuscripts used to evaluate the AI-based revision workflow.** The title and keywords of a manuscript are used in prompts for revising paragraphs. IDs are used in the text to refer to them, and they link to their GitHub repositories. {#tbl:manuscripts}


We assessed the performance of our automated revision tool using three manuscripts of our own authorship (Table @tbl:manuscripts): Clustermatch Correlation Coefficient (CCC) [@doi:10.1101/2022.06.15.496326], PhenoPLIER [@doi:10.1101/2021.07.05.450786] and Manubot-AI (this manuscript).
CCC is a new correlation coefficient evaluated in transcriptomic data, while PhenoPLIER is a framework comprising three different methods applied in the field of genetic studies.
CCC is in the field of computational biology, and PhenoPLIER is in the field of genomic medicine.
CCC is simpler, describing one method applied to one data type (correlation to gene expression).
PhenoPLIER is more complex, with more figures and tables and a Methods section including equations.
Manubot-AI provides an example with a simpler structure and was written and revised using our tool before submission, providing a real AI-based revision use case.
We tested and improved our prompts using these manuscripts, and our findings are reported below.


We enabled the Manubot AI revision workflow in the GitHub repositories of three manuscripts (CCC: `https://github.com/greenelab/ccc-manuscript`, PhenoPLIER: `https://github.com/greenelab/phenoplier_manuscript`, Manubot-AI: `https://github.com/greenelab/manubot-gpt-manuscript`).
We triggered the workflow manually and used three language models to produce one pull request (PR) per manuscript and model.
These PRs, titled *"GPT (MODEL) used to revise manuscript"* with *MODEL* being the model's identifier, are accessible from the repositories' "Pull requests" tab and show the differences between the original text and the AI-based revision suggestions.
Our findings based on these PRs across different sections of the manuscripts are discussed below.


### Performance of language models

We found that Davinci models outperformed the Curie model across all manuscripts.
The Curie model is faster and less expensive than Davinci models.
However, the PRs show that the model was not able to produce acceptable revisions for any of the manuscripts.
Most of its suggestions were not coherent with the original text in any of the sections.


Table 1 shows the scores for each model.

We found that the `text-davinci-edit-001` (edits endpoint) model produced revisions of inferior quality compared to `text-davinci-003` (completion endpoint).
For example, in paragraphs from the introduction, it failed to keep references to other scientific articles in CCC and in PhenoPLIER it did not produce a meaningful revision.
This may be due to the edits endpoint still being in beta.
Table 1 shows the scores for each model.


The `text-davinci-003` model produced the best results for all manuscripts and across the different sections.
Since both `text-davinci-003` and `text-davinci-edit-001` are based on the same models, we only report the results of `text-davinci-003` below.


### Revision of different sections

We inspected the PRs generated by the AI-based workflow and found interesting changes suggested by the tool across different sections of the manuscripts.
Our subjective assessments of the quality of the revisions are presented in the manuscripts' GitHub repositories and Supplementary Files 1 (CCC), 2 (PhenoPLIER) and 3 (Manubot-AI).
We encourage readers to inspect the PRs to see the full diffs and draw their own conclusions.


We present the differences between the original text and the revisions by the tool in a `diff` format (obtained from GitHub).
Line numbers are included to show the length differences.
Single words are highlighted in colors to more clearly see the differences within a single sentence.
Red indicates words removed, green indicates words added, and no highlight indicates words kept unchanged.
To view the full diffs, inspect the Pull Requests for each manuscript and model, and then click on the "Files changed" tab.


#### Abstract

![
**Abstract of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/abstract/ccc-abstract.svg "Diffs - CCC abstract"){#fig:abstract:ccc width="100%"}

We applied the AI-based revision workflow to the CCC abstract (Figure 1).
The tool completely rewrote the text, leaving only the last sentence mostly unchanged.
The text was significantly shortened, with longer sentences than the original ones.
The revision removed the first two sentences and details about the method, and focused on the aims and results obtained.
The last sentence suggested a broader application of the coefficient to other data domains.
The main concepts were still present in the revised text.


The revised text for the abstract of PhenoPLIER was significantly shortened (from 10 sentences in the original, to only 3 in the revised version).
However, in this case, important concepts (such as GWAS, TWAS, CRISPR) and a proper amount of background information were missing, producing a less informative abstract.


#### Introduction

![
**First paragraph in the Introduction section of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/introduction/ccc-paragraph-01.svg "Diffs - CCC introduction paragraph 01"){#fig:intro:ccc width="100%"}

The tool significantly revised the Introduction section of CCC (Figure 1), producing a more concise and clear introductory paragraph.
It concisely incorporated ideas from the original two sentences, introducing the concept of "large datasets" and the opportunities for scientific exploration.
It also generated a more concise second sentence introducing the "need for efficient tools" to find "multiple relationships" in these datasets.
The third sentence connected nicely with the previous one.
References to scientific literature were kept in the Manubot format, and the rest of the sentences were correctly revised, requiring only minor changes for incorporation into the manuscript.


We observed a high quality revision of the introduction of PhenoPLIER.
However, the model failed to maintain the formatting of citations in one paragraph.
The model also did not converge to a revised text for the last paragraph, and our tool returned an error message as an HTML comment at the top: `The AI model returned an empty string`.
Debugging the prompts revealed this issue, which may be due to the complexity of the paragraph.
Rerunning the automated revision should solve this, as the model is stochastic.


#### Results

![
**A paragraph in the Results section of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/results/ccc-paragraph-01.svg "Diffs - CCC results paragraph 01"){#fig:results:ccc width="100%"}

We tested the tool on a paragraph from the Results section of CCC (@fig:results:ccc).
This paragraph described Figure 1 of the CCC manuscript [@doi:10.1101/2022.06.15.496326], which showed four datasets with two variables each, and four different relationships or patterns.
Manubot correctly referenced the figure and kept math in the original LaTeX format.
The model generated a concise summary of how all coefficients performed in the last two, nonlinear patterns and why CCC was able to capture them, while increasing the complexity of the model to capture the relationships.
We found it remarkable that the model rewrote the original paragraph into three new sentences with the same meaning but more concisely and clearly.
The model also produced high-quality revisions for several other paragraphs that only required minor changes.


For example, in Figure 3, the model removed the reference to the MeSH terms, which we re-added.

In some cases, the model generated a revised text that was too simplified and removed important details.
For example, Figure 3 shows that the model removed the reference to the MeSH terms.
To address this, we re-added the missing details to the revised text.
In other cases, the model generated a text that was shorter and more direct, but it changed the meaning of the original paragraph.
We accepted the simplified sentence structure but added back the missing details.


![
**A paragraph in the Results section of PhenoPLIER.**
Original text is on the left and suggested revision on the right.
](images/diffs/results/phenoplier-paragraph-01.svg "Diffs - PhenoPLIER results paragraph 01"){#fig:results:phenoplier width="100%"}


When applied to the PhenoPLIER manuscript, the model produced high-quality revisions for most paragraphs, while preserving citations and references to figures, tables, and other sections of the manuscript.
In some cases, important details were missing but could be easily added back while preserving the improved sentence structure.
In other cases, the model's output demonstrated the limitations of revising one paragraph at a time without considering the rest of the text.
For instance, one paragraph described a CRISPR screening approach to assess whether top genes in a latent variable could represent good therapeutic targets.
However, the model generated a paragraph with a different meaning (Figure @fig:results:phenoplier).
It omitted the CRISPR screen and the gene symbols associated with the regulation of lipids, which were key elements in the original text.
Instead, the new text described an experiment that does not exist with a reference to a nonexisting section.
This suggests the model focused on the title and keywords of the manuscript (Table @tbl:manuscripts) that were part of every prompt (Figure @fig:ai_revision).
For example, it included the idea of "gene co-expression" analysis (a keyword) to identify "therapeutic targets" (another keyword) and replaced the mention of "sets of genes" in the original text with "clusters of genes".
This was a poor model-based revision, indicating the original paragraph was too short or disconnected and should be merged with the next one (which describes follow-up and related experiments).


#### Discussion

In both the CCC and PhenoPLIER manuscripts, revisions to the discussion section appeared to be of high quality.
The model kept the correct format when necessary (e.g., using italics for gene symbols), maintained most of the citations, and improved the readability of the text in general.
Revisions for some paragraphs introduced minor mistakes that a human author could readily fix.

![
**A paragraph in the Discussion section of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/discussion/ccc-paragraph-01.svg "Diffs - CCC discussion paragraph 01"){#fig:discussion:ccc width="100%"}

The results of the CCC analysis showed a strong correlation between not-only-linear coefficients and genetic studies of complex traits (Figure @fig:discussion:ccc).
The model was able to correctly interpret the format of citations, merging two references in the original text into a single citation block in the revised text.
This demonstrates the potential of AI-assisted authoring for increasing efficiency in scholarly publishing.


#### Methods

Prompts for the Methods section were the most challenging to design, especially when the sections included equations.
The prompt for Methods (Figure @fig:ai_revision) is more focused in keeping the technical details, which was especially important for PhenoPLIER, whose Methods section contains paragraphs with several mathematical expressions.

![
**A paragraph in the Methods section of PhenoPLIER.**
Original text is on the left and suggested revision on the right.
](images/diffs/methods/phenoplier-paragraph-01.svg "Diffs - PhenoPLIER methods paragraph 01"){#fig:methods:phenoplier width="100%"}

We revised a paragraph in PhenoPLIER that contained two numbered equations (Figure \ref{fig:methods:phenoplier}).
The model made very few changes and preserved all equations, citations, and most of the original text.
Remarkably, the model identified and fixed a wrong reference to a mathematical symbol.
The equation with the univariate model used by PrediXcan (lines 4-6 in the original) included the *true* effect size $\gamma_l$ (`\gamma_l`) instead of the *estimated* one $\hat{\gamma}_l$ (`\hat{\gamma}_l`).


In PhenoPLIER, we found one large paragraph with several equations that the model failed to revise, although it performed relatively well in revising the rest of the section.
In CCC, the revision of this section was good overall, with some minor and easy-to-fix issues as in the other sections.


This resulted in a figure being generated in the rendered output, which was not intended but was a result of the AIâ€™s attempt to fill in the missing context.

In PhenoPLIER, we observed issues when revising one paragraph at a time without context.
For example, in the first paragraph the linear models used by S-PrediXcan and S-MultiXcan were mentioned without providing equations or details.
The model added the equations immediately (in the correct Manubot/Markdown format), resulting in a figure being generated in the rendered output, which was not intended.
This was due to the AI's attempt to fill in the missing context.


![
**A paragraph in the Methods section of ManubotAI.**
Original text is on the left and suggested revision on the right.
The revision (right) contains a repeated set of sentences at the top that we removed to improve the clarity of the figure.
](images/diffs/methods/manubotai-paragraph-01.svg "Diffs - ManubotAI methods paragraph 01"){#fig:methods:manubotai width="100%"}


When revising the Methods section of Manubot-AI (this manuscript), the model added novel sentences with incorrect information in some cases.
For example, it added a formula (in the correct Manubot format) to supposedly predict the cost of a revision run (Figure @fig:methods:manubotai).
Additionally, it added sentences saying the model was "trained on a corpus of scientific papers in the same field as the manuscript" and that its suggested revisions resulted in a "modified version of the manuscript ready for submission".
These are important future directions, but do not accurately describe the present work.
