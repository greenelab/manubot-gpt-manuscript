# Observations of AI-based revisions {#sec:results}

## Evaluation setup

We evaluated our AI-based revision workflow by testing different language models and manuscripts.
For this, we used three different GPT-3 models from OpenAI: `text-davinci-003`, `text-davinci-edit-001`, and `text-curie-001`.
The first two are based on the most capable Davinci models, (see [OpenAI - GPT-3 models](https://beta.openai.com/docs/models/gpt-3)).
The difference between them is that `text-davinci-003` is a production-ready model for the completion endpoint, whereas `text-davinci-edit-001` is used for the newly edits endopoint (in beta).
The edits endpoint provides a more natural interface for the revision of manuscripts since it has two inputs: the instructions and the text to revise.
This is different from the completion endpoint, where there is a single input that contains the instructions and the text to revise.
Finally, we also selected `text-curie-001` because, in addition to being faster and chepear than Davinci models, it is defined as a "very capable" model by their authors (see [OpenAI - GPT-3 models](https://beta.openai.com/docs/models/gpt-3)).


| Manuscript ID    | Title | Keywords |
|:-------|:----------------------|:----------|
| [CCC](https://github.com/greenelab/ccc-manuscript) | An efficient not-only-linear correlation coefficient based on machine learning                                   | correlation coefficient, nonlinear relationships, gene expression |
| [PhenoPLIER](https://github.com/greenelab/phenoplier_manuscript) | Projecting genetic associations through gene expression patterns highlights disease etiology and drug mechanisms | genetic studies, functional genomics, gene co-expression, therapeutic targets, drug repurposing, clustering of complex traits |
| [Manubot-AI](https://github.com/greenelab/manubot-gpt-manuscript) | A publishing infrastructure for AI-assisted academic authoring | manubot, artificial intelligence, scholarly publishing, software |

Table: **Manuscripts used to evaluate the AI-based revision workflow.** The title and keywords of a manuscript are used in prompts for revising paragraphs. IDs are used in the text to refer to them, and they link to their GitHub repositories. {#tbl:manuscripts}


Assessing the performance of an automated revision tool is not straightforward, since a review of a revision will be subjective.
For this reason, we used three manuscripts of our own authorship to be able to more accurately assess the quality of the revision (Table @tbl:manuscripts).
The first two are existing manuscripts that were previously written, and the third one is this manuscript which was written and then revised using our tool before submission.
The first manuscript describes the Clustermatch Correlation Coefficient (CCC) [@doi:10.1101/2022.06.15.496326], a new correlation coefficient that was evaluated in transcriptomic data to find novel, potentially nonlinear relationships between gene pairs in the Genotype-Tissue Expression v8 (GTEx) project.
The second manuscript describes PhenoPLIER [@doi:10.1101/2021.07.05.450786], a framework that comprises three different methods to improve the interpretability of genetic studies of complex diseases.
We refer to these two manuscripts as CCC and PhenoPLIER, respectively.
CCC is in the field of computational biology, whereas PhenoPLIER is in the field of genomic medicine.
CCC describes one computational method applied to one data type (correlation to gene expression).
PhenoPLIER describes a framework that comprises three different approaches (regression, clustering and drug-disease prediction) using data from genome-wide and transcription-wide association studies (GWAS and TWAS), gene expression, and transcriptional responses to small molecule perturbations.
Therefore, CCC provides has a simpler structure, whereas PhenoPLIER is a more complex manuscript with more figures and tables and a Methods section including equations for different methods.
The third manuscript is this one, where we describe software that uses machine learning models for the automated revision of scientific manuscripts, and we refer to it as Manubot-AI.
Manubot-AI provides an example with a simple structure and significantly less figures than the rest.
It was written and revised using our tool before submission, which provides a more real AI-based revision use case.
These three manuscripts allowed us to significantly improve and test our prompts, and we report these findings below.


We enabled the Manubot AI revision workflow in the GitHub repositories of the three manuscripts (CCC: `https://github.com/greenelab/ccc-manuscript`, PhenoPLIER: `https://github.com/greenelab/phenoplier_manuscript`, Manubot-AI: `https://github.com/greenelab/manubot-gpt-manuscript`).
This added the `"AI-revision"`{.red} workflow to the "Actions" tab of each repository, which allows to be manually triggered by the user.
Then, we ran the workflow on the three manuscripts using the three language models described above, producing one pull request (PR) per manuscript and model.
These PRs (three per manuscript) can be accessed from the "Pull requests" tab from each repository, where they are titled *"GPT (MODEL) used to revise manuscript"* with *MODEL* being the identifier of the model used.
PRs show the differences between the original text and the suggestions made by the AI-based revision tool.
Below we discussed our findings based on these PRs using the language models across different sections of the manuscripts.


## Performance of language models

We found that Davinci models, as expected, were superior than the Curie model for all manuscripts.
The Curie model is described as "very capable", and it is faster and less expensive than Davinci models.
However, as shown in the PRs generated using this model (titled `GPT (text-curie-001) used to revise manuscript`), the model was not able to produce acceptable revisions for any of the manuscripts.
Most of its suggestions were not coherent with the original text in any of the sections.


Among Davinci models, we found that for `text-davinci-edit-001` (edits endpoint), the quality of the revisions was subjectively inferior to `text-davinci-003` (completion endpoint).
In general, the model either did not produce a revision (such as for abstracts) or the suggested changes were minimal or did not improve the original text.
In paragraphs from the introduction, for instance, this model failed to keep references to other scientific articles in CCC, and in PhenoPLIER it didn't produce a meaningful revision.
This might be explained by the fact that the edits endpoint is still in beta.


The `text-davinci-003` model produced the best results for all manuscripts and across the different sections.
Since both `text-davinci-003` and `text-davinci-edit-001` are based on the same models, we only report the results of `text-davinci-003` below.


## Revision of different sections

We inspected the PRs generated by the AI-based workflow, and highlight below some of the most interesting changes suggested by the tool across different section of the manuscripts.
We show the differences between the original text and the revisions by the tool in a `diff` format (obtained from GitHub), where the original text is on the left, and the suggested one on the right.
Line numbers were also included to more easily see the differences in length.
When applicable, single words are also underlined and highlighted in colors to more clearly see the differences within a single sentence.
In these cases, words underlined in red were removed by the tool, whereas words underlined in green were added and words not underlined were kept unchanged.
The full diffs can be seen by inspecting the PRs for each manuscript and model, and then clicking on the "Files changed" tab.


### Abstract

This is the revision for the abstract of CCC:

![
](images/diffs/abstract/ccc-abstract.svg "Diffs - CCC abstract"){width="100%"}

The tool completely rewrote the text, where only the last sentence was mostly unchanged.
The text was significantly shortened, although sentences are longer than the original ones which could make the abstract slightly harder to read.
The revision removed the first two sentences that introduces correlation analyses and transcriptomics, and directly stated from the beginning the purpose of the manuscript.
It also removed details about the method (line 5), and focused on the aims and the results obtained, ending with almost the same last sentence which suggest a more broad application of the coefficient to other data domains (as originally intended by the authors of CCC).
However, none of the ideas suggested to be removed were critical, and all the main concepts are still present in the revised text.


The revised text for the abstract of PhenoPLIER was significantly shortened (from 10 sentences in the original, to only 3 in the revised version).
However, in this case, important concepts (such as GWAS, TWAS, CRISPR) and a proper amount of background information were missing, producing a less informative abstract.


### Introduction

This is the revision of the first paragraph of the introduction of CCC:

![
](images/diffs/introduction/ccc-paragraph-01.svg "Diffs - CCC introduction paragraph 01"){width="100%"}

The tool, again, significantly revised the text, producing a much better and more concise introductory paragraph.
For example, the revised first sentence, in contrast with the original one, incorportes the ideas of "large datasets", and the "opportunities/possibilities" for "scientific exploration" that they provide.
Then the model generated a more concise and clear second sentence introducing the problem ("we need efficient tools" to find "multiple relationships" in these large datasets).
The third sentence also nicely connects with the previous one.
In comparison, the rest of the changes are minor but they still significantly improved the text.
All references to scientific literature were kept using the correct Manubot format for citations, although our prompts never specify the format of the text ("Manubot" or "Markdown" is never mentioned).
The rest of the sentences in this section of CCC were also correctly revised, and could be directly incorporated into the manuscript with minor or no further changes at all.


We also observed a high quality revision of the introdution of PhenoPLIER.
For some paragraphs, however, the model failed to keep the format of citations, or the the models did not converge to a revised text.


### Results

### Discussion

### Methods

