# Evaluating the AI-based revision workflow {#sec:results}

## Evaluation setup

We evaluated our AI-based revision workflow by testing different language models and manuscripts.
For this, we used three different GPT-3 models from OpenAI: `text-davinci-003`, `text-davinci-edit-001`, and `text-curie-001`.
The first two are based on the most capable Davinci models, (see [OpenAI - GPT-3 models](https://beta.openai.com/docs/models/gpt-3)).
The difference between them is that `text-davinci-003` is a production-ready model for the completion endpoint, whereas `text-davinci-edit-001` is used for the newly edits endopoint (in beta).
The edits endpoint provides a more natural interface for the revision of manuscripts since it has two inputs: the instructions and the text to revise.
This is different from the completion endpoint, where there is a single input that contains the instructions and the text to revise.
Finally, we also selected `text-curie-001` because, in addition to being faster and chepear than Davinci models, it is defined as a "very capable" model by their authors (see [OpenAI - GPT-3 models](https://beta.openai.com/docs/models/gpt-3)).


| Manuscript ID    | Title | Keywords |
|:-------|:----------------------|:----------|
| [CCC](https://github.com/greenelab/ccc-manuscript) | An efficient not-only-linear correlation coefficient based on machine learning                                   | correlation coefficient, nonlinear relationships, gene expression |
| [PhenoPLIER](https://github.com/greenelab/phenoplier_manuscript) | Projecting genetic associations through gene expression patterns highlights disease etiology and drug mechanisms | genetic studies, functional genomics, gene co-expression, therapeutic targets, drug repurposing, clustering of complex traits |
| [Manubot-AI](https://github.com/greenelab/manubot-gpt-manuscript) | A publishing infrastructure for AI-assisted academic authoring | manubot, artificial intelligence, scholarly publishing, software |

Table: **Manuscripts used to evaluate the AI-based revision workflow.** The title and keywords of a manuscript are used in prompts for revising paragraphs. IDs are used in the text to refer to them, and they link to their GitHub repositories. {#tbl:manuscripts}


Assessing the performance of an automated revision tool is not straightforward.
For this reason, we used three manuscripts of our own authorship to be able to more accurately assess the quality of the revision (Table @tbl:manuscripts).
The first two are existing manuscripts that were previously written, and the third one is this manuscript which was written and then revised using our tool before submission.
The first manuscript describes the Clustermatch Correlation Coefficient (CCC) [@doi:10.1101/2022.06.15.496326], a new correlation coefficient that was evaluated in transcriptomic data to find novel, potentially nonlinear relationships between gene pairs in the Genotype-Tissue Expression v8 (GTEx) project.
The second manuscript describes PhenoPLIER [@doi:10.1101/2021.07.05.450786], a framework that comprises three different methods to improve the interpretability of genetic studies of complex diseases.
We refer to these two manuscripts as CCC and PhenoPLIER, respectively.
CCC is in the field of computational biology, whereas PhenoPLIER is in the field of genomic medicine.
CCC describes one computational method applied to one data type (correlation to gene expression).
PhenoPLIER describes a framework that comprises three different approaches (regression, clustering and drug-disease prediction) using data from genome-wide and transcription-wide association studies (GWAS and TWAS), gene expression, and transcriptional responses to small molecule perturbations.
Therefore, CCC provides has a simpler structure, whereas PhenoPLIER is a more complex manuscript with more figures and tables and a Methods section including equations for different methods.
The third manuscript is this one, where we describe software that uses machine learning models for the automated revision of scientific manuscripts, and we refer to it as Manubot-AI.
Manubot-AI provides an example with a simple structure and significantly less figures than the rest.
It was written and revised using our tool before submission, which provides a more real AI-based revision use case.
These three manuscripts allowed us to significantly improve and test our prompts, and we report these findings below.


We enabled the Manubot AI revision workflow in the GitHub repositories of the three manuscripts (CCC: `https://github.com/greenelab/ccc-manuscript`, PhenoPLIER: `https://github.com/greenelab/phenoplier_manuscript`, Manubot-AI: `https://github.com/greenelab/manubot-gpt-manuscript`).
This added the `"AI-revision"`{.red} workflow to the "Actions" tab of each repository, which allows to be manually triggered by the user.
Then, we ran the workflow on the three manuscripts using the three language models described above, producing one pull request (PR) per manuscript and model.
These PRs (three per manuscript) can be accessed from the "Pull requests" tab from each repository, where they are titled *"GPT (MODEL) used to revise manuscript"* with *MODEL* being the identifier of the model used.
PRs show the differences between the original text and the suggestions made by the AI-based revision tool.
Below we discussed our findings based on these PRs using the language models across different sections of the manuscripts.


## Performance of language models

We found that Davinci models, as expected, were FAR superior than the Curie model for all manuscripts.
The Curie model is described as "very capable" (see [OpenAI - Models overview](https://beta.openai.com/docs/models/overview)), and it is faster and less expensive than Davinci ones.
However, in general it was not able to produce revisions that were acceptable for any of the manuscripts.
The PRs generated using this model (titled `GPT (text-curie-001) used to revise manuscript`) clearly show that the model is not able to produce revisions that are coherent with the original text.


Among Davinci models, we found that `text-davinci-003` (completion endpoint) produced the best results for all manuscripts.
When the model is used from the edits endpoint (`text-davinci-edit-001`), it produces revisions that are not as good as the completion endpoint, but still acceptable in several of the paragraphs revised.
This might be explained by the fact that the edits endpoint is still in beta.
Since both `text-davinci-003` and `text-davinci-edit-001` are based on the same models, we only report the results of `text-davinci-003`.


## Revision of different sections

We found that, in general, the tool generated reasonable suggestions for most sections of the manuscripts, with paragraph that were greatly improved by the tool and others that needed minimal human review.
We also found, though, that some paragraph were not improved by the tool, and in some cases it even made the text worse.
This was particularly true for the Methods and Results sections, where the lack of more context made the tool less effective in the review of some paragraphs.


Here we highlight some of the most interesting findings by inspecting the PRs generated by the AI-based workflow.
We reviewed the changes suggested by the tool across different sections of the manuscripts.
We show the differences between the original text and the revisions suggested by the model in a `diff` format, where the original text is on the left, and the suggested one on the right.
Line numbers were also included to more easily see the differences in length.
When applicable, single words are also underlined and highlighted to more clearly see the differences.
In these cases, words underlined in red were removed by the model, words underlined in green were added by the model, and words not underlined were kept unchanged.
The full diffs can be seen by going to the PRs in GitHub for each manuscript and model, and clicking on the "Files changed" tab.


### Abstract

Paragraph in CCC and PhenoPLIER were significantly shortened by the tool, CHECK THIS: and in some cases the meaning of the paragraph was changed.
Sometimes the revised abstract included less background information than necessary.

MAYBE THE EXAMPLE OF PHENOPLIER HERE, where gene networks for instance are not mentioned in the revised abstract.


### Introduction

This is the first paragraph of the Introduction in CCC:

![
](images/diffs/introduction/ccc-paragraph-01.svg "AI-based revision applied on a Manubot manuscript"){width="100%"}

It can be seen that the revised text reads better than the original one.
References to other manuscripts are retained correctly and with the right format, such as `[@doi:10.1073/pnas.1217269109]`.
Ideas and concepts expressed across several sentences in the original text are condensed into a single sentence in the revised text.

AND NOW SUMMARIZE THE REST OF THE INTRODUCTIONS, highlighting good and bad results.


### Results

### Discussion

### Methods

I LIKE THE EXAMPLE OF THE FIRST PARAGRAPH in PhenoPLIER of the section "The Predixcan family..."
  - it added a formula
  - the first sentence does not make sense, but the rest is good
  - it didn finish generating because of the limit of tokens I guess

I also like the paragraph of S-PrediXcan, with a formula that did not have the correct symbol


- we found that sometimes paragraphs were split into more concice paragraphs
  - for example, in PROMPT #1 in the Methods sections of the CCC manuscript.

- modify the PRs with each model to add an explanation of why those PR are there: they are part of the paper

- curie vs davinci:
  - curie is much worse in revising though, maybe we should close this PR
  - for example, davinci models understand citations in Markdown/Manubot

WE PROCESS THE manuscript paragraph by paragraph, mainly because the model has a limit in the number of tokens for each request
THIS PRODUCES SOME PROBLEMS, for example:
  - shorts like TWAS are some times interpreted wrongly by the model, like PhenoPLIER PROMPT #2 Discussions

SOMETIMES, the model takes on paragraph and splits it in different ones, for example:
 - PhenoPLIER, PROMPT 2 Discussion
 - CCC: there is another example I dont remember

REREFENCE HERE the pull requests generated for the three papers
    - AND also include the diffs in the supplementary material

in PROMPT #5 PhenoPLIER introduction, the bad keywords for the manuscript made it make terrible mistakes in re-writting the paragraph
THIS WAS DRASTICALLY IMPROVED by adjusting keywords (see PROMPT #6 PhenoPLIER Introduction)
USUALLY, when the model splits one paragraph in several ones, it is an indication of a bad paragraph.

- each query to the API is indendependt, so the context is lost
  - for example, acrynomics are always defined again, such as MIC

- edit endpoints are more conservative (and mention also that it is in beta)
- completion endpoints seems to have more freedom to fully change the paragraph from scratch
