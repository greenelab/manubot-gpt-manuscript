# Observations of AI-based revisions {#sec:results}

## Evaluation setup

We evaluated our AI-based revision workflow by testing different language models and manuscripts.
For this, we used three different GPT-3 models from OpenAI: `text-davinci-003`, `text-davinci-edit-001`, and `text-curie-001`.
The first two are based on the most capable Davinci models, (see [OpenAI - GPT-3 models](https://beta.openai.com/docs/models/gpt-3)).
The difference between them is that `text-davinci-003` is a production-ready model for the completion endpoint, whereas `text-davinci-edit-001` is used for the newly edits endopoint (in beta).
The edits endpoint provides a more natural interface for the revision of manuscripts since it has two inputs: the instructions and the text to revise.
This is different from the completion endpoint, where there is a single input that contains the instructions and the text to revise.
Finally, we also selected `text-curie-001` because, in addition to being faster and chepear than Davinci models, it is defined as a "very capable" model by their authors (see [OpenAI - GPT-3 models](https://beta.openai.com/docs/models/gpt-3)).


| Manuscript ID    | Title | Keywords |
|:-------|:----------------------|:----------|
| [CCC](https://github.com/greenelab/ccc-manuscript) | An efficient not-only-linear correlation coefficient based on machine learning                                   | correlation coefficient, nonlinear relationships, gene expression |
| [PhenoPLIER](https://github.com/greenelab/phenoplier_manuscript) | Projecting genetic associations through gene expression patterns highlights disease etiology and drug mechanisms | genetic studies, functional genomics, gene co-expression, therapeutic targets, drug repurposing, clustering of complex traits |
| [Manubot-AI](https://github.com/greenelab/manubot-gpt-manuscript) | A publishing infrastructure for AI-assisted academic authoring | manubot, artificial intelligence, scholarly publishing, software |

Table: **Manuscripts used to evaluate the AI-based revision workflow.** The title and keywords of a manuscript are used in prompts for revising paragraphs. IDs are used in the text to refer to them, and they link to their GitHub repositories. {#tbl:manuscripts}


Assessing the performance of an automated revision tool is not straightforward, since a review of a revision will necessarily be subjective.
For this reason, we used three manuscripts of our own authorship to be able to more accurately assess the quality of the revision (Table @tbl:manuscripts).
The first two are existing manuscripts that were previously written, and the third one is this manuscript which was written and then revised using our tool before submission.
The first manuscript describes the Clustermatch Correlation Coefficient (CCC) [@doi:10.1101/2022.06.15.496326], a new correlation coefficient that was evaluated in transcriptomic data to find novel, potentially nonlinear relationships between gene pairs in the Genotype-Tissue Expression v8 (GTEx) project.
The second manuscript describes PhenoPLIER [@doi:10.1101/2021.07.05.450786], a framework that comprises three different methods to improve the interpretability of genetic studies of complex diseases.
We refer to these two manuscripts as CCC and PhenoPLIER, respectively.
CCC is in the field of computational biology, whereas PhenoPLIER is in the field of genomic medicine.
CCC describes one computational method applied to one data type (correlation to gene expression).
PhenoPLIER describes a framework that comprises three different approaches (regression, clustering and drug-disease prediction) using data from genome-wide and transcription-wide association studies (GWAS and TWAS), gene expression, and transcriptional responses to small molecule perturbations.
Therefore, CCC provides has a simpler structure, whereas PhenoPLIER is a more complex manuscript with more figures and tables and a Methods section including equations for different methods.
The third manuscript is this one, where we describe software that uses machine learning models for the automated revision of scientific manuscripts, and we refer to it as Manubot-AI.
Manubot-AI provides an example with a simple structure and significantly less figures than the rest.
It was written and revised using our tool before submission, which provides a more real AI-based revision use case.
These three manuscripts allowed us to significantly improve and test our prompts, and we report these findings below.


We enabled the Manubot AI revision workflow in the GitHub repositories of the three manuscripts (CCC: `https://github.com/greenelab/ccc-manuscript`, PhenoPLIER: `https://github.com/greenelab/phenoplier_manuscript`, Manubot-AI: `https://github.com/greenelab/manubot-gpt-manuscript`).
This added the `"AI-revision"`{.red} workflow to the "Actions" tab of each repository, which allows to be manually triggered by the user.
Then, we ran the workflow on the three manuscripts using the three language models described above, producing one pull request (PR) per manuscript and model.
These PRs (three per manuscript) can be accessed from the "Pull requests" tab from each repository, where they are titled *"GPT (MODEL) used to revise manuscript"* with *MODEL* being the identifier of the model used.
PRs show the differences between the original text and the suggestions made by the AI-based revision tool.
Below we discussed our findings based on these PRs using the language models across different sections of the manuscripts.


## Performance of language models

We found that Davinci models, as expected, were superior than the Curie model for all manuscripts.
The Curie model is described as "very capable", and it is faster and less expensive than Davinci models.
However, as shown in the PRs generated using this model (titled `GPT (text-curie-001) used to revise manuscript`), the model was not able to produce acceptable revisions for any of the manuscripts.
Most of its suggestions were not coherent with the original text in any of the sections.


Among Davinci models, we found that for `text-davinci-edit-001` (edits endpoint), the quality of the revisions was subjectively inferior to `text-davinci-003` (completion endpoint).
In general, the model either did not produce a revision (such as for abstracts) or the suggested changes were minimal or did not improve the original text.
In paragraphs from the introduction, for instance, this model failed to keep references to other scientific articles in CCC, and in PhenoPLIER it didn't produce a meaningful revision.
This might be explained by the fact that the edits endpoint is still in beta.


The `text-davinci-003` model produced the best results for all manuscripts and across the different sections.
Since both `text-davinci-003` and `text-davinci-edit-001` are based on the same models, we only report the results of `text-davinci-003` below.


## Revision of different sections

We inspected the PRs generated by the AI-based workflow, and highlight below some of the most interesting changes suggested by the tool across different section of the manuscripts.
These are our subjective assessments of the quality of the revisions, and we encourage the reader to inspect the PRs for each manuscript and model to see the full diffs and make their own conclusions.
These PRs are available in the manuscripts' GitHub repositories and included as diff files in Supplementary File 1 (CCC) and 2 (PhenoPLIER).


We show the differences between the original text and the revisions by the tool in a `diff` format (obtained from GitHub), where the original text is on the left, and the suggested one on the right.
Line numbers were also included to more easily see the differences in length.
When applicable, single words are also underlined and highlighted in colors to more clearly see the differences within a single sentence.
In these cases, words underlined in red were removed by the tool, whereas words underlined in green were added and words not underlined were kept unchanged.
The full diffs can be seen by inspecting the PRs for each manuscript and model, and then clicking on the "Files changed" tab.


### Abstract

This is the revision for the abstract of CCC:

![
](images/diffs/abstract/ccc-abstract.svg "Diffs - CCC abstract"){width="100%"}

The tool completely rewrote the text, where only the last sentence was mostly unchanged.
The text was significantly shortened, although sentences are longer than the original ones which could make the abstract slightly harder to read.
The revision removed the first two sentences that introduces correlation analyses and transcriptomics, and directly stated from the beginning the purpose of the manuscript.
It also removed details about the method (line 5), and focused on the aims and the results obtained, ending with almost the same last sentence which suggest a more broad application of the coefficient to other data domains (as originally intended by the authors of CCC).
However, none of the ideas suggested to be removed were critical, and all the main concepts are still present in the revised text.


The revised text for the abstract of PhenoPLIER was significantly shortened (from 10 sentences in the original, to only 3 in the revised version).
However, in this case, important concepts (such as GWAS, TWAS, CRISPR) and a proper amount of background information were missing, producing a less informative abstract.


### Introduction

This is the revision of the first paragraph of the introduction of CCC:

![
](images/diffs/introduction/ccc-paragraph-01.svg "Diffs - CCC introduction paragraph 01"){width="100%"}

The tool, again, significantly revised the text, producing a much better and more concise introductory paragraph.
For example, the revised first sentence (on the right) incorportes the ideas of "large datasets", and the "opportunities/possibilities" for "scientific exploration" in a clearly and briefly.
These ideas are present in the first two sentences of the original text (on the left).
Then the model generated a more concise and clear second sentence introducing the problem ("we need efficient tools" to find "multiple relationships" in these large datasets).
The third sentence also nicely connects with the previous one.
In comparison, the rest of the changes are minor but they still significantly improved the reading.
All references to scientific literature were kept using the correct Manubot format for citations, although our prompts do not specify the format of the text ("Manubot", "Markdown", or specific instructions about formatting are never mentioned).
The rest of the sentences in this section of CCC were also correctly revised, and could be directly incorporated into the manuscript with minor or no further changes at all.


We also observed a high quality revision of the introdution of PhenoPLIER.
For one paragraph, however, the model failed to keep the format of citations.
Additionally, the model did not converge to a revised text for the last paragraph, and our tool left the error message as an HTML comment at the top of it: `The AI model returned an empty string`.
We observed this issue when debugging the prompts, and it could be related to the fact that the paragraph is large and has a more complex structure than the rest.
However, since the model is stochastic, this can be solved by running the automated revision again.


### Results

Below is a paragraph of the Results section of CCC describing a set of simulations using different datasets shown in a figure (which is Figure 1 in [@doi:10.1101/2022.06.15.496326]).
The figure shows four different datasets with two variables each, and different relationships or patterns named random/independent, non-coexistence, quadratic, and two-lines.

![
](images/diffs/results/ccc-paragraph-01.svg "Diffs - CCC results paragraph 01"){width="100%"}

In addition to having fewer sentences that are slightly longer, the revised paragraph consistently uses only the past tense, whereas the original one has tense shifts.
This makes the text more consistent and easier to read.
The revised paragraph also kept all citations, which although is not explicitely mentioned in the prompts for this section (as it is for introductions), in this case is important.
Math was also kept in the original LaTeX format and the figure was correctly referenced using the Manubot syntax.
The model retained the order of the descriptions of the different relationships in the figure (random/independent, non-coexistence, quadratic, and two-lines), which in this case is desirable since it is the same order as in the figure.
In the third sentence of the revised paragraph (line 3), the model generated a good summary of how all coefficients performed in the last two, nonlinear patterns, and why CCC was able to capture them.
We, as human authors, would make a single change by the end of this sentence to avoid repeating the word "complexity": *"..., while CCC increased the complexity of the model ~~by using different degrees of complexity~~ to capture the relationships"*.
Since a good summary of the performance of all coefficients was already provided, the next two sentences simply describe what the figure shows while keeping a focus on how CCC works.
In this case study, we found it remarkable that the model mixed the ideas in the last sentences in the original paragraph (lines 4 to 8) to generate three new ones (lines 3 to 5) with the same meaning but more concisely and clearly.
The model also produced high-quality revisions for several other paragraphs that would only need minor changes.


Other paragraphs in CCC, however, needed more changes before being ready to be incorporated into the manuscript.
For instance, for some paragraphs, the model generated a revised text that is shorter, more direct and clear.
However, important details were removed, and sometimes sentences changed the meaning.
In this case, we could accept the simplified sentence structure but add back the missing details.


In PhenoPLIER, the model also produced high-quality revisions for most paragraphs, while keeping citations and references to figures, tables and other section of the manuscript in the Manubot/Markdown format.
In some cases, important details were left out, but they could be easily added back while keeping the improved sentence structure of the revised version.
Other cases clearly showed the limitations of revising one paragraph at a time without considering the rest of the text.
An example of this is when we describe our CRISPR screening approach to assess whether top genes in an LV could represent good therapeutic targets:

![
](images/diffs/results/phenoplier-paragraph-01.svg "Diffs - PhenoPLIER results paragraph 01"){width="100%"}

The model generated a paragraph that has a completely different meaning.
The revised paragraph describes an experiment that does not exist and references a nonexisting section.
There is no mention of the CRISPR screen and the gene symbols identified to be associated with the regulation of lipids, which are the key elements in the original text.
Instead, the model seemed to have focused more on the title and keywords of the manuscript (Table @tbl:manuscripts) that are part of every prompt (Figure @fig:ai_revision).
It included the idea of a "gene co-expression" analysis (a keyword) to identify "therapeutic targets" (another keyword), and replaced the mention of "sets of genes" in the original text with "clusters of genes" in the revision (closer to the "clustering" keyword).
Although this is clearly a bad revision, the output of the model also suggests that the original paragraph is maybe too short or disconnected from the rest, and that it should probably be merged with the next one (which describes follow-up and related experiments).


### Discussion

In both the CCC and PhenoPLIER manuscripts, revisions to the discussion section appeared to be of high quality.
The model kept the correct format when necessary (e.g., using italics for gene symbols), maintained most of the citations, and improved the readability of the text in general.
Some paragraphs had minor mistakes that are easy to fix by a human author.

Below we show a paragraph of CCC where we discuss how not-only-linear correlation coefficients could potentially impact genetic studies of complex traits:

![
](images/diffs/discussion/ccc-paragraph-01.svg "Diffs - CCC discussion paragraph 01"){width="100%"}

Although some minor changes could be added, we believe the revised text reads better than the original.
It is also interesting how the model understood the format of citations and built more complex structures from it.
For instance, the two articles referenced in lines 2 and 3 in the original text were correctly merged into a single citation block and separated with ";" in line 2 of the revised text.


### Methods

Prompts for the Methods section were the most challenging to design, especially when it comes to the definition of equations.
The prompt for Methods (Figure @fig:ai_revision) is more focused in keeping the technical details, which was especially important for PhenoPLIER, whose Methods section contains paragraphs with several mathematical expressions.

We show below the revision of a paragraph in PhenoPLIER that contains two numbered equations:

![
](images/diffs/methods/phenoplier-paragraph-01.svg "Diffs - PhenoPLIER methods paragraph 01"){width="100%"}

The revised text contains very few changes: all the equations, citations and most of the original text was preserved.
However, it is interesting how the model identified a mistake in the original text (line 8) and fixed it in the revision (line 7).
Indeed, the equation with the univariate model used by PrediXcan (lines 4 to 6 in the original) includes the *true* effect size $\gamma_l$ (`\gamma_l`) instead of the *estimated* one $\hat{\gamma}_l$ (`\hat{\gamma}_l`).
In PhenoPLIER, the model failed to revise one large paragraph with several equations, although it performed relatively well in revising the rest of the section.
Overall, the revision of this section in CCC was good, with some minor and easy-to-fix issues as in the other sections.


We also observed issues from revising one paragraph at a time without context.
In the case of the Methods section, this translated also in the generation of new equations that did not exist.
For instance, in PhenoPLIER, one of the first paragraphs in this section mentions the linear models used by S-PrediXcan and S-MultiXcan.
The equations for these models are presented the following paragraphs, but since the model have not seen that yet, it opted to add those equations right away (in the correct Manubot/Markdown format).
