## Observations of AI-based revisions {#sec:results}

### Evaluation setup

We assessed the effectiveness of our AI-assisted revision workflow using three GPT-3 models from OpenAI: `text-davinci-003`, `text-davinci-edit-001`, and `text-curie-001`.
The first two are based on the powerful Davinci models (refer to [OpenAI - GPT-3 models](https://beta.openai.com/docs/models/gpt-3)).
`text-davinci-003` is a fully operational model for the completion endpoint, while `text-davinci-edit-001` is still in beta and is utilized for the edits endpoint.
The latter model provides a more intuitive interface for manuscript revisions as it requires two inputs: instructions and the text to be revised.
The `text-curie-001` model is a faster and more economical option than the Davinci models, and its creators describe it as "very capable" (refer to [OpenAI - GPT-3 models](https://beta.openai.com/docs/models/gpt-3)).


| Manuscript ID    | Title | Keywords |
|:-------|:----------------------|:----------|
| [CCC](https://github.com/greenelab/ccc-manuscript) | An efficient not-only-linear correlation coefficient based on machine learning                                   | correlation coefficient, nonlinear relationships, gene expression |
| [PhenoPLIER](https://github.com/greenelab/phenoplier_manuscript) | Projecting genetic associations through gene expression patterns highlights disease etiology and drug mechanisms | genetic studies, functional genomics, gene co-expression, therapeutic targets, drug repurposing, clustering of complex traits |
| [Manubot-AI](https://github.com/greenelab/manubot-gpt-manuscript) | A publishing infrastructure for AI-assisted academic authoring | manubot, artificial intelligence, scholarly publishing, software |

Table: **Manuscripts used to evaluate the AI-based revision workflow.** The title and keywords of a manuscript are used in prompts for revising paragraphs. IDs are used in the text to refer to them, and they link to their GitHub repositories. {#tbl:manuscripts}


Evaluating the effectiveness of an automated revision tool poses a challenge due to the subjective nature of a revision review.
To address this, we utilized three manuscripts authored by us (Table @tbl:manuscripts): Clustermatch Correlation Coefficient (CCC) [@doi:10.1101/2022.06.15.496326], PhenoPLIER [@doi:10.1101/2021.07.05.450786], and Manubot-AI (this manuscript).
CCC is a correlation coefficient for transcriptomic data, while PhenoPLIER is a framework incorporating three different methods for genetic studies.
CCC is in computational biology, whereas PhenoPLIER is in genomic medicine.
CCC describes one computational method applied to one data type, while PhenoPLIER describes a framework utilizing various data sources.
CCC has a simpler structure, while PhenoPLIER is a more complex manuscript with more figures, tables, and equations.
The third manuscript, Manubot-AI, illustrates a simpler structure and was written and revised using our tool before submission, providing a practical AI-based revision use case.
We tested and refined our prompts using these manuscripts and report our findings below.


We incorporated the Manubot AI revision workflow into the GitHub repositories of three manuscripts: CCC (`https://github.com/greenelab/ccc-manuscript`), PhenoPLIER (`https://github.com/greenelab/phenoplier_manuscript`), and Manubot-AI (`https://github.com/greenelab/manubot-gpt-manuscript`).
This workflow added the "ai-revision" option to the "Actions" tab in each repository.
We initiated the workflow manually and utilized the three language models mentioned earlier to create one pull request (PR) per manuscript and model.
These PRs can be viewed in the "Pull requests" tab of each repository, and are named *"GPT (MODEL) used to revise manuscript"*, with *MODEL* representing the model used.
The PRs demonstrate the differences between the original text and the AI-based revision suggestions.
In the following sections, we present our observations based on these PRs.


### Performance of language models

We found that Davinci models outperformed the Curie model across all manuscripts.
The Curie model is faster and less expensive than Davinci models.
However, the PRs show that the model was not able to produce acceptable revisions for any of the manuscripts.
Most of its suggestions were not coherent with the original text in any of the sections.


The `text-davinci-edit-001` (edits endpoint) model produced lower quality revisions compared to `text-davinci-003` (completion endpoint).
The suggested changes were either minimal or did not enhance the original text.
In some cases, such as abstracts, no revisions were produced.
The model also failed to maintain references to other scientific articles in CCC and did not provide significant revisions for PhenoPLIER.
This could be due to the beta stage of the edits endpoint.


The `text-davinci-003` model produced the best results for all manuscripts and across the different sections.
Since both `text-davinci-003` and `text-davinci-edit-001` are based on the same models, we only report the results of `text-davinci-003` below.


### Revision of different sections

In our study titled 'A publishing infrastructure for AI-assisted academic authoring', we examined the changes proposed by the AI-based workflow in the manuscripts' PRs.
We observed significant modifications in various sections of the manuscripts.
We recommend readers to review the PRs for each manuscript and model to obtain a complete understanding of the changes made.
The PRs are accessible in the manuscripts' GitHub repositories and can also be found as diff files in Supplementary File 1 (CCC), 2 (PhenoPLIER), and 3 (Manubot-AI).


In this paper titled "A Publishing Infrastructure for AI-assisted Academic Authoring" with keywords "Manubot, Artificial Intelligence, Scholarly Publishing, Software", we present the differences between the original text and the revisions made by the tool.
We used a `diff` format obtained from GitHub and included line numbers to show the length differences.
Single words are underlined and highlighted in colors to make the differences within a sentence more apparent.
Red indicates words removed by the tool, green indicates words added, and no underlining indicates words kept unchanged.
The complete diffs can be viewed by inspecting the PRs for each manuscript and model, and then clicking on the "Files changed" tab.


#### Abstract

![
**Abstract of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/abstract/ccc-abstract.svg "Diffs - CCC abstract"){#fig:abstract:ccc width="100%"}

We utilized the AI-based revision workflow on the CCC abstract (Figure @fig:abstract:ccc).
The tool completely restructured the text, with only the last sentence remaining mostly unchanged.
The revised abstract was significantly shorter, with longer sentences that may make it slightly more challenging to read.
The revision eliminated the first two sentences that introduced correlation analyses and transcriptomics and instead directly stated the manuscript's purpose.
It also removed details about the method (line 5) and focused on the aims and results, concluding with the same last sentence that suggested the coefficient's broader application to other data domains (as originally intended by the authors of CCC).
Despite the changes, the revised text still conveyed the main concepts.


The revised text for the abstract of PhenoPLIER was significantly shortened (from 10 sentences in the original, to only 3 in the revised version).
However, in this case, important concepts (such as GWAS, TWAS, CRISPR) and a proper amount of background information were missing, producing a less informative abstract.


#### Introduction

![
**First paragraph in the Introduction section of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/introduction/ccc-paragraph-01.svg "Diffs - CCC introduction paragraph 01"){#fig:intro:ccc width="100%"}

The Introduction section of CCC was significantly improved by our tool (Figure @fig:intro:ccc), producing a clearer and more concise paragraph.
The first sentence was revised to include ideas from the original two sentences, introducing "large datasets" and scientific exploration opportunities.
The second sentence was also made more concise, introducing the "need for efficient tools" to find "multiple relationships" in these datasets.
The third sentence flowed well from the previous one.
All references to scientific literature were correctly formatted according to Manubot standards, although our prompts did not specify the format.
The remaining sentences in this section were also appropriately revised and can be incorporated into the manuscript with little or no further changes.


In our study on a publishing infrastructure for AI-assisted academic authoring using manubot and artificial intelligence, we found that the introdution of PhenoPLIER was significantly improved after revision.
However, there were issues with the citation format in one paragraph, and the model failed to converge to a revised text for the last paragraph, resulting in an error message.
Further investigation showed that this may be due to the complexity of the paragraph.
We suggest rerunning the automated revision, as the model is stochastic and may resolve the issue.


#### Results

![
**A paragraph in the Results section of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/results/ccc-paragraph-01.svg "Diffs - CCC results paragraph 01"){#fig:results:ccc width="100%"}

We utilized our AI-assisted publishing infrastructure, Manubot, to test its functionality on a paragraph from the Results section of CCC (Figure @fig:results:ccc).
This paragraph details the content of Figure 1 in the CCC manuscript [@doi:10.1101/2022.06.15.496326], which displays four distinct datasets, each with two variables, showcasing various patterns and relationships such as random/independent, non-coexistence, quadratic, and two-lines.
The revised paragraph is more concise, using past tense consistently and avoiding tense shifts.
It retains all citations, and math is preserved in the original LaTeX format.
The paragraph also accurately references the figure using Manubot syntax.
In the third sentence of the revised paragraph, the AI model provides a succinct summary of how all coefficients performed in the last two nonlinear patterns and explains how CCC was able to capture them.
We made a minor adjustment to avoid repetition in the sentence, changing "by using different degrees of complexity" to "to capture the relationships." The revised paragraph is clearer and more concise, effectively conveying the information about the figure and how CCC works.
It is noteworthy that the AI model was able to rephrase some of the concepts in the original paragraph (lines 4 to 8) into three new sentences (lines 3 to 5) with the same meaning but more concisely and clearly.
The model also produced high-quality revisions for several other paragraphs that required only minor changes.


In the Results section of our academic paper titled 'A publishing infrastructure for AI-assisted academic authoring' with keywords 'manubot, artificial intelligence, scholarly publishing, software', we found that certain paragraphs in CCC required further modifications before being suitable for inclusion in the manuscript.
The AI model generated revised versions of some paragraphs that were more concise and straightforward, but at times, crucial details were omitted, and the meaning of sentences was altered.
To resolve this issue, we opted to retain the simplified sentence structure while reintroducing the missing information.


![
**A paragraph in the Results section of PhenoPLIER.**
Original text is on the left and suggested revision on the right.
](images/diffs/results/phenoplier-paragraph-01.svg "Diffs - PhenoPLIER results paragraph 01"){#fig:results:phenoplier width="100%"}


The model applied to the PhenoPLIER manuscript produced high-quality revisions for most paragraphs, while preserving citations and references to figures, tables, and other sections in the Manubot/Markdown format.
Although some important details were missing, they were easily added back while maintaining the improved sentence structure of the revised version.
However, the model's output also revealed the limitations of revising one paragraph at a time without considering the rest of the text.
For example, one paragraph describing our CRISPR screening approach omitted key elements, such as the CRISPR screen and associated gene symbols, and instead described a non-existent experiment with a reference to a non-existent section (Figure @fig:results:phenoplier).
This suggests that the model focused on the title and keywords of the manuscript (Table @tbl:manuscripts) that were part of every prompt (Figure @fig:ai_revision).
The model-based revision was poor and indicated that the original paragraph may be too short or disconnected from the rest and could benefit from merging with the next one, which describes follow-up and related experiments.


#### Discussion

In both the CCC and PhenoPLIER manuscripts, revisions to the discussion section appeared to be of high quality.
The model kept the correct format when necessary (e.g., using italics for gene symbols), maintained most of the citations, and improved the readability of the text in general.
Revisions for some paragraphs introduced minor mistakes that a human author could readily fix.

![
**A paragraph in the Discussion section of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/discussion/ccc-paragraph-01.svg "Diffs - CCC discussion paragraph 01"){#fig:discussion:ccc width="100%"}

One paragraph in the Results section of our academic paper titled 'A publishing infrastructure for AI-assisted academic authoring' and with the keywords 'manubot, artificial intelligence, scholarly publishing, software', discusses the potential impact of non-linear correlation coefficients on genetic studies of complex traits (see Figure @fig:discussion:ccc).
The revised text is more concise and clear than the original, while still retaining references to figures and tables.
It is also noteworthy that the AI model was able to understand and correctly format citations, as demonstrated by the merging of the two articles referenced in lines 2 and 3 of the original text into a single citation block separated by ";" in line 2 of the revised text.


#### Methods

Prompts for the Methods section were the most challenging to design, especially when the sections included equations.
The prompt for Methods (Figure @fig:ai_revision) is more focused in keeping the technical details, which was especially important for PhenoPLIER, whose Methods section contains paragraphs with several mathematical expressions.

![
**A paragraph in the Methods section of PhenoPLIER.**
Original text is on the left and suggested revision on the right.
](images/diffs/methods/phenoplier-paragraph-01.svg "Diffs - PhenoPLIER methods paragraph 01"){#fig:methods:phenoplier width="100%"}

We made revisions to a paragraph in PhenoPLIER (Figure @fig:methods:phenoplier), which included two numbered equations.
The model made minimal changes and preserved all equations, citations, and most of the original text.
Notably, the model corrected a reference to a mathematical symbol (line 8) in the revised version (line 7).
The univariate model equation used by PrediXcan (lines 4-6 in the original) included the *true* effect size $\gamma_l$ (`\gamma_l`) instead of the *estimated* one $\hat{\gamma}_l$ (`\hat{\gamma}_l`).
This observation is remarkable.


In PhenoPLIER, we found one large paragraph with several equations that the model failed to revise, although it performed relatively well in revising the rest of the section.
In CCC, the revision of this section was good overall, with some minor and easy-to-fix issues as in the other sections.


During our study on AI-assisted academic authoring using the Manubot software, we encountered a problem when revising paragraphs out of context.
Specifically, we noticed that in the PhenoPLIER section, the initial paragraph briefly mentioned the linear models utilized by S-PrediXcan and S-MultiXcan without providing any further information.
Although the following paragraphs did present the equations and details, the Manubot software added those equations right away since it had not encountered them yet.
However, these equations were eventually corrected to the proper Manubot/Markdown format.


![
**A paragraph in the Methods section of ManubotAI.**
Original text is on the left and suggested revision on the right.
The revision (right) contains a repeated set of sentences at the top that we removed to improve the clarity of the figure.
](images/diffs/methods/manubotai-paragraph-01.svg "Diffs - ManubotAI methods paragraph 01"){#fig:methods:manubotai width="100%"}


During the revision of the Methods section of this manuscript, Manubot-AI occasionally generated erroneous sentences.
For example, in one paragraph, the model inserted a formula in the correct Manubot format, purportedly predicting the cost of a revision run.
In another paragraph (Figure @fig:methods:manubotai), it added sentences claiming that the model was trained on a corpus of scientific papers from the same field as the manuscript and that its suggested revisions produced a modified version of the manuscript that was submission-ready.
While these are valuable future directions, they do not accurately reflect the current work.
