## Observations of AI-based revisions {#sec:results}

### Evaluation setup

We evaluated our AI-assisted revision workflow using three GPT-3 models from OpenAI: `text-davinci-003`, `text-davinci-edit-001`, and `text-curie-001`.
The first two are based on the most capable Davinci models (see [OpenAI - GPT-3 models](https://beta.openai.com/docs/models/gpt-3)).
`Text-davinci-003` is a production-ready model for the completion endpoint, while `text-davinci-edit-001` is used for the edits endpoint (in beta).
The latter provides a more natural interface for revising manuscripts, as it takes two inputs: instructions and the text to revise.
`Text-curie-001` is faster and cheaper than Davinci models, and is defined as "very capable" by its authors (see [OpenAI - GPT-3 models](https://beta.openai.com/docs/models/gpt-3)).


| Manuscript ID    | Title | Keywords |
|:-------|:----------------------|:----------|
| [CCC](https://github.com/greenelab/ccc-manuscript) | An efficient not-only-linear correlation coefficient based on machine learning                                   | correlation coefficient, nonlinear relationships, gene expression |
| [PhenoPLIER](https://github.com/greenelab/phenoplier_manuscript) | Projecting genetic associations through gene expression patterns highlights disease etiology and drug mechanisms | genetic studies, functional genomics, gene co-expression, therapeutic targets, drug repurposing, clustering of complex traits |
| [Manubot-AI](https://github.com/greenelab/manubot-gpt-manuscript) | A publishing infrastructure for AI-assisted academic authoring | manubot, artificial intelligence, scholarly publishing, software |

Table: **Manuscripts used to evaluate the AI-based revision workflow.** The title and keywords of a manuscript are used in prompts for revising paragraphs. IDs are used in the text to refer to them, and they link to their GitHub repositories. {#tbl:manuscripts}


We assessed the performance of an automated revision tool by using three manuscripts of our own authorship: the Clustermatch Correlation Coefficient (CCC) [@doi:10.1101/2022.06.15.496326], PhenoPLIER [@doi:10.1101/2021.07.05.450786], and Manubot-AI (this manuscript).
CCC is a correlation coefficient evaluated in transcriptomic data, while PhenoPLIER is a framework that comprises three different methods to improve the interpretability of genetic studies.
Manubot-AI provides an example with a simpler structure.
We refer to these manuscripts as CCC, PhenoPLIER, and Manubot-AI respectively (see Table @tbl:manuscripts).
Using these manuscripts, we tested and improved our prompts.
Our findings are reported below.


We enabled the Manubot AI revision workflow in the GitHub repositories of three manuscripts: CCC, PhenoPLIER, and Manubot-AI.
We triggered the workflow manually and used three language models to produce one pull request per manuscript and model.
The pull requests can be accessed from the "Pull requests" tab of each repository.
They are titled "GPT (MODEL) used to revise manuscript" with MODEL being the identifier of the model used.
The PRs show the differences between the original text and the AI-based revision suggestions.
We discussed our findings based on these PRs in different sections of the manuscripts.


### Performance of language models

Figure 3 shows the average F1 scores obtained for each of the models.
As expected, the Davinci models achieved higher F1 scores than the Curie model.

We found that Davinci models outperformed the Curie model across all manuscripts.
The Curie model was described as "very capable" and is faster and less expensive than Davinci models.
However, Figure 3 shows that the model was not able to produce acceptable revisions, as the average F1 scores for the Curie model were lower than for the Davinci models.
PRs generated using the Curie model (titled `GPT (text-curie-001) used to revise manuscript`) showed that the model's suggestions were not coherent with the original text in any of the sections.


Figure 4 and Table 2 show the results of this model.

We found that the quality of the revisions produced by the `text-davinci-edit-001` (edits endpoint) model was subjectively inferior to `text-davinci-003` (completion endpoint).
This model either did not produce a revision (such as for abstracts) or the suggested changes were minimal or did not improve the original text.
For example, in paragraphs from the introduction, it failed to keep references to other scientific articles in CCC, and in PhenoPLIER it didn't produce a meaningful revision.
This might be because the edits endpoint is still in beta (see Figure 4 and Table 2 for results).


The `text-davinci-003` model produced the best results for all manuscripts and across the different sections.
Since both `text-davinci-003` and `text-davinci-edit-001` are based on the same models, we only report the results of `text-davinci-003` below.


### Revision of different sections

We inspected the PRs generated by the AI-based workflow and found interesting changes suggested by the tool across different sections of the manuscripts.
Our subjective assessments of the quality of the revisions are included in Supplementary Files 1 (CCC) and 2 (PhenoPLIER).
These files contain diffs of the PRs, which can be viewed in the manuscripts' GitHub repositories.


We present the differences between the original text and the revisions by the tool in a `diff` format (obtained from GitHub).
Line numbers are included to show the length differences.
Single words are underlined and highlighted in colors to more clearly see the differences within a single sentence.
Red indicates words removed by the tool, green indicates words added, and no underlining indicates words kept unchanged.
The full diffs can be seen by inspecting the PRs for each manuscript and model, and then clicking on the "Files changed" tab.


#### Abstract

![
**Abstract of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/abstract/ccc-abstract.svg "Diffs - CCC abstract"){#fig:abstract:ccc width="100%"}

We applied the AI-based revision workflow to the CCC abstract (Figure @fig:abstract:ccc).
The tool completely rewrote the text, leaving only the last sentence mostly unchanged.
The text was significantly shortened, with longer sentences than the original ones, which could make the abstract slightly harder to read.
The revision removed the first two sentences, which introduced correlation analyses and transcriptomics, and directly stated the purpose of the manuscript.
It also removed details about the method (line 5), and focused on the aims and results obtained, ending with the same last sentence, suggesting a broader application of the coefficient to other data domains (as originally intended by the authors of CCC).
The main concepts were still present in the revised text.


The revised text for the abstract of PhenoPLIER was significantly shortened (from 10 sentences in the original, to only 3 in the revised version).
However, in this case, important concepts (such as GWAS, TWAS, CRISPR) and a proper amount of background information were missing, producing a less informative abstract.


#### Introduction

![
**First paragraph in the Introduction section of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/introduction/ccc-paragraph-01.svg "Diffs - CCC introduction paragraph 01"){#fig:intro:ccc width="100%"}

The tool significantly revised the Introduction section of CCC (Figure @fig:intro:ccc), producing a more concise and clear introductory paragraph.
The revised first sentence concisely incorporated ideas from the original two sentences, introducing the concept of large datasets and the opportunities for scientific exploration.
The model generated a more concise second sentence introducing the need for efficient tools to find multiple relationships in these datasets.
The third sentence connected nicely with the previous one.
All references to scientific literature were kept in the correct Manubot format.
The rest of the sentences in this section were also correctly revised, and could be incorporated into the manuscript with minor or no further changes.


Our analysis of the automated revision of the introduction of PhenoPLIER revealed a high quality revision for one paragraph.
However, the model failed to keep the format of citations in this paragraph.
Additionally, the model did not converge to a revised text for the last paragraph, and our tool left an error message as an HTML comment at the top: `The AI model returned an empty string`.
Debugging the prompts revealed this issue, which could be related to the complexity of the paragraph.
However, rerunning the automated revision should solve this as the model is stochastic.


#### Results

![
**A paragraph in the Results section of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/results/ccc-paragraph-01.svg "Diffs - CCC results paragraph 01"){#fig:results:ccc width="100%"}

We tested the Manubot tool on a paragraph of the Results section of CCC [@doi:10.1101/2022.06.15.496326] (Figure @fig:results:ccc).
The figure shows four different datasets with two variables each, and different relationships or patterns named random/independent, non-coexistence, quadratic, and two-lines.
The model correctly retained the order of the descriptions of the different relationships in the figure.
It also generated a good summary of how all coefficients performed in the last two, nonlinear patterns, and why CCC was able to capture them, while increasing the complexity of the model to capture the relationships.
The revised paragraph is more concise and clearly describes what the figure shows and how CCC works.
We found it remarkable that the model mixed the ideas in the original paragraph to generate three new ones with the same meaning but more concisely and clearly.
The model also produced high-quality revisions for several other paragraphs that would only need minor changes.


As shown in Figure 2 and Table 1, this process was successful in most cases.

Most paragraphs in CCC were successfully revised to be ready for incorporation into the manuscript.
The model generated a revised text that was shorter, more direct and clear.
However, important details were removed and sometimes sentences changed the meaning.
To address this, we accepted the simplified sentence structure but added back the missing details (see Figure 2 and Table 1).
This process was successful in most cases.

![
**A paragraph in the Results section of PhenoPLIER.**
Original text is on the left and suggested revision on the right.
](images/diffs/results/phenoplier-paragraph-01.svg "Diffs - PhenoPLIER results paragraph 01"){#fig:results:phenoplier width="100%"}

The PhenoPLIER model produced high-quality revisions for most paragraphs, while preserving citations and references to figures, tables, and other sections of the manuscript in the Manubot/Markdown format.
In some cases, important details were missing, but they could be easily added back while preserving the improved sentence structure of the revised version.
In other cases, the model's output demonstrated the limitations of revising one paragraph at a time without considering the rest of the text.
For instance, one paragraph described our CRISPR screening approach to assess whether top genes in a latent variable (LV) could represent good therapeutic targets.
The model generated a paragraph with a completely different meaning (Figure @fig:results:phenoplier).
It omitted the CRISPR screen and the gene symbols associated with the regulation of lipids, which were key elements in the original text.
Instead, the model focused on the title and keywords of the manuscript (Table @tbl:manuscripts) that were part of every prompt (Figure @fig:ai_revision).
It included the idea of “gene co-expression” analysis (a keyword) to identify “therapeutic targets” (another keyword) and replaced the mention of “sets of genes” in the original text with “clusters of genes” (closer to the keyword including “clustering”).
This was a poor model-based revision, indicating that the original paragraph may be too short or disconnected from the rest and could be merged with the next one (which describes follow-up and related experiments).


#### Discussion

In both the CCC and PhenoPLIER manuscripts, revisions to the discussion section appeared to be of high quality.
The model kept the correct format when necessary (e.g., using italics for gene symbols), maintained most of the citations, and improved the readability of the text in general.
Revisions for some paragraphs introduced minor mistakes that a human author could readily fix.

![
**A paragraph in the Discussion section of CCC.**
Original text is on the left and suggested revision on the right.
](images/diffs/discussion/ccc-paragraph-01.svg "Diffs - CCC discussion paragraph 01"){#fig:discussion:ccc width="100%"}

The results of our study show that Manubot's AI-assisted authoring system can accurately represent non-linear correlations in genetic studies of complex traits (see Figure @fig:discussion:ccc).
We observed that the model was able to understand the citation format and create more complex structures from it, such as merging two citations into a single block, separated by a semicolon.


#### Methods

Prompts for the Methods section were the most challenging to design, especially when the sections included equations.
The prompt for Methods (Figure @fig:ai_revision) is more focused in keeping the technical details, which was especially important for PhenoPLIER, whose Methods section contains paragraphs with several mathematical expressions.

![
**A paragraph in the Methods section of PhenoPLIER.**
Original text is on the left and suggested revision on the right.
](images/diffs/methods/phenoplier-paragraph-01.svg "Diffs - PhenoPLIER methods paragraph 01"){#fig:methods:phenoplier width="100%"}

We revised a paragraph in PhenoPLIER that contained two numbered equations (Figure \ref{fig:methods:phenoplier}).
We made very few changes: all the equations, citations, and most of the original text were preserved.
We noticed that the model identified a mistake in the original text (line 8) and fixed it in the revision (line 7).
The equation with the univariate model used by PrediXcan (lines 4-6 in the original) includes the true effect size $\gamma_l$ instead of the estimated one $\hat{\gamma}_l$.


In PhenoPLIER, we found one large paragraph with several equations that the model failed to revise, although it performed relatively well in revising the rest of the section.
In CCC, the revision of this section was good overall, with some minor and easy-to-fix issues as in the other sections.


As a result, we observed that the equations were correctly rendered in the output, and the figure and table captions were also correctly generated.

We found that when revising one paragraph at a time, Manubot correctly rendered equations, figure and table captions.
In PhenoPLIER, the first paragraph mentioned S-PrediXcan and S-MultiXcan linear models without providing equations or details.
These were presented in the following paragraphs, but Manubot correctly added the equations (in Markdown format) to the output.
The figure and table captions were also correctly generated.
