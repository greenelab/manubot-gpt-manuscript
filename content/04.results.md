# Results {#sec:results}

We evaluated our AI-based revision workflow by testing different language models across a set of existing manuscripts as well as to author a new one.
For this, we used three different GPT-3 models from OpenAI: `text-davinci-003`, `text-davinci-edit-001`, and `text-curie-001`.
The first two are based on the most capable Davinci models, [according to their authors](https://beta.openai.com/docs/models/gpt-3).
The difference between them is that `text-davinci-003` is a production-ready model for the completion endpoint, whereas `text-davinci-edit-001` is used for the newly edits endopoint (in beta) and provides a more natural interface for the revision of manuscript since it has two inputs: the instructions and the text to revise.
Finally, `text-curie-001` is faster and cheaper than Davinci models, although less capable.


Assessing the performance of an automated revision tool is not straightforward.
For this reason, we used three manuscripts of our own authorship to be able to more accurately assess the quality of the revision.
The first two are existing manuscripts that were previously written, and the third one is this manuscript which was written and then revised using our tool before submission.
The first manuscript  describes the Clustermatch Correlation Coefficient (CCC) [@doi:10.1101/2022.06.15.496326], a new correlation coefficient that was evaluated in transcriptomic data to find novel, potentially nonlinear relationships between gene pairs in the Genotype-Tissue Expression v8 (GTEx) project.
The second manuscript describes PhenoPLIER [@doi:10.1101/2021.07.05.450786], a framework that comprises three different methods to improve the interpretability of genetic studies of complex diseases.
We refer to these two manuscripts as CCC and PhenoPLIER, respectively.
CCC is in the field of computational biology, whereas PhenoPLIER is in the field of genomic medicine.
CCC describes one computational method applied to one data type (correlation to gene expression).
PhenoPLIER describes a framework that comprises three different approaches (regression, clustering and drug-disease prediction) using data from genome-wide and transcription-wide association studies (GWAS and TWAS), gene expression, and transcriptional responses to small molecule perturbations (profiled in LINCS L1000).
Therefore, CCC provides a revision use case with a simpler structure, whereas PhenoPLIER is a more complex manuscript with more figures and tables, and a Methods section including equations for different methods.
The third manuscript is this one, where we describe software that uses machine learning models for the automated revision of scientific manuscripts, and we refer to it as Manubot-AI.
Manubot-AI provides an example with a simple structure, significantly less figures than the rest, that was written and revised using our tool before submission, providing a revision use case that is closer to a real-world scenario.
These three manuscripts allowed us to significantly improve and test our prompts, and we report these findings below.


First, we enabled the Manubot AI revision workflow in the GitHub repositories of the three manuscripts (CCC: https://github.com/greenelab/ccc-manuscript, PhenoPLIER: https://github.com/greenelab/phenoplier_manuscript/, Manubot-AI: https://github.com/greenelab/manubot-gpt-manuscript/).
This added the "AI-revision" workflow to the GitHub Actions of each repository, which is manually triggered by the user.
Then, we ran the workflow on the three manuscripts using the three language models described above, producing one pull request (PR) per manuscript and model.
These PRs show the diffs produces by the AI-based revision tool and can be accessed from the "Pull request" tab of the GitHub repositories of each manuscript.
The title of these PRs is "GPT (MODEL) used to revise manuscript", where MODEL is the identifier of the model used.
Below we discuss our findings for each manuscript and model.


regarding the language models, we found that `text-davinci-003` was in general the best model for all three manuscripts.
although `text-curie-001` was fast and cheap, it was not able to produce revisions that were acceptable for any of the manuscripts.
`text-davinci-edit-001` (edits endpoint) produced acceptable results, although not as good as the `text-davinci-003`.
This might be explained by the fact that the edits endpoint is still in beta.
Since both `text-davinci-003` and `text-davinci-edit-001` are based on the Davinci models, we only report the results of `text-davinci-003`.
The edits endpoint, however, provides the most natural interface for the revision of manuscripts, and since it's based on the Davinci models, we believe that it might become the best option in the future.


### Revision of Abstracts

### Revision of Introductions

### Revision of Results

### Revision of Discussions

### Revision of Methods



- we found that sometimes paragraphs were split into more concice paragraphs
  - for example, in PROMPT #1 in the Methods sections of the CCC manuscript.

- modify the PRs with each model to add an explanation of why those PR are there: they are part of the paper

- curie vs davinci:
  - curie is much worse in revising though, maybe we should close this PR
  - for example, davinci models understand citations in Markdown/Manubot

WE PROCESS THE manuscript paragraph by paragraph, mainly because the model has a limit in the number of tokens for each request
THIS PRODUCES SOME PROBLEMS, for example:
  - shorts like TWAS are some times interpreted wrongly by the model, like PhenoPLIER PROMPT #2 Discussions

SOMETIMES, the model takes on paragraph and splits it in different ones, for example:
 - PhenoPLIER, PROMPT 2 Discussion
 - CCC: there is another example I dont remember

REREFENCE HERE the pull requests generated for the three papers
    - AND also include the diffs in the supplementary material

in PROMPT #5 PhenoPLIER introduction, the bad keywords for the manuscript made it make terrible mistakes in re-writting the paragraph
THIS WAS DRASTICALLY IMPROVED by adjusting keywords (see PROMPT #6 PhenoPLIER Introduction)
USUALLY, when the model splits one paragraph in several ones, it is an indication of a bad paragraph.

- each query to the API is indendependt, so the context is lost
  - for example, acrynomics are always defined again, such as MIC

- edit endpoints are more conservative (and mention also that it is in beta)
- completion endpoints seems to have more freedom to fully change the paragraph from scratch


We used this infrastructure to revise an existing manuscript as well as to author a new one.
We back-ported the changes in Manubot to a manuscript describing the Clustermatch Correlation Coefficient (CCC) [@doi:10.1101/2022.06.15.496326].
The CCC was designed to capture both linear and non-linear relationships between variables.
The CCC manuscript describes its use, in particular with gene expression data.

The abstract of the CCC manuscript before revision had a Flesh-Kincaid readability score of 186.36 and a grade level of college level.

After suggested revisions, the readability score was 184.29, the grade level was college level and read as follows:
```
The Clustermatch Correlation Coefficient (CCC) is a powerful, efficient, and easy-to-use tool for detecting meaningful linear and nonlinear patterns in data.
CCC is particularly useful for transcriptomics, where it can identify genes with correlated expression that may be involved in the same biological processes or diseases.
Compared to other correlation coefficients, CCC is faster and more accurate, as it captures general patterns in data by comparing clustering solutions.
When applied to human gene expression data, CCC identified robust linear relationships as well as nonlinear patterns associated with sex differences that were missed by linear-only methods.
Furthermore, CCC-ranked gene pairs were enriched for interactions in integrated networks, suggesting that CCC can detect functional relationships that linear-only methods fail to capture.
CCC is an ideal tool for quickly and accurately uncovering meaningful linear and nonlinear patterns in genome-scale data and other domains across different data types.
```

The full manuscript before AI-based revision is available at [link], and the revised version is available at [new_link].
We noticed that the model has difficulty with the Manubot citation style, which may lead to some references becoming incorrect.
This pipeline is not fully automated: authors will need to review changes and verify the output.

We also used this framework in the context of authoring a new manuscript that described a publishing infrastructure that implemented large language models to suggest revisions.
The abstract before revisions had a Flesh-Kincaid readability score of 180.47, a grade level of college level, and it reads as follows:
```
Academics often communicate through scholarly manuscripts.
These manuscripts describe new advances, summarize existing literature, or argue for changes in the status quo.
Writing and revising manuscripts can be a time-consuming process.
Large language models are bringing new capabilities to many areas of knowledge work.
We integrated the use of large language models into the Manubot publishing ecosystem.
Users of Manubot can run a workflow, which will trigger a series of queries to OpenAI's language models, produce revisions, and create a timestamped set of suggested revisions.
Given the amount of time that researchers put into crafting prose, we expect this advance to radically transform the type of knowledge work that academics perform.
```

After suggested revisions, abstract had a Flesh-Kincaid readability score of 177.7761, a grade level of college level, and it reads as follows:
```
Academics communicate through scholarly manuscripts, which often describe new advances, summarize existing literature, or argue for changes in the status quo.
Writing and revising these manuscripts can be a time-consuming process.
To address this, we have integrated large language models into the Manubot publishing ecosystem.
This workflow can trigger a series of queries to OpenAI's language models, produce revisions, and create a timestamped set of suggested revisions.
We expect this to revolutionize the type of knowledge work that academics perform, as it will save them time and effort in crafting prose.
```


