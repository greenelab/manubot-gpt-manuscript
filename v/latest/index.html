<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Milton Pividori" />
  <meta name="author" content="Casey S. Greene" />
  <meta name="dcterms.date" content="2023-01-21" />
  <meta name="keywords" content="manubot, artificial intelligence, scholarly publishing, software" />
  <title>A publishing infrastructure for AI-assisted academic authoring</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <!--
  Manubot generated metadata rendered from header-includes-template.html.
  Suggest improvements at https://github.com/manubot/manubot/blob/main/manubot/process/header-includes-template.html
  -->
  <meta name="dc.format" content="text/html" />
  <meta property="og:type" content="article" />
  <meta name="dc.title" content="A publishing infrastructure for AI-assisted academic authoring" />
  <meta name="citation_title" content="A publishing infrastructure for AI-assisted academic authoring" />
  <meta property="og:title" content="A publishing infrastructure for AI-assisted academic authoring" />
  <meta property="twitter:title" content="A publishing infrastructure for AI-assisted academic authoring" />
  <meta name="dc.date" content="2023-01-21" />
  <meta name="citation_publication_date" content="2023-01-21" />
  <meta property="article:published_time" content="2023-01-21" />
  <meta name="dc.modified" content="2023-01-21T19:25:19+00:00" />
  <meta property="article:modified_time" content="2023-01-21T19:25:19+00:00" />
  <meta name="dc.language" content="en-US" />
  <meta name="citation_language" content="en-US" />
  <meta name="dc.relation.ispartof" content="Manubot" />
  <meta name="dc.publisher" content="Manubot" />
  <meta name="citation_journal_title" content="Manubot" />
  <meta name="citation_technical_report_institution" content="Manubot" />
  <meta name="citation_author" content="Milton Pividori" />
  <meta name="citation_author_institution" content="Department of Genetics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA 19104, USA" />
  <meta name="citation_author_orcid" content="0000-0002-3035-4403" />
  <meta name="twitter:creator" content="@miltondp" />
  <meta name="citation_author" content="Casey S. Greene" />
  <meta name="citation_author_institution" content="Center for Health AI, University of Colorado School of Medicine, Aurora, CO 80045, USA" />
  <meta name="citation_author_institution" content="Department of Biomedical Informatics, University of Colorado School of Medicine, Aurora, CO 80045, USA" />
  <meta name="citation_author_orcid" content="0000-0001-8713-9213" />
  <meta name="twitter:creator" content="@GreeneScientist" />
  <link rel="canonical" href="https://greenelab.github.io/manubot-gpt-manuscript/" />
  <meta property="og:url" content="https://greenelab.github.io/manubot-gpt-manuscript/" />
  <meta property="twitter:url" content="https://greenelab.github.io/manubot-gpt-manuscript/" />
  <meta name="citation_fulltext_html_url" content="https://greenelab.github.io/manubot-gpt-manuscript/" />
  <meta name="citation_pdf_url" content="https://greenelab.github.io/manubot-gpt-manuscript/manuscript.pdf" />
  <link rel="alternate" type="application/pdf" href="https://greenelab.github.io/manubot-gpt-manuscript/manuscript.pdf" />
  <link rel="alternate" type="text/html" href="https://greenelab.github.io/manubot-gpt-manuscript/v/629d383139fa238ca3a5fa5570b2cd35cac61a28/" />
  <meta name="manubot_html_url_versioned" content="https://greenelab.github.io/manubot-gpt-manuscript/v/629d383139fa238ca3a5fa5570b2cd35cac61a28/" />
  <meta name="manubot_pdf_url_versioned" content="https://greenelab.github.io/manubot-gpt-manuscript/v/629d383139fa238ca3a5fa5570b2cd35cac61a28/manuscript.pdf" />
  <meta property="og:type" content="article" />
  <meta property="twitter:card" content="summary_large_image" />
  <link rel="icon" type="image/png" sizes="192x192" href="https://manubot.org/favicon-192x192.png" />
  <link rel="mask-icon" href="https://manubot.org/safari-pinned-tab.svg" color="#ad1457" />
  <meta name="theme-color" content="#ad1457" />
  <!-- end Manubot generated metadata -->
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">A publishing infrastructure for AI-assisted academic authoring</h1>
</header>
<p><small><em>
This manuscript
(<a href="https://greenelab.github.io/manubot-gpt-manuscript/v/629d383139fa238ca3a5fa5570b2cd35cac61a28/">permalink</a>)
was automatically generated
from <a href="https://github.com/greenelab/manubot-gpt-manuscript/tree/629d383139fa238ca3a5fa5570b2cd35cac61a28">greenelab/manubot-gpt-manuscript@629d383</a>
on January 21, 2023.
</em></small></p>
<h2 id="authors">Authors</h2>
<ul>
<li><p><strong>Milton Pividori</strong>
<br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0000-0002-3035-4403">0000-0002-3035-4403</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/miltondp">miltondp</a>
· <img src="images/twitter.svg" class="inline_icon" width="16" height="16" alt="Twitter icon" />
<a href="https://twitter.com/miltondp">miltondp</a>
<br>
<small>
Department of Genetics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA 19104, USA
</small></p></li>
<li><p><strong>Casey S. Greene</strong>
<sup><a href="#correspondence">✉</a></sup><br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0000-0001-8713-9213">0000-0001-8713-9213</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/cgreene">cgreene</a>
· <img src="images/twitter.svg" class="inline_icon" width="16" height="16" alt="Twitter icon" />
<a href="https://twitter.com/GreeneScientist">GreeneScientist</a>
<br>
<small>
Center for Health AI, University of Colorado School of Medicine, Aurora, CO 80045, USA; Department of Biomedical Informatics, University of Colorado School of Medicine, Aurora, CO 80045, USA
· Funded by The Gordon and Betty Moore Foundation, GBMF4552; The National Human Genome Research Institute, R01 HG010067; The Eunice Kennedy Shriver National Institute of Child Health and Human Development, R01 HD109765
</small></p></li>
</ul>
<div id="correspondence">
<p>✉ — Correspondence possible via <a href="https://github.com/greenelab/manubot-gpt-manuscript/issues">GitHub Issues</a>
or email to
Casey S. Greene &lt;casey.s.greene@cuanschutz.edu&gt;.</p>
</div>
<h2 class="page_break_before" id="abstract">Abstract</h2>
<p>In this work we investigate how models with advanced natural language processing capabilities can be used to reduce the time-consuming process of writing and revising scholarly manuscripts.
To this end, we integrate large language models into the Manubot publishing ecosystem to suggest revisions for scholarly text.
We tested our AI-based revision workflow in three case studies of existing manuscripts, including the present one.
Our results suggest that these models can capture the concepts in the scholarly text and produce high-quality revisions that improve clarity.
Given the amount of time that researchers put into crafting prose, we anticipate that this advance will revolutionize the type of knowledge work performed by academics.</p>
<h2 id="introduction">Introduction</h2>
<p>Manuscripts have been around for thousands of years, but scientific journals have only been around for about 350 years <span class="citation" data-cites="F3iZfGUC">[<a href="#ref-F3iZfGUC" role="doc-biblioref">1</a>]</span>.
External peer review, which is used by many journals, is even more recent, having been around for less than 100 years <span class="citation" data-cites="1HMhNrQq1">[<a href="#ref-1HMhNrQq1" role="doc-biblioref">2</a>]</span>.
Most manuscripts are written by humans or teams of humans working together to describe new advances, summarize existing literature, or argue for changes in the status quo.
However, scholarly writing is a time-consuming process where results of a study are presented using a specific style and format.
Academics can sometimes be long-winded in getting to key points, making writing more impenetrable to their audience <span class="citation" data-cites="19YWsShi0">[<a href="#ref-19YWsShi0" role="doc-biblioref">3</a>]</span>.</p>
<p>Recent advances in computing capabilities and the widespread availability of text, images, and other data on the internet have laid the foundation for artificial intelligence (AI) models with billions of parameters.
Large language models, in particular, are opening the floodgates to new technologies with the capability to transform how society operates <span class="citation" data-cites="xq1uEbPa">[<a href="#ref-xq1uEbPa" role="doc-biblioref">4</a>]</span>.
OpenAI’s models, for instance, have been trained on vast amounts of data and can generate human-like text <span class="citation" data-cites="bYOaJHMe">[<a href="#ref-bYOaJHMe" role="doc-biblioref">5</a>]</span>.
These models are based on the transformer architecture which uses self-attention mechanisms to model the complexities of language.
The most well-known of these models is the Generative Pre-trained Transformer 3 (GPT-3), which have been shown to be highly effective for a range of language tasks such as generating text, completing code, and answering questions <span class="citation" data-cites="bYOaJHMe">[<a href="#ref-bYOaJHMe" role="doc-biblioref">5</a>]</span>.
This has the potential to revolutionize how scientists write and revise scholarly manuscripts, saving time and effort and enabling researchers to focus on more high-level tasks such as data analysis and interpretation.</p>
<p>We present a novel AI-assisted revision tool that envisions a future where authors collaborate with large language models in the writing of their manuscripts.
This workflow builds on the Manubot infrastructure for scholarly publishing <span class="citation" data-cites="YuJbg3zO">[<a href="#ref-YuJbg3zO" role="doc-biblioref">6</a>]</span>, a platform designed to enable both individual and large-scale collaborative projects <span class="citation" data-cites="PZMP42Ak 10gsAq0o">[<a href="#ref-PZMP42Ak" role="doc-biblioref">7</a>,<a href="#ref-10gsAq0o" role="doc-biblioref">8</a>]</span>.
Our workflow involves parsing the manuscript, utilizing a large language model with section-specific prompts for revision, and then generating a set of suggested changes to be integrated into the main document.
These changes are presented to the user through the GitHub interface for review.
To evaluate our workflow, we conducted a case study with three Manubot-authored manuscripts that included sections of varying complexity.
Our findings indicate that, in most cases, the models were able to maintain the original meaning of text, improve the writing style, and even interpret mathematical expressions.
Our AI-assisted writing workflow can be incorporated into any Manubot manuscript, and we anticipate it will help authors more effectively communicate their work.</p>
<h2 id="implementing-ai-based-revision-into-the-manubot-publishing-ecosystem">Implementing AI-based revision into the Manubot publishing ecosystem</h2>
<h3 id="overview">Overview</h3>
<div id="fig:ai_revision" class="fignos">
<figure>
<img src="images/figure_1.svg" style="width:85.0%" alt="Figure 1: AI-based revision applied on a Manubot-based manuscript. a) A manuscript (written with Manubot) with different sections. b) Section-specific prompts used to process each paragraph. If a paragraph belongs to a non-standard section, then a default prompt will be used to perform a basic revision only. The prompt for the Methods section includes the formatting of equations with identifiers. All sections’ prompts include these instructions: “the text grammar is correct, spelling errors are fixed, and the text has a clear sentence structure”, although these are only shown for abstracts." />
<figcaption aria-hidden="true"><span>Figure 1:</span> <strong>AI-based revision applied on a Manubot-based manuscript.</strong>
<strong>a)</strong> A manuscript (written with Manubot) with different sections.
<strong>b)</strong> Section-specific prompts used to process each paragraph.
If a paragraph belongs to a non-standard section, then a default prompt will be used to perform a basic revision only.
The prompt for the Methods section includes the formatting of equations with identifiers.
All sections’ prompts include these instructions: <em>“the text grammar is correct, spelling errors are fixed, and the text has a clear sentence structure”</em>, although these are only shown for abstracts.</figcaption>
</figure>
</div>
<p>We implemented an AI-based revision infrastructure in Manubot <span class="citation" data-cites="YuJbg3zO">[<a href="#ref-YuJbg3zO" role="doc-biblioref">6</a>]</span>, a tool for collaborative writing of scientific manuscripts.
Manubot integrates with popular version control platforms such as GitHub, allowing authors to easily track changes and collaborate on writing in real time.
Furthermore, Manubot automates the process of generating a formatted manuscript (such as HTML, PDF, DOCX; Figure <a href="#fig:ai_revision">1</a>a shows the HTML output).
Built on this modern and open paradigm, our AI-based revision software was developed using GitHub Actions, which allows the user to easily trigger an automated revision task on the entire manuscript or specific sections of it.</p>
<p>When the user triggers the action, the manuscript is parsed by section and then by paragraph (Figure <a href="#fig:ai_revision">1</a>b) and passed to the language model along with a set of custom prompts.
The model then returns a revised version of the text.
Our workflow then uses the GitHub API to generate a new pull request, allowing the user to review and modify the output before merging the changes into the manuscript.
This workflow attributes text to either the human user or to the AI language model, which may be important in light of potential future legal decisions that alter the copyright landscape around the outputs of generative models.</p>
<p>We used the <a href="https://openai.com/api/">OpenAI API</a> for access to these models.
Since this API incurs a cost with each run that depends on manuscript length, we implemented a workflow in GitHub Actions that can be manually triggered by the user.
Our implementation allows users to tune the costs to their needs by allowing them to select specific sections to be revised instead of the entire manuscript.
Additionally, several model parameters can be adjusted to tune costs even further, such as the language model version (including Davinci and Curie, and potentially newly published ones), how much risk the model will take, or the “quality” of the completions.
For instance, using Davinci models (the most complex and capable ones), the cost per run is under $0.50 for most manuscripts.</p>
<h3 id="implementation-details">Implementation details</h3>
<p>Our tools are comprised of Python scripts that perform the AI-based revision (<a href="https://github.com/greenelab/manubot-ai-editor">https://github.com/greenelab/manubot-ai-editor</a>) and a GitHub Actions workflow integrated with Manubot.
To run the workflow, the user must specify the branch that will be revised, select the files/sections of the manuscript (optional), specify the language model to use (<code>text-davinci-003</code> by default), and provide the output branch name.
For more advanced users, it is also possible to change most of the tool’s behavior or the language model parameters.</p>
<p>When the workflow is triggered, it downloads the manuscript by cloning the specified branch.
It revises all of the manuscript files, or only some of them if the user specifies a subset.
Next, each paragraph in the file is read and submitted to the OpenAI API for revision.
If the request is successful, the tool will write the revised paragraph in place of the original one, using one sentence per line (which is the recommended format for the input text).
If the request fails, the tool might try again (up to five times by default) if it is a common error (such as “server overloaded”) or a model-specific error that requires changing some of its parameters.
If the error cannot be handled or the maximum number of retries is reached, the original paragraph is written instead with an HTML comment at the top explaining the cause of the error.
This allows the user to debug the problem and attempt to fix it if desired.</p>
<p>As shown in Figure <a href="#fig:ai_revision">1</a>b, each API request comprises a prompt (the instructions given to the model) and the paragraph to be revised.
The prompt uses the manuscript title and keywords, so both must be accurate to obtain the best revision outcomes.
The other key component to process a paragraph is its section.
For instance, the abstract is a set of sentences with no citations, whereas a paragraph from the Introduction section has several references to other scientific papers.
A paragraph in the Results section has fewer citations but many references to figures or tables, and must provide enough details about the experiments to understand and interpret the outcomes.
The Methods section is more dependent on the type of paper, but in general it has to provide technical details and sometimes mathematical formulas and equations.
Therefore, we designed section-specific prompts, which we found led to the most useful suggestions.
Figures and tables captions, as well as paragraphs that contain only one or two sentences and less than sixty words, are not processed and are copied directly to the output file.</p>
<p>The section of a paragraph is automatically inferred from the file name using a simple strategy, such as if “introduction” or “methods” is part of the file name.
If the tool fails to infer a section from the file, then the user is still able to specify which section the file belongs to.
The section can be a standard one (abstract, introduction, results, methods, or discussion) for which a specific prompt is used (Figure <a href="#fig:ai_revision">1</a>b), or a non-standard one for which a default prompt is used to instruct the model to perform basic revision (minimizing the use of jargon, ensuring text grammar is correct, fixing spelling errors, and making sure the text has a clear sentence structure).</p>
<h3 id="properties-of-language-models">Properties of language models</h3>
<p>Our AI-based revision workflow uses <a href="https://beta.openai.com/docs/guides/completion">text completion</a> to process each paragraph.
We tested our tool using Davinci and Curie models, including <code>text-davinci-003</code>, <code>text-davinci-edit-001</code> and <code>text-curie-001</code>.
Davinci models are the most powerful GPT-3 model, whereas Curie ones are less capable but faster and less expensive.
We mainly focused on the completion endpoint, as the edits endpoint is currently in beta.
All models can be fine-tuned using different parameters (see <a href="https://beta.openai.com/docs/api-reference/completions">OpenAI - API Reference</a>), and the most important ones can be easily adjusted using our tool.</p>
<p>Language models for text completion have a context length that indicates the limit of tokens they can process (tokens are common character sequences in text).
This limit includes the size of the prompt and the paragraph, as well as the maximum number of tokens to generate for the completion (parameter <code>max_tokens</code>).
For instance, the context length of Davinci models is 4,000 and 2,048 for Curie (see <a href="https://beta.openai.com/docs/models/overview">OpenAI - Models overview</a>).
Therefore, it is not possible to use the entire manuscript as input, not even entire sections.
To address this limitation, our AI-assisted revision software processes each paragraph of the manuscript with section-specific prompts, as shown in Figure <a href="#fig:ai_revision">1</a>b.
This approach allows us to process large manuscripts by breaking them into small chunks of text.
However, since the language model only processes a single paragraph from a section, it can potentially lose important context to produce a better output.
Nonetheless, we find that the model still produces high-quality revisions (see <a href="#sec:results">Results</a>).
Additionally, the maximum number of tokens (parameter <code>max_tokens</code>) is set as twice the estimated number of tokens in the paragraph (one token approximately represents four characters, see <a href="https://beta.openai.com/tokenizer%5D">OpenAI - Tokenizer</a>.
The tool automatically adjusts this parameter and performs the request again if a related error is returned by the API.
The user can also force the tool to either use a fixed value for <code>max_tokens</code> for all paragraphs, or change the fraction of maximum tokens based on the estimated paragraph size (two by default).</p>
<p>The language models used are stochastic, meaning they generate a different revision for the same input paragraph each time.
This behavior can be adjusted by using the “sampling temperature” or “nucleus sampling” parameters (we use <code>temperature=0.5</code> by default).
Although we selected default values that worked well across multiple manuscripts, these parameters can be changed to make the model more deterministic.
The user can also instruct the model to generate several completions and select the one with the highest log probability per token, which can improve the quality of the revision.
Our proof-of-concept implementation generates only one completion (parameter <code>best_of=1</code>) to avoid potentially high costs for the user.
Additionally, our workflow allows the user to process either the entire manuscript or individual sections.
This allows for more cost-effective control while focusing on a single piece of text, wherein the user can run the tool several times and pick the preferred revised text.</p>
<h3 id="installation-and-use">Installation and use</h3>
<p>We have contributed our workflow (<a href="https://github.com/manubot/rootstock/pull/484">https://github.com/manubot/rootstock/pull/484</a>) to the standard Manubot template manuscript, which is called rootstock and available at <a href="https://github.com/manubot/rootstock">https://github.com/manubot/rootstock</a>.
Users who wish to use the workflow before it is fully integrated into rootstock can copy the files from the linked pull request in the GitHub repository of their manuscript.
After that, the workflow (named <code>ai-revision</code>) will be available in the Actions tab of the repository.</p>
<h2 id="sec:results">Observations of AI-based revisions</h2>
<h3 id="evaluation-setup">Evaluation setup</h3>
<p>We evaluated our AI-assisted revision workflow using three GPT-3 models from OpenAI: <code>text-davinci-003</code>, <code>text-davinci-edit-001</code>, and <code>text-curie-001</code>.
The first two are based on the most capable Davinci models (see <a href="https://beta.openai.com/docs/models/gpt-3">OpenAI - GPT-3 models</a>).
Whereas <code>text-davinci-003</code> is a production-ready model for the completion endpoint, <code>text-davinci-edit-001</code> is used for the edits endpoint and is still in beta.
The latter provides a more natural interface for revising manuscripts, as it takes two inputs: instructions and the text to revise.
Model <code>text-curie-001</code> is faster and cheaper than Davinci models, and is defined as “very capable” by its authors (see <a href="https://beta.openai.com/docs/models/gpt-3">OpenAI - GPT-3 models</a>).</p>
<div id="tbl:manuscripts" class="tablenos">
<table id="tbl:manuscripts">
<caption><span>Table 1:</span> <strong>Manuscripts used to evaluate the AI-based revision workflow.</strong> The title and keywords of a manuscript are used in prompts for revising paragraphs. IDs are used in the text to refer to them, and they link to their GitHub repositories. </caption>
<colgroup>
<col style="width: 19%" />
<col style="width: 54%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Manuscript ID</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Keywords</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><a href="https://github.com/greenelab/ccc-manuscript">CCC</a></td>
<td style="text-align: left;">An efficient not-only-linear correlation coefficient based on machine learning</td>
<td style="text-align: left;">correlation coefficient, nonlinear relationships, gene expression</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="https://github.com/greenelab/phenoplier_manuscript">PhenoPLIER</a></td>
<td style="text-align: left;">Projecting genetic associations through gene expression patterns highlights disease etiology and drug mechanisms</td>
<td style="text-align: left;">genetic studies, functional genomics, gene co-expression, therapeutic targets, drug repurposing, clustering of complex traits</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="https://github.com/greenelab/manubot-gpt-manuscript">Manubot-AI</a></td>
<td style="text-align: left;">A publishing infrastructure for AI-assisted academic authoring</td>
<td style="text-align: left;">manubot, artificial intelligence, scholarly publishing, software</td>
</tr>
</tbody>
</table>
</div>
<p>Assessing the performance of an automated revision tool is not straightforward, since a review of a revision will necessarily be subjective.
To mitigate this, we used three manuscripts of our own authorship (Table <a href="#tbl:manuscripts">1</a>): the Clustermatch Correlation Coefficient (CCC) <span class="citation" data-cites="eirYTTyk">[<a href="#ref-eirYTTyk" role="doc-biblioref">9</a>]</span>, PhenoPLIER <span class="citation" data-cites="FEh1bD0A">[<a href="#ref-FEh1bD0A" role="doc-biblioref">10</a>]</span>, and Manubot-AI (this manuscript).
CCC is a new correlation coefficient evaluated in transcriptomic data, while PhenoPLIER is a framework that comprises three different methods applied in the field of genetic studies.
CCC is in the field of computational biology, whereas PhenoPLIER is in the field of genomic medicine.
CCC describes one computational method applied to one data type (correlation to gene expression).
PhenoPLIER describes a framework that comprises three different approaches (regression, clustering and drug-disease prediction) using data from genome-wide and transcription-wide association studies (GWAS and TWAS), gene expression, and transcriptional responses to small molecule perturbations.
Therefore, CCC has a simpler structure, whereas PhenoPLIER is a more complex manuscript with more figures and tables and a Methods section including equations.
The third manuscript, Manubot-AI, provides an example with a simpler structure, and it was written and revised using our tool before submission, which provides a more real AI-based revision use case.
Using these manuscripts, we tested and improved our prompts.
Our findings are reported below.</p>
<p>We enabled the Manubot AI revision workflow in the GitHub repositories of the three manuscripts (CCC: <code>https://github.com/greenelab/ccc-manuscript</code>, PhenoPLIER: <code>https://github.com/greenelab/phenoplier_manuscript</code>, Manubot-AI: <code>https://github.com/greenelab/manubot-gpt-manuscript</code>).
This added the “ai-revision” workflow to the “Actions” tab of each repository.
We triggered the workflow manually and used the three language models described above to produce one pull request (PR) per manuscript and model.
These PRs can be accessed from the “Pull requests” tab of each repository.
They are titled <em>“GPT (MODEL) used to revise manuscript”</em> with <em>MODEL</em> being the identifier of the model used.
The PRs show the differences between the original text and the AI-based revision suggestions.
We discuss below our findings based on these PRs across different sections of the manuscripts.</p>
<h3 id="performance-of-language-models">Performance of language models</h3>
<p>We found that Davinci models outperformed the Curie model across all manuscripts.
The Curie model is faster and less expensive than Davinci models.
However, the PRs show that the model was not able to produce acceptable revisions for any of the manuscripts.
Most of its suggestions were not coherent with the original text in any of the sections.</p>
<p>We found that the quality of the revisions produced by the <code>text-davinci-edit-001</code> (edits endpoint) model was subjectively inferior to <code>text-davinci-003</code> (completion endpoint).
This model either did not produce a revision (such as for abstracts) or the suggested changes were minimal or did not improve the original text.
For example, in paragraphs from the introduction, it failed to keep references to other scientific articles in CCC, and in PhenoPLIER it didn’t produce a meaningful revision.
This might be because the edits endpoint is still in beta.</p>
<p>The <code>text-davinci-003</code> model produced the best results for all manuscripts and across the different sections.
Since both <code>text-davinci-003</code> and <code>text-davinci-edit-001</code> are based on the same models, we only report the results of <code>text-davinci-003</code> below.</p>
<h3 id="revision-of-different-sections">Revision of different sections</h3>
<p>We inspected the PRs generated by the AI-based workflow and found interesting changes suggested by the tool across different sections of the manuscripts.
These are our subjective assessments of the quality of the revisions, and we encourage the reader to inspect the PRs for each manuscript and model to see the full diffs and make their own conclusions.
These PRs are available in the manuscripts’ GitHub repositories and also included as diff files in Supplementary File 1 (CCC), 2 (PhenoPLIER) and 3 (Manubot-AI).</p>
<p>We present the differences between the original text and the revisions by the tool in a <code>diff</code> format (obtained from GitHub).
Line numbers are included to show the length differences.
When applicable, single words are underlined and highlighted in colors to more clearly see the differences within a single sentence.
Red indicates words removed by the tool, green indicates words added, and no underlining indicates words kept unchanged.
The full diffs can be seen by inspecting the PRs for each manuscript and model, and then clicking on the “Files changed” tab.</p>
<h4 id="abstract-1">Abstract</h4>
<div id="fig:abstract:ccc" class="fignos">
<figure>
<img src="images/diffs/abstract/ccc-abstract.svg" style="width:100.0%" alt="Figure 2: Abstract of CCC. Original text is on the left and suggested revision on the right." />
<figcaption aria-hidden="true"><span>Figure 2:</span> <strong>Abstract of CCC.</strong>
Original text is on the left and suggested revision on the right.</figcaption>
</figure>
</div>
<p>We applied the AI-based revision workflow to the CCC abstract (Figure <a href="#fig:abstract:ccc">2</a>).
The tool completely rewrote the text, leaving only the last sentence mostly unchanged.
The text was significantly shortened, with longer sentences than the original ones, which could make the abstract slightly harder to read.
The revision removed the first two sentences, which introduced correlation analyses and transcriptomics, and directly stated the purpose of the manuscript.
It also removed details about the method (line 5), and focused on the aims and results obtained, ending with the same last sentence, suggesting a broader application of the coefficient to other data domains (as originally intended by the authors of CCC).
The main concepts were still present in the revised text.</p>
<p>The revised text for the abstract of PhenoPLIER was significantly shortened (from 10 sentences in the original, to only 3 in the revised version).
However, in this case, important concepts (such as GWAS, TWAS, CRISPR) and a proper amount of background information were missing, producing a less informative abstract.</p>
<h4 id="introduction-1">Introduction</h4>
<div id="fig:intro:ccc" class="fignos">
<figure>
<img src="images/diffs/introduction/ccc-paragraph-01.svg" style="width:100.0%" alt="Figure 3: First paragraph in the Introduction section of CCC. Original text is on the left and suggested revision on the right." />
<figcaption aria-hidden="true"><span>Figure 3:</span> <strong>First paragraph in the Introduction section of CCC.</strong>
Original text is on the left and suggested revision on the right.</figcaption>
</figure>
</div>
<p>The tool significantly revised the Introduction section of CCC (Figure <a href="#fig:intro:ccc">3</a>), producing a more concise and clear introductory paragraph.
The revised first sentence concisely incorporated ideas from the original two sentences, introducing the concept of “large datasets” and the opportunities for scientific exploration.
The model generated a more concise second sentence introducing the “need for efficient tools” to find “multiple relationships” in these datasets.
The third sentence connected nicely with the previous one.
All references to scientific literature were kept in the correct Manubot format, although our prompts do not specify the format of the text.
The rest of the sentences in this section were also correctly revised, and could be incorporated into the manuscript with minor or no further changes.</p>
<p>We also observed a high quality revision of the introdution of PhenoPLIER.
However, the model failed to keep the format of citations in one paragraph.
Additionally, the model did not converge to a revised text for the last paragraph, and our tool left an error message as an HTML comment at the top: <code>The AI model returned an empty string</code>.
Debugging the prompts revealed this issue, which could be related to the complexity of the paragraph.
However, rerunning the automated revision should solve this as the model is stochastic.</p>
<h4 id="results">Results</h4>
<div id="fig:results:ccc" class="fignos">
<figure>
<img src="images/diffs/results/ccc-paragraph-01.svg" style="width:100.0%" alt="Figure 4: A paragraph in the Results section of CCC. Original text is on the left and suggested revision on the right." />
<figcaption aria-hidden="true"><span>Figure 4:</span> <strong>A paragraph in the Results section of CCC.</strong>
Original text is on the left and suggested revision on the right.</figcaption>
</figure>
</div>
<p>We tested the tool on a paragraph of the Results section of CCC (Figure <a href="#fig:results:ccc">4</a>).
That paragraph describes Figure 1 of the CCC manuscript <span class="citation" data-cites="eirYTTyk">[<a href="#ref-eirYTTyk" role="doc-biblioref">9</a>]</span>, which shows four different datasets with two variables each, and different relationships or patterns named random/independent, non-coexistence, quadratic, and two-lines.
In addition to having fewer sentences that are slightly longer, the revised paragraph consistently uses only the past tense, whereas the original one has tense shifts.
The revised paragraph also kept all citations, which although is not explicitely mentioned in the prompts for this section (as it is for introductions), in this case is important.
Math was also kept in the original LaTeX format and the figure was correctly referenced using the Manubot syntax.
In the third sentence of the revised paragraph (line 3), the model generated a good summary of how all coefficients performed in the last two, nonlinear patterns, and why CCC was able to capture them.
We, as human authors, would make a single change by the end of this sentence to avoid repeating the word “complexity”: <em>“…, while CCC increased the complexity of the model <del>by using different degrees of complexity</del> to capture the relationships”</em>.
The revised paragraph is more concise and clearly describes what the figure shows and how CCC works.
We found it remarkable that the model rewrote some of the concepts in the original paragraph (lines 4 to 8) into three new sentences (lines 3 to 5) with the same meaning but more concisely and clearly.
The model also produced high-quality revisions for several other paragraphs that would only need minor changes.</p>
<p>Other paragraphs in CCC, however, needed more changes before being ready to be incorporated into the manuscript.
For instance, for some paragraphs, the model generated a revised text that is shorter, more direct and clear.
However, important details were removed and sometimes sentences changed the meaning.
To address this, we could accept the simplified sentence structure but add back the missing details.</p>
<div id="fig:results:phenoplier" class="fignos">
<figure>
<img src="images/diffs/results/phenoplier-paragraph-01.svg" style="width:100.0%" alt="Figure 5: A paragraph in the Results section of PhenoPLIER. Original text is on the left and suggested revision on the right." />
<figcaption aria-hidden="true"><span>Figure 5:</span> <strong>A paragraph in the Results section of PhenoPLIER.</strong>
Original text is on the left and suggested revision on the right.</figcaption>
</figure>
</div>
<p>When applied to the PhenoPLIER manuscript, the model produced high-quality revisions for most paragraphs, while preserving citations and references to figures, tables, and other sections of the manuscript in the Manubot/Markdown format.
In some cases, important details were missing, but they could be easily added back while preserving the improved sentence structure of the revised version.
In other cases, the model’s output demonstrated the limitations of revising one paragraph at a time without considering the rest of the text.
For instance, one paragraph described our CRISPR screening approach to assess whether top genes in a latent variable (LV) could represent good therapeutic targets.
The model generated a paragraph with a completely different meaning (Figure <a href="#fig:results:phenoplier">5</a>).
It omitted the CRISPR screen and the gene symbols associated with the regulation of lipids, which were key elements in the original text.
Instead, the new text describes an experiment that does not exist with a reference to a nonexisting section.
This suggests that the model focused on the title and keywords of the manuscript (Table <a href="#tbl:manuscripts">1</a>) that were part of every prompt (Figure <a href="#fig:ai_revision">1</a>).
For example, it included the idea of “gene co-expression” analysis (a keyword) to identify “therapeutic targets” (another keyword) and replaced the mention of “sets of genes” in the original text with “clusters of genes” (closer to the keyword including “clustering”).
This was a poor model-based revision, indicating that the original paragraph may be too short or disconnected from the rest and could be merged with the next one (which describes follow-up and related experiments).</p>
<h4 id="discussion">Discussion</h4>
<p>In both the CCC and PhenoPLIER manuscripts, revisions to the discussion section appeared to be of high quality.
The model kept the correct format when necessary (e.g., using italics for gene symbols), maintained most of the citations, and improved the readability of the text in general.
Revisions for some paragraphs introduced minor mistakes that a human author could readily fix.</p>
<div id="fig:discussion:ccc" class="fignos">
<figure>
<img src="images/diffs/discussion/ccc-paragraph-01.svg" style="width:100.0%" alt="Figure 6: A paragraph in the Discussion section of CCC. Original text is on the left and suggested revision on the right." />
<figcaption aria-hidden="true"><span>Figure 6:</span> <strong>A paragraph in the Discussion section of CCC.</strong>
Original text is on the left and suggested revision on the right.</figcaption>
</figure>
</div>
<p>One paragraph of CCC discusses how not-only-linear correlation coefficients could potentially impact genetic studies of complex traits (Figure <a href="#fig:discussion:ccc">6</a>).
Although some minor changes could be added, we believe the revised text reads better than the original.
It is also interesting how the model understood the format of citations and built more complex structures from it.
For instance, the two articles referenced in lines 2 and 3 in the original text were correctly merged into a single citation block and separated with “;” in line 2 of the revised text.</p>
<h4 id="methods">Methods</h4>
<p>Prompts for the Methods section were the most challenging to design, especially when the sections included equations.
The prompt for Methods (Figure <a href="#fig:ai_revision">1</a>) is more focused in keeping the technical details, which was especially important for PhenoPLIER, whose Methods section contains paragraphs with several mathematical expressions.</p>
<div id="fig:methods:phenoplier" class="fignos">
<figure>
<img src="images/diffs/methods/phenoplier-paragraph-01.svg" style="width:100.0%" alt="Figure 7: A paragraph in the Methods section of PhenoPLIER. Original text is on the left and suggested revision on the right." />
<figcaption aria-hidden="true"><span>Figure 7:</span> <strong>A paragraph in the Methods section of PhenoPLIER.</strong>
Original text is on the left and suggested revision on the right.</figcaption>
</figure>
</div>
<p>We revised a paragraph in PhenoPLIER that contained two numbered equations (Figure <a href="#fig:methods:phenoplier">7</a>).
The model made very few changes, and all the equations, citations, and most of the original text were preserved.
However, we found it remarkable how the model identified a wrong reference to a mathematical symbol (line 8) and fixed it in the revision (line 7).
Indeed, the equation with the univariate model used by PrediXcan (lines 4-6 in the original) includes the <em>true</em> effect size <span class="math inline">\(\gamma_l\)</span> (<code>\gamma_l</code>) instead of the <em>estimated</em> one <span class="math inline">\(\hat{\gamma}_l\)</span> (<code>\hat{\gamma}_l</code>).</p>
<p>In PhenoPLIER, we found one large paragraph with several equations that the model failed to revise, although it performed relatively well in revising the rest of the section.
In CCC, the revision of this section was good overall, with some minor and easy-to-fix issues as in the other sections.</p>
<p>We also observed issues from revising one paragraph at a time without context.
For instance, in PhenoPLIER, one of the first paragraphs mentions the linear models used by S-PrediXcan and S-MultiXcan, without providing any equations or details.
These were presented in the following paragraphs, but since the model had not encountered that yet, it opted to add those equations immediately (in the correct Manubot/Markdown format).</p>
<div id="fig:methods:manubotai" class="fignos">
<figure>
<img src="images/diffs/methods/manubotai-paragraph-01.svg" style="width:100.0%" alt="Figure 8: A paragraph in the Methods section of ManubotAI. Original text is on the left and suggested revision on the right. The revision (right) contains a repeated set of sentences at the top that we removed to improve the clarity of the figure." />
<figcaption aria-hidden="true"><span>Figure 8:</span> <strong>A paragraph in the Methods section of ManubotAI.</strong>
Original text is on the left and suggested revision on the right.
The revision (right) contains a repeated set of sentences at the top that we removed to improve the clarity of the figure.</figcaption>
</figure>
</div>
<p>When revising the Methods sections of Manubot-AI (this manuscript), in some cases the model added novel sentences with wrong information.
For instance, for one paragraph, it added a formula (using the correct Manubot format) to presumably predict the cost of a revision run.
In another paragraph (Figure <a href="#fig:methods:manubotai">8</a>), it added new sentences saying that the model was <em>“trained on a corpus of scientific papers from the same field as the manuscript”</em> and that its suggested revisions resulted in a <em>“modified version of the manuscript that is ready for submission”</em>.
Although these are important future directions, neither accurately describes the present work.</p>
<h2 id="conclusions">Conclusions</h2>
<p>We implemented AI-based revision models into the Manubot publishing platform.
Writing academic papers can be time-consuming and challenging to read, so we sought to use technology to help researchers communicate their findings to the community.
We created a workflow that authors can easily trigger to suggest revisions.
This workflow uses GPT-3 models through the OpenAI API, generating a pull request of revisions that authors can review.
We set default parameters for GPT-3 models that work well for our use cases across different sections and manuscripts.
Users can also customize the revision by selecting specific sections or adjusting the model’s behavior to fit their needs and budget.
Although the evaluation of the revision tool is subjective, we found that many paragraphs were improved.
The AI model also highlighted certain paragraphs that were difficult to revise, which could be challenging for human readers too.</p>
<p>We designed section-specific prompts to guide the revision of text using GPT-3.
Surprisingly, in one Methods section, the model detected an error when referencing a symbol in an equation that had been overlooked by humans.
However, abstracts were more challenging for the model to revise, where revisions often removed background information about the research problem.
There are opportunities to improve the AI-based revisions, such as further refining prompts using few-shot learning <span class="citation" data-cites="S1Lim9f9">[<a href="#ref-S1Lim9f9" role="doc-biblioref">11</a>]</span> or fine-tuning the model using an additional corpus of academic writing focused on particularly challenging sections.
Fine-tuning using preprint-publication pairs <span class="citation" data-cites="WVt383GU">[<a href="#ref-WVt383GU" role="doc-biblioref">12</a>]</span> may help to identify sections or phrases likely to be changed during peer review.
Our approach used GPT-3 to process each paragraph of the text, but it lacked a contextual thread between queries, which mainly affected the Results and Methods sections.
Using chatbots that retain context, such as <a href="https://openai.com/blog/chatgpt">OpenAI’s ChatGPT</a>, could enable the revision of individual paragraphs while considering previously processed text.
Once an official API becomes available for ChatGPT, we plan to update our workflow to support this strategy.
Other open models, such as BLOOM <span class="citation" data-cites="I4d1F0yv">[<a href="#ref-I4d1F0yv" role="doc-biblioref">13</a>]</span>, GLM <span class="citation" data-cites="1GG6zItDi">[<a href="#ref-1GG6zItDi" role="doc-biblioref">14</a>]</span>, or OPT <span class="citation" data-cites="nEabcf28">[<a href="#ref-nEabcf28" role="doc-biblioref">15</a>]</span>, provide similar capabilities but lack the user-friendly OpenAI API.
Despite these limitations, we found that models captured the main ideas and generated a revision that often communicated the intended meaning more clearly and concisely.
It is important to note, however, that our assessment of performance in case studies was necessarily subjective, as there could be writing styles that are not widely shared across researchers.</p>
<p>The use of AI-assisted tools for scientific authoring is controversial <span class="citation" data-cites="1EAonKBXJ KJTJqmxc">[<a href="#ref-1EAonKBXJ" role="doc-biblioref">16</a>,<a href="#ref-KJTJqmxc" role="doc-biblioref">17</a>]</span>.
Questions arise concerning the originality and ownership of texts generated by these models.
For example, the International Conference on Machine Learning (ICML) has prohibited the submission of <em>“papers that include text generated from a large-scale language model (LLM)”</em> <span class="citation" data-cites="K58CKD6D">[<a href="#ref-K58CKD6D" role="doc-biblioref">18</a>]</span>, although editing tools for grammar and spelling correction are allowed.
Our work focuses on revising <em>existing</em> text written by a human author, similar to other tools such as <a href="https://www.grammarly.com">Grammarly</a>.
Despite the concerns, there are also significant opportunities.
Our work lays the foundation for a future in which humans and machines construct academic manuscripts.
Scientific articles need to adhere to a certain style, which can make the writing time-consuming and require a significant amount of effort to think about <em>how</em> to communicate a result or finding that has already been obtained.
As machines become increasingly capable of improving scholarly text, humans can focus more on <em>what</em> to communicate to others, rather than on <em>how</em> to write it.
This could lead to a more equitable and productive future for research, where scientists are only limited by their ideas and ability to conduct experiments to uncover the underlying organizing principles of ourselves and our environment.</p>
<h2 class="page_break_before" id="references">References</h2>
<!-- Explicitly insert bibliography here -->
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-F3iZfGUC" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline"><strong>A history of scientific &amp; technical periodicals: the origins and development of the scientific and technical press, 1665-1790</strong> <div class="csl-block">David A Kronick</div> <em>Scarecrow Press</em> (1976) <div class="csl-block">ISBN: 9780810808447</div></div>
</div>
<div id="ref-1HMhNrQq1" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline"><strong>The history of the peer-review process</strong> <div class="csl-block">Ray Spier</div> <em>Trends in Biotechnology</em> (2002-08) <a href="https://doi.org/d26d8b">https://doi.org/d26d8b</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/s0167-7799(02)01985-6">10.1016/s0167-7799(02)01985-6</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/12127284">12127284</a></div></div>
</div>
<div id="ref-19YWsShi0" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline"><strong>How to write a first-class paper</strong> <div class="csl-block">Virginia Gewin</div> <em>Nature</em> (2018-02-28) <a href="https://doi.org/ggh63n">https://doi.org/ggh63n</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/d41586-018-02404-4">10.1038/d41586-018-02404-4</a></div></div>
</div>
<div id="ref-xq1uEbPa" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline"><strong>Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models</strong> <div class="csl-block">Alex Tamkin, Miles Brundage, Jack Clark, Deep Ganguli</div> <em>arXiv</em> (2021-02-05) <a href="https://arxiv.org/abs/2102.02503">https://arxiv.org/abs/2102.02503</a></div>
</div>
<div id="ref-bYOaJHMe" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline"><strong>Language Models are Few-Shot Learners</strong> <div class="csl-block">Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, … Dario Amodei</div> <em>arXiv</em> (2020-07-24) <a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></div>
</div>
<div id="ref-YuJbg3zO" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline"><strong>Open collaborative writing with Manubot</strong> <div class="csl-block">Daniel S Himmelstein, Vincent Rubinetti, David R Slochower, Dongbo Hu, Venkat S Malladi, Casey S Greene, Anthony Gitter</div> <em>PLOS Computational Biology</em> (2019-06-24) <a href="https://doi.org/c7np">https://doi.org/c7np</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1371/journal.pcbi.1007128">10.1371/journal.pcbi.1007128</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/31233491">31233491</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6611653">PMC6611653</a></div></div>
</div>
<div id="ref-PZMP42Ak" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline"><strong>Opportunities and obstacles for deep learning in biology and medicine</strong> <div class="csl-block">Travers Ching, Daniel S Himmelstein, Brett K Beaulieu-Jones, Alexandr A Kalinin, Brian T Do, Gregory P Way, Enrico Ferrero, Paul-Michael Agapow, Michael Zietz, Michael M Hoffman, … Casey S Greene</div> <em>Journal of The Royal Society Interface</em> (2018-04) <a href="https://doi.org/gddkhn">https://doi.org/gddkhn</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1098/rsif.2017.0387">10.1098/rsif.2017.0387</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29618526">29618526</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5938574">PMC5938574</a></div></div>
</div>
<div id="ref-10gsAq0o" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline"><strong>An Open-Publishing Response to the COVID-19 Infodemic.</strong> <div class="csl-block">Halie M Rando, Simina M Boca, Lucy D'Agostino McGowan, Daniel S Himmelstein, Michael P Robson, Vincent Rubinetti, Ryan Velazquez, Casey S Greene, Anthony Gitter</div> <em>ArXiv</em> (2021-09-17) <a href="https://www.ncbi.nlm.nih.gov/pubmed/34545336">https://www.ncbi.nlm.nih.gov/pubmed/34545336</a> <div class="csl-block">PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/34545336">34545336</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8452106">PMC8452106</a></div></div>
</div>
<div id="ref-eirYTTyk" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline"><strong>An efficient not-only-linear correlation coefficient based on machine learning</strong> <div class="csl-block">Milton Pividori, Marylyn D Ritchie, Diego H Milone, Casey S Greene</div> <em>Cold Spring Harbor Laboratory</em> (2022-06-17) <a href="https://doi.org/gqcvbw">https://doi.org/gqcvbw</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2022.06.15.496326">10.1101/2022.06.15.496326</a></div></div>
</div>
<div id="ref-FEh1bD0A" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline"><strong>Projecting genetic associations through gene expression patterns highlights disease etiology and drug mechanisms</strong> <div class="csl-block">Milton Pividori, Sumei Lu, Binglan Li, Chun Su, Matthew E Johnson, Wei-Qi Wei, Qiping Feng, Bahram Namjou, Krzysztof Kiryluk, Iftikhar Kullo, … Casey S Greene</div> <em>Cold Spring Harbor Laboratory</em> (2021-07-06) <a href="https://doi.org/gk9g25">https://doi.org/gk9g25</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2021.07.05.450786">10.1101/2021.07.05.450786</a></div></div>
</div>
<div id="ref-S1Lim9f9" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline"><strong>Generalizing from a Few Examples</strong> <div class="csl-block">Yaqing Wang, Quanming Yao, James T Kwok, Lionel M Ni</div> <em>ACM Computing Surveys</em> (2020-06-12) <a href="https://doi.org/gg37m2">https://doi.org/gg37m2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/3386252">10.1145/3386252</a></div></div>
</div>
<div id="ref-WVt383GU" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline"><strong>Examining linguistic shifts between preprints and publications</strong> <div class="csl-block">David N Nicholson, Vincent Rubinetti, Dongbo Hu, Marvin Thielk, Lawrence E Hunter, Casey S Greene</div> <em>PLOS Biology</em> (2022-02-01) <a href="https://doi.org/gqqzn2">https://doi.org/gqqzn2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1371/journal.pbio.3001470">10.1371/journal.pbio.3001470</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/35104289">35104289</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8806061">PMC8806061</a></div></div>
</div>
<div id="ref-I4d1F0yv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline"><strong>BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</strong> <div class="csl-block">BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, … Thomas Wolf</div> <em>arXiv</em> (2022-12-13) <a href="https://arxiv.org/abs/2211.05100">https://arxiv.org/abs/2211.05100</a></div>
</div>
<div id="ref-1GG6zItDi" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline"><strong>GLM-130B: An Open Bilingual Pre-trained Model</strong> <div class="csl-block">Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, … Jie Tang</div> <em>arXiv</em> (2022-10-06) <a href="https://arxiv.org/abs/2210.02414">https://arxiv.org/abs/2210.02414</a></div>
</div>
<div id="ref-nEabcf28" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline"><strong>OPT: Open Pre-trained Transformer Language Models</strong> <div class="csl-block">Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, … Luke Zettlemoyer</div> <em>arXiv</em> (2022-06-22) <a href="https://arxiv.org/abs/2205.01068">https://arxiv.org/abs/2205.01068</a></div>
</div>
<div id="ref-1EAonKBXJ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline"><strong>Abstracts written by ChatGPT fool scientists</strong> <div class="csl-block">Holly Else</div> <em>Nature</em> (2023-01-12) <a href="https://doi.org/js2g">https://doi.org/js2g</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/d41586-023-00056-7">10.1038/d41586-023-00056-7</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/36635510">36635510</a></div></div>
</div>
<div id="ref-KJTJqmxc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline"><strong>ChatGPT listed as author on research papers: many scientists disapprove</strong> <div class="csl-block">Chris Stokel-Walker</div> <em>Nature</em> (2023-01-18) <a href="https://doi.org/grn72b">https://doi.org/grn72b</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/d41586-023-00107-z">10.1038/d41586-023-00107-z</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/36653617">36653617</a></div></div>
</div>
<div id="ref-K58CKD6D" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline"><strong>ICML 2023</strong> <a href="https://icml.cc/Conferences/2023/llm-policy">https://icml.cc/Conferences/2023/llm-policy</a></div>
</div>
</div>
<!-- default theme -->

<style>
  /* import google fonts */
  @import url("https://fonts.googleapis.com/css?family=Open+Sans:400,600,700");
  @import url("https://fonts.googleapis.com/css?family=Source+Code+Pro");

  /* -------------------------------------------------- */
  /* global */
  /* -------------------------------------------------- */

  /* all elements */
  * {
    /* force sans-serif font unless specified otherwise */
    font-family: "Open Sans", "Helvetica", sans-serif;

    /* prevent text inflation on some mobile browsers */
    -webkit-text-size-adjust: none !important;
    -moz-text-size-adjust: none !important;
    -o-text-size-adjust: none !important;
    text-size-adjust: none !important;
  }

  @media only screen {
    /* "page" element */
    body {
      position: relative;
      box-sizing: border-box;
      font-size: 12pt;
      line-height: 1.5;
      max-width: 8.5in;
      margin: 20px auto;
      padding: 40px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* "page" element */
    body {
      padding: 20px;
      margin: 0;
      border-radius: 0;
      border: none;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05) inset;
      background: none;
    }
  }

  /* -------------------------------------------------- */
  /* headings */
  /* -------------------------------------------------- */

  /* all headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    margin: 20px 0;
    padding: 0;
    font-weight: bold;
  }

  /* biggest heading */
  h1 {
    margin: 40px 0;
    text-align: center;
  }

  /* second biggest heading */
  h2 {
    margin-top: 30px;
    padding-bottom: 5px;
    border-bottom: solid 1px #bdbdbd;
  }

  /* heading font sizes */
  h1 {
    font-size: 2em;
  }
  h2 {
    font-size: 1.5em;
  }
  h3 {
    font-size: 1.35em;
  }
  h4 {
    font-size: 1.25em;
  }
  h5 {
    font-size: 1.15em;
  }
  h6 {
    font-size: 1em;
  }

  /* -------------------------------------------------- */
  /* manuscript header */
  /* -------------------------------------------------- */

  /* manuscript title */
  header > h1 {
    margin: 0;
  }

  /* manuscript title caption text (ie "automatically generated on") */
  header + p {
    text-align: center;
    margin-top: 10px;
  }

  /* -------------------------------------------------- */
  /* text elements */
  /* -------------------------------------------------- */

  /* links */
  a {
    color: #2196f3;
    overflow-wrap: break-word;
  }

  /* superscripts and subscripts */
  sub,
  sup {
    /* prevent from affecting line height */
    line-height: 0;
  }

  /* unordered and ordered lists*/
  ul,
  ol {
    padding-left: 20px;
  }

  /* class for styling text semibold */
  .semibold {
    font-weight: 600;
  }

  /* class for styling elements horizontally left aligned */
  .left {
    display: block;
    text-align: left;
    margin-left: auto;
    margin-right: 0;
    justify-content: left;
  }

  /* class for styling elements horizontally centered */
  .center {
    display: block;
    text-align: center;
    margin-left: auto;
    margin-right: auto;
    justify-content: center;
  }

  /* class for styling elements horizontally right aligned */
  .right {
    display: block;
    text-align: right;
    margin-left: 0;
    margin-right: auto;
    justify-content: right;
  }

  /* -------------------------------------------------- */
  /* section elements */
  /* -------------------------------------------------- */

  /* horizontal divider line */
  hr {
    border: none;
    height: 1px;
    background: #bdbdbd;
  }

  /* paragraphs, horizontal dividers, figures, tables, code */
  p,
  hr,
  figure,
  table,
  pre {
    /* treat all as "paragraphs", with consistent vertical margins */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* figures */
  /* -------------------------------------------------- */

  /* figure */
  figure {
    max-width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure caption */
  figcaption {
    padding: 0;
    padding-top: 10px;
  }

  /* figure image element */
  figure > img,
  figure > svg {
    max-width: 100%;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure auto-number */
  img + figcaption > span:first-of-type,
  svg + figcaption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* tables */
  /* -------------------------------------------------- */

  /* table */
  table {
    border-collapse: collapse;
    border-spacing: 0;
    width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* table cells */
  th,
  td {
    border: solid 1px #bdbdbd;
    padding: 10px;
    /* squash table if too wide for page by forcing line breaks */
    overflow-wrap: break-word;
    word-break: break-word;
  }

  /* header row and even rows */
  th,
  tr:nth-child(2n) {
    background-color: #fafafa;
  }

  /* odd rows */
  tr:nth-child(2n + 1) {
    background-color: #ffffff;
  }

  /* table caption */
  caption {
    text-align: left;
    padding: 0;
    padding-bottom: 10px;
  }

  /* table auto-number */
  table > caption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* code */
  /* -------------------------------------------------- */

  /* multi-line code block */
  pre {
    padding: 10px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
    break-inside: avoid;
    text-align: left;
  }

  /* inline code, ie code within normal text */
  :not(pre) > code {
    padding: 0 4px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
  }

  /* code text */
  /* apply all children, to reach syntax highlighting sub-elements */
  code,
  code * {
    /* force monospace font */
    font-family: "Source Code Pro", "Courier New", monospace;
  }

  /* -------------------------------------------------- */
  /* quotes */
  /* -------------------------------------------------- */

  /* quoted text */
  blockquote {
    margin: 0;
    padding: 0;
    border-left: 4px solid #bdbdbd;
    padding-left: 16px;
    break-inside: avoid;
  }

  /* -------------------------------------------------- */
  /* banners */
  /* -------------------------------------------------- */

  /* info banners */
  .banner {
    box-sizing: border-box;
    display: block;
    position: relative;
    width: 100%;
    margin-top: 20px;
    margin-bottom: 20px;
    padding: 20px;
    text-align: center;
  }

  /* paragraph in banner */
  .banner > p {
    margin: 0;
  }

  /* -------------------------------------------------- */
  /* highlight colors */
  /* -------------------------------------------------- */

  .white {
    background: #ffffff;
  }
  .lightgrey {
    background: #eeeeee;
  }
  .grey {
    background: #757575;
  }
  .darkgrey {
    background: #424242;
  }
  .black {
    background: #000000;
  }
  .lightred {
    background: #ffcdd2;
  }
  .lightyellow {
    background: #ffecb3;
  }
  .lightgreen {
    background: #dcedc8;
  }
  .lightblue {
    background: #e3f2fd;
  }
  .lightpurple {
    background: #f3e5f5;
  }
  .red {
    background: #f44336;
  }
  .orange {
    background: #ff9800;
  }
  .yellow {
    background: #ffeb3b;
  }
  .green {
    background: #4caf50;
  }
  .blue {
    background: #2196f3;
  }
  .purple {
    background: #9c27b0;
  }
  .white,
  .lightgrey,
  .lightred,
  .lightyellow,
  .lightgreen,
  .lightblue,
  .lightpurple,
  .orange,
  .yellow,
  .white a,
  .lightgrey a,
  .lightred a,
  .lightyellow a,
  .lightgreen a,
  .lightblue a,
  .lightpurple a,
  .orange a,
  .yellow a {
    color: #000000;
  }
  .grey,
  .darkgrey,
  .black,
  .red,
  .green,
  .blue,
  .purple,
  .grey a,
  .darkgrey a,
  .black a,
  .red a,
  .green a,
  .blue a,
  .purple a {
    color: #ffffff;
  }

  /* -------------------------------------------------- */
  /* buttons */
  /* -------------------------------------------------- */

  /* class for styling links like buttons */
  .button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    margin: 5px;
    padding: 10px 20px;
    font-size: 0.75em;
    font-weight: 600;
    text-transform: uppercase;
    text-decoration: none;
    letter-spacing: 1px;
    background: none;
    color: #2196f3;
    border: solid 1px #bdbdbd;
    border-radius: 5px;
  }

  /* buttons when hovered */
  .button:hover:not([disabled]),
  .icon_button:hover:not([disabled]) {
    cursor: pointer;
    background: #f5f5f5;
  }

  /* buttons when disabled */
  .button[disabled],
  .icon_button[disabled] {
    opacity: 0.35;
    pointer-events: none;
  }

  /* class for styling buttons containg only single icon */
  .icon_button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    text-decoration: none;
    margin: 0;
    padding: 0;
    background: none;
    border-radius: 5px;
    border: none;
    width: 20px;
    height: 20px;
    min-width: 20px;
    min-height: 20px;
  }

  /* icon button inner svg image */
  .icon_button > svg {
    height: 16px;
  }

  /* -------------------------------------------------- */
  /* icons */
  /* -------------------------------------------------- */

  /* class for styling icons inline with text */
  .inline_icon {
    height: 1em;
    position: relative;
    top: 0.125em;
  }

  /* -------------------------------------------------- */
  /* references */
  /* -------------------------------------------------- */

  .csl-entry {
    margin-top: 15px;
    margin-bottom: 15px;
  }

  /* -------------------------------------------------- */
  /* print control */
  /* -------------------------------------------------- */

  @media print {
    @page {
      /* suggested printing margin */
      margin: 0.5in;
    }

    /* document and "page" elements */
    html,
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
    }

    /* "page" element */
    body {
      font-size: 11pt !important;
      line-height: 1.35;
    }

    /* all headings */
    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin: 15px 0;
    }

    /* figures and tables */
    figure,
    table {
      font-size: 0.85em;
    }

    /* table cells */
    th,
    td {
      padding: 5px;
    }

    /* shrink font awesome icons */
    i.fas,
    i.fab,
    i.far,
    i.fal {
      transform: scale(0.85);
    }

    /* decrease banner margins */
    .banner {
      margin-top: 15px;
      margin-bottom: 15px;
      padding: 15px;
    }

    /* class for centering an element vertically on its own page */
    .page_center {
      margin: auto;
      width: 100%;
      height: 100%;
      display: flex;
      align-items: center;
      vertical-align: middle;
      break-before: page;
      break-after: page;
    }

    /* always insert a page break before the element */
    .page_break_before {
      break-before: page;
    }

    /* always insert a page break after the element */
    .page_break_after {
      break-after: page;
    }

    /* avoid page break before the element */
    .page_break_before_avoid {
      break-before: avoid;
    }

    /* avoid page break after the element */
    .page_break_after_avoid {
      break-after: avoid;
    }

    /* avoid page break inside the element */
    .page_break_inside_avoid {
      break-inside: avoid;
    }
  }

  /* -------------------------------------------------- */
  /* override pandoc css quirks */
  /* -------------------------------------------------- */

  .sourceCode {
    /* prevent unsightly overflow in wide code blocks */
    overflow: auto !important;
  }

  div.sourceCode {
    /* prevent background fill on top-most code block  container */
    background: none !important;
  }

  .sourceCode * {
    /* force consistent line spacing */
    line-height: 1.5 !important;
  }

  div.sourceCode {
    /* style code block margins same as <pre> element */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* tablenos */
  /* -------------------------------------------------- */

  /* tablenos wrapper */
  .tablenos {
    width: 100%;
    margin: 20px 0;
  }

  .tablenos > table {
    /* move margins from table to table_wrapper to allow margin collapsing */
    margin: 0;
  }

  @media only screen {
    /* tablenos wrapper */
    .tablenos {
      /* show scrollbar on tables if necessary to prevent overflow */
      overflow-x: auto !important;
    }

    .tablenos th,
    .tablenos td {
      overflow-wrap: unset !important;
      word-break: unset !important;
    }

    /* table in wrapper */
    .tablenos table,
    .tablenos table * {
      /* don't break table words */
      overflow-wrap: normal !important;
    }
  }
</style>
<!-- 
    Plugin Core

    Functions needed for and shared across all first-party plugins.
-->

<script>
  // get element that is target of hash (from link element or url)
  function getHashTarget(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector(`[id="${id}"]`);
    if (!target) return;

    // if figure or table, modify target to get expected element
    if (id.indexOf("fig:") === 0) target = target.querySelector("figure");
    if (id.indexOf("tbl:") === 0) target = target.querySelector("table");

    return target;
  }

  // get position/dimensions of element or viewport
  function getRectInView(element) {
    let rect = {};
    rect.left = 0;
    rect.top = 0;
    rect.right = document.documentElement.clientWidth;
    rect.bottom = document.documentElement.clientHeight;
    let style = {};

    if (element instanceof HTMLElement) {
      rect = element.getBoundingClientRect();
      style = window.getComputedStyle(element);
    }

    const margin = {};
    margin.left = parseFloat(style.marginLeftWidth) || 0;
    margin.top = parseFloat(style.marginTopWidth) || 0;
    margin.right = parseFloat(style.marginRightWidth) || 0;
    margin.bottom = parseFloat(style.marginBottomWidth) || 0;

    const border = {};
    border.left = parseFloat(style.borderLeftWidth) || 0;
    border.top = parseFloat(style.borderTopWidth) || 0;
    border.right = parseFloat(style.borderRightWidth) || 0;
    border.bottom = parseFloat(style.borderBottomWidth) || 0;

    const newRect = {};
    newRect.left = rect.left + margin.left + border.left;
    newRect.top = rect.top + margin.top + border.top;
    newRect.right = rect.right + margin.right + border.right;
    newRect.bottom = rect.bottom + margin.bottom + border.bottom;
    newRect.width = newRect.right - newRect.left;
    newRect.height = newRect.bottom - newRect.top;

    return newRect;
  }

  // get position of element relative to page
  function getRectInPage(element) {
    const rect = getRectInView(element);
    const body = getRectInView(document.body);

    const newRect = {};
    newRect.left = rect.left - body.left;
    newRect.top = rect.top - body.top;
    newRect.right = rect.right - body.left;
    newRect.bottom = rect.bottom - body.top;
    newRect.width = rect.width;
    newRect.height = rect.height;

    return newRect;
  }

  // get closest element before specified element that matches query
  function firstBefore(element, query) {
    while (element && element !== document.body && !element.matches(query))
      element = element.previousElementSibling || element.parentNode;

    return element;
  }

  // check if element is part of collapsed heading
  function isCollapsed(element) {
    while (element && element !== document.body) {
      if (element.dataset.collapsed === "true") return true;
      element = element.parentNode;
    }
    return false;
  }

  // expand any collapsed parent containers of element if necessary
  function expandElement(element) {
    if (isCollapsed(element)) {
      // accordion plugin
      const heading = firstBefore(element, "h2");
      if (heading) heading.click();
      // details/summary HTML element
      const summary = firstBefore(element, "summary");
      if (summary) summary.click();
    }
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);

    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // get list of elements after a start element up to element matching query
  function nextUntil(element, query, exclude) {
    const elements = [];
    while (((element = element.nextElementSibling), element)) {
      if (element.matches(query)) break;
      if (!element.matches(exclude)) elements.push(element);
    }
    return elements;
  }
</script>
<!--
  Accordion Plugin

  Allows sections of content under h2 headings to be collapsible.
-->

<script type="module">
  // whether to always start expanded ('false'), always start collapsed
  // ('true'), or start collapsed when screen small ('auto')
  const startCollapsed = "auto";

  // start script
  function start() {
    // run through each <h2> heading
    const headings = document.querySelectorAll("h2");
    for (const heading of headings) {
      addArrow(heading);

      // start expanded/collapsed based on option
      if (
        startCollapsed === "true" ||
        (startCollapsed === "auto" && isSmallScreen()) ||
        heading.dataset.collapsed === "true"
      )
        collapseHeading(heading);
      else expandElement(heading);
    }

    // attach hash change listener to window
    window.addEventListener("hashchange", onHashChange);
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) goToElement(target);
  }

  // add arrow to heading
  function addArrow(heading) {
    // add arrow button
    const arrow = document.createElement("button");
    arrow.innerHTML = document.querySelector(".icon_angle_down").innerHTML;
    arrow.classList.add("icon_button", "accordion_arrow");
    heading.insertBefore(arrow, heading.firstChild);

    // attach click listener to heading and button
    heading.addEventListener("click", onHeadingClick);
    arrow.addEventListener("click", onArrowClick);
  }

  // determine if on mobile-like device with small screen
  function isSmallScreen() {
    return Math.min(window.innerWidth, window.innerHeight) < 480;
  }

  // when <h2> heading is clicked
  function onHeadingClick(event) {
    // only collapse if <h2> itself is target of click (eg, user did
    // not click on anchor within <h2>)
    if (event.target === this) toggleCollapse(this);
  }

  // when arrow button is clicked
  function onArrowClick() {
    toggleCollapse(this.parentNode);
  }

  // collapse section if expanded, expand if collapsed
  function toggleCollapse(heading) {
    if (heading.dataset.collapsed === "false") collapseHeading(heading);
    else expandElement(heading);
  }

  // elements to exclude from collapse, such as table of contents panel,
  // hypothesis panel, etc
  const exclude = "#toc_panel, div.annotator-frame, #lightbox_overlay";

  // collapse section
  function collapseHeading(heading) {
    heading.setAttribute("data-collapsed", "true");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "true");
  }

  // expand section
  function expandElement(heading) {
    heading.setAttribute("data-collapsed", "false");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "false");
  }

  // get list of elements between this <h2> and next <h2> or <h1>
  // ("children" of the <h2> section)
  function getChildren(heading) {
    return nextUntil(heading, "h2, h1", exclude);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle down icon -->

<template class="icon_angle_down">
  <!-- modified from: https://fontawesome.com/icons/angle-down -->
  <svg width="16" height="16" viewBox="0 0 448 512">
    <path
      fill="currentColor"
      d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* accordion arrow button */
    .accordion_arrow {
      margin-right: 10px;
    }

    /* arrow icon when <h2> data-collapsed attribute true */
    h2[data-collapsed="true"] > .accordion_arrow > svg {
      transform: rotate(-90deg);
    }

    /* all elements (except <h2>'s) when data-collapsed attribute true */
    *:not(h2)[data-collapsed="true"] {
      display: none;
    }

    /* accordion arrow button when hovered and <h2>'s when hovered */
    .accordion_arrow:hover,
    h2[data-collapsed="true"]:hover,
    h2[data-collapsed="false"]:hover {
      cursor: pointer;
    }
  }

  /* always hide accordion arrow button on print */
  @media only print {
    .accordion_arrow {
      display: none;
    }
  }
</style>
<!--
  Anchors Plugin

  Adds an anchor next to each of a certain type of element that provides a
  human-readable url to that specific item/position in the document (e.g.
  "manuscript.html#abstract"). It also makes it such that scrolling out of view
  of a target removes its identifier from the url.
-->

<script type="module">
  // which types of elements to add anchors next to, in "document.querySelector"
  // format
  const typesQuery =
    'h1, h2, h3, div[id^="fig:"], div[id^="tbl:"], span[id^="eq:"]';

  // start script
  function start() {
    // add anchor to each element of specified types
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) addAnchor(element);

    // attach scroll listener to window
    window.addEventListener("scroll", onScroll);
  }

  // when window is scrolled
  function onScroll() {
    // if url has hash and user has scrolled out of view of hash
    // target, remove hash from url
    const tolerance = 100;
    const target = getHashTarget();
    if (target) {
      if (
        target.getBoundingClientRect().top > window.innerHeight + tolerance ||
        target.getBoundingClientRect().bottom < 0 - tolerance
      )
        history.pushState(null, null, " ");
    }
  }

  // add anchor to element
  function addAnchor(element) {
    let addTo; // element to add anchor button to

    // if figure or table, modify withId and addTo to get expected
    // elements
    if (element.id.indexOf("fig:") === 0) {
      addTo = element.querySelector("figcaption");
    } else if (element.id.indexOf("tbl:") === 0) {
      addTo = element.querySelector("caption");
    } else if (element.id.indexOf("eq:") === 0) {
      addTo = element.querySelector(".eqnos-number");
    }

    addTo = addTo || element;
    const id = element.id || null;

    // do not add anchor if element doesn't have assigned id.
    // id is generated by pandoc and is assumed to be unique and
    // human-readable
    if (!id) return;

    // create anchor button
    const anchor = document.createElement("a");
    anchor.innerHTML = document.querySelector(".icon_link").innerHTML;
    anchor.title = "Link to this part of the document";
    anchor.classList.add("icon_button", "anchor");
    anchor.dataset.ignore = "true";
    anchor.href = "#" + id;
    addTo.appendChild(anchor);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- link icon -->

<template class="icon_link">
  <!-- modified from: https://fontawesome.com/icons/link -->
  <svg width="16" height="16" viewBox="0 0 512 512">
    <path
      fill="currentColor"
      d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* anchor button */
    .anchor {
      opacity: 0;
      margin-left: 5px;
    }

    /* anchor buttons within <h2>'s */
    h2 .anchor {
      margin-left: 10px;
    }

    /* anchor buttons when hovered/focused and anything containing an anchor button when hovered */
    *:hover > .anchor,
    .anchor:hover,
    .anchor:focus {
      opacity: 1;
    }

    /* anchor button when hovered */
    .anchor:hover {
      cursor: pointer;
    }
  }

  /* always show anchor button on devices with no mouse/hover ability */
  @media (hover: none) {
    .anchor {
      opacity: 1;
    }
  }

  /* always hide anchor button on print */
  @media only print {
    .anchor {
      display: none;
    }
  }
</style>
<!-- 
    Attributes Plugin

    Allows arbitrary HTML attributes to be attached to (almost) any element.
    Place an HTML comment inside or next to the desired element with the content:
    $attribute="value"
-->

<script type="module">
  // start script
  function start() {
    // get list of comments in document
    const comments = findComments();

    for (const comment of comments)
      if (comment.parentElement)
        addAttributes(comment.parentElement, comment.nodeValue.trim());
  }

  // add html attributes to specified element based on string of
  // html attributes and values
  function addAttributes(element, text) {
    // regex's for finding attribute/value pairs in the format of
    // attribute="value" or attribute='value
    const regex2 = /\$([a-zA-Z\-]+)?=\"(.+?)\"/;
    const regex1 = /\$([a-zA-Z\-]+)?=\'(.+?)\'/;

    // loop through attribute/value pairs
    let match;
    while ((match = text.match(regex2) || text.match(regex1))) {
      // get attribute and value from regex capture groups
      let attribute = match[1];
      let value = match[2];

      // remove from string
      text = text.substring(match.index + match[0].length);

      if (!attribute || !value) break;

      // set attribute of parent element
      try {
        element.setAttribute(attribute, value);
      } catch (error) {
        console.log(error);
      }

      // special case for colspan
      if (attribute === "colspan") removeTableCells(element, value);
    }
  }

  // get list of comment elements in document
  function findComments() {
    const comments = [];

    // iterate over comment nodes in document
    function acceptNode(node) {
      return NodeFilter.FILTER_ACCEPT;
    }
    const iterator = document.createNodeIterator(
      document.body,
      NodeFilter.SHOW_COMMENT,
      acceptNode
    );
    let node;
    while ((node = iterator.nextNode())) comments.push(node);

    return comments;
  }

  // remove certain number of cells after specified cell
  function removeTableCells(cell, number) {
    number = parseInt(number);
    if (!number) return;

    // remove elements
    for (; number > 1; number--) {
      if (cell.nextElementSibling) cell.nextElementSibling.remove();
    }
  }

  // start script on DOMContentLoaded instead of load to ensure this plugins
  // runs before other plugins
  window.addEventListener("DOMContentLoaded", start);
</script>
<!--
  Jump to First Plugin

  Adds a button next to each reference entry, figure, and table that jumps the
  page to the first occurrence of a link to that item in the manuscript.
-->

<script type="module">
  // whether to add buttons next to reference entries
  const references = "true";
  // whether to add buttons next to figures
  const figures = "true";
  // whether to add buttons next to tables
  const tables = "true";

  // start script
  function start() {
    if (references !== "false")
      makeButtons(`div[id^="ref-"]`, ".csl-left-margin", "reference");
    if (figures !== "false")
      makeButtons(`div[id^="fig:"]`, "figcaption", "figure");
    if (tables !== "false") makeButtons(`div[id^="tbl:"]`, "caption", "table");
  }

  // when jump button clicked
  function onButtonClick() {
    const first = getFirstOccurrence(this.dataset.id);
    if (!first) return;

    // update url hash so navigating "back" in history will return user to button
    window.location.hash = this.dataset.id;
    // scroll to link
    const timeout = function () {
      goToElement(first, window.innerHeight * 0.5);
    };
    window.setTimeout(timeout, 0);
  }

  // get first occurrence of link to item in document
  function getFirstOccurrence(id) {
    let query = "a";
    query += '[href="#' + id + '"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelector(query);
  }

  // add button next to each reference entry, figure, or table
  function makeButtons(query, containerQuery, subject) {
    const elements = document.querySelectorAll(query);
    for (const element of elements) {
      const id = element.id;
      const buttonContainer = element.querySelector(containerQuery);
      const first = getFirstOccurrence(id);

      // if can't find link to reference or place to put button, ignore
      if (!first || !buttonContainer) continue;

      // make jump button
      let button = document.createElement("button");
      button.classList.add("icon_button", "jump_arrow");
      button.title = `Jump to the first occurrence of this ${subject} in the document`;
      const icon = document.querySelector(".icon_angle_double_up");
      button.innerHTML = icon.innerHTML;
      button.dataset.id = id;
      button.dataset.ignore = "true";
      button.addEventListener("click", onButtonClick);
      buttonContainer.prepend(button);
    }
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle double up icon -->

<template class="icon_angle_double_up">
  <!-- modified from: https://fontawesome.com/icons/angle-double-up -->
  <svg width="16" height="16" viewBox="0 0 320 512">
    <path
      fill="currentColor"
      d="M177 255.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 351.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 425.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1zm-34-192L7 199.7c-9.4 9.4-9.4 24.6 0 33.9l22.6 22.6c9.4 9.4 24.6 9.4 33.9 0l96.4-96.4 96.4 96.4c9.4 9.4 24.6 9.4 33.9 0l22.6-22.6c9.4-9.4 9.4-24.6 0-33.9l-136-136c-9.2-9.4-24.4-9.4-33.8 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* jump button */
    .jump_arrow {
      position: relative;
      top: 0.125em;
      margin-right: 5px;
    }
  }

  /* always hide jump button on print */
  @media only print {
    .jump_arrow {
      display: none;
    }
  }
</style>
<!-- 
    Lightbox Plugin

    Makes it such that when a user clicks on an image, the image fills the
    screen and the user can pan/drag/zoom the image and navigate between other
    images in the document.
-->

<script type="module">
  // list of possible zoom/scale factors
  const zooms =
    "0.1, 0.25, 0.333333, 0.5, 0.666666, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.5, 3, 3.5, 4, 5, 6, 7, 8";
  // whether to fit image to view ('fit'), display at 100% and shrink if
  // necessary ('shrink'), or always display at 100% ('100')
  const defaultZoom = "fit";
  // whether to zoom in/out toward center of view ('true') or mouse ('false')
  const centerZoom = "false";

  // start script
  function start() {
    // run through each <img> element
    const imgs = document.querySelectorAll("figure > img");
    let count = 1;
    for (const img of imgs) {
      img.classList.add("lightbox_document_img");
      img.dataset.number = count;
      img.dataset.total = imgs.length;
      img.addEventListener("click", openLightbox);
      count++;
    }

    // attach mouse and key listeners to window
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("keyup", onKeyUp);
  }

  // when mouse is moved anywhere in window
  function onWindowMouseMove(event) {
    window.mouseX = event.clientX;
    window.mouseY = event.clientY;
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("lightbox_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("lightbox_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeLightbox();
        break;
    }
  }

  // open lightbox
  function openLightbox() {
    const lightbox = makeLightbox(this);
    if (!lightbox) return;

    blurBody(lightbox);
    document.body.appendChild(lightbox);
  }

  // make lightbox
  function makeLightbox(img) {
    // delete lightbox if it exists, start fresh
    closeLightbox();

    // create screen overlay containing lightbox
    const overlay = document.createElement("div");
    overlay.id = "lightbox_overlay";

    // create image info boxes
    const numberInfo = document.createElement("div");
    const zoomInfo = document.createElement("div");
    numberInfo.id = "lightbox_number_info";
    zoomInfo.id = "lightbox_zoom_info";

    // create container for image
    const imageContainer = document.createElement("div");
    imageContainer.id = "lightbox_image_container";
    const lightboxImg = makeLightboxImg(
      img,
      imageContainer,
      numberInfo,
      zoomInfo
    );
    imageContainer.appendChild(lightboxImg);

    // create bottom container for caption and navigation buttons
    const bottomContainer = document.createElement("div");
    bottomContainer.id = "lightbox_bottom_container";
    const caption = makeCaption(img);
    const prevButton = makePrevButton(img);
    const nextButton = makeNextButton(img);
    bottomContainer.appendChild(prevButton);
    bottomContainer.appendChild(caption);
    bottomContainer.appendChild(nextButton);

    // attach top middle and bottom to overlay
    overlay.appendChild(numberInfo);
    overlay.appendChild(zoomInfo);
    overlay.appendChild(imageContainer);
    overlay.appendChild(bottomContainer);

    return overlay;
  }

  // make <img> object that is intuitively draggable and zoomable
  function makeLightboxImg(sourceImg, container, numberInfoBox, zoomInfoBox) {
    // create copy of source <img>
    const img = sourceImg.cloneNode(true);
    img.classList.remove("lightbox_document_img");
    img.removeAttribute("id");
    img.removeAttribute("width");
    img.removeAttribute("height");
    img.style.position = "unset";
    img.style.margin = "0";
    img.style.padding = "0";
    img.style.width = "";
    img.style.height = "";
    img.style.minWidth = "";
    img.style.minHeight = "";
    img.style.maxWidth = "";
    img.style.maxHeight = "";
    img.id = "lightbox_img";

    // build sorted list of zoomSteps
    const zoomSteps = zooms.split(/[^0-9.]/).map((step) => parseFloat(step));
    zoomSteps.sort((a, b) => a - b);

    // <img> object property variables
    let zoom = 1;
    let translateX = 0;
    let translateY = 0;
    let clickMouseX = undefined;
    let clickMouseY = undefined;
    let clickTranslateX = undefined;
    let clickTranslateY = undefined;

    updateNumberInfo();

    // update image numbers displayed in info box
    function updateNumberInfo() {
      numberInfoBox.innerHTML =
        sourceImg.dataset.number + " of " + sourceImg.dataset.total;
    }

    // update zoom displayed in info box
    function updateZoomInfo() {
      let zoomInfo = zoom * 100;
      if (!Number.isInteger(zoomInfo)) zoomInfo = zoomInfo.toFixed(2);
      zoomInfoBox.innerHTML = zoomInfo + "%";
    }

    // move to closest zoom step above current zoom
    const zoomIn = function () {
      for (const zoomStep of zoomSteps) {
        if (zoomStep > zoom) {
          zoom = zoomStep;
          break;
        }
      }
      updateTransform();
    };

    // move to closest zoom step above current zoom
    const zoomOut = function () {
      zoomSteps.reverse();
      for (const zoomStep of zoomSteps) {
        if (zoomStep < zoom) {
          zoom = zoomStep;
          break;
        }
      }
      zoomSteps.reverse();

      updateTransform();
    };

    // update display of <img> based on scale/translate properties
    const updateTransform = function () {
      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      // get new width/height after scale
      const rect = img.getBoundingClientRect();
      // limit translate
      translateX = Math.max(translateX, -rect.width / 2);
      translateX = Math.min(translateX, rect.width / 2);
      translateY = Math.max(translateY, -rect.height / 2);
      translateY = Math.min(translateY, rect.height / 2);

      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      updateZoomInfo();
    };

    // fit <img> to container
    const fit = function () {
      // no x/y offset, 100% zoom by default
      translateX = 0;
      translateY = 0;
      zoom = 1;

      // widths of <img> and container
      const imgWidth = img.naturalWidth;
      const imgHeight = img.naturalHeight;
      const containerWidth = parseFloat(
        window.getComputedStyle(container).width
      );
      const containerHeight = parseFloat(
        window.getComputedStyle(container).height
      );

      // how much zooming is needed to fit <img> to container
      const xRatio = imgWidth / containerWidth;
      const yRatio = imgHeight / containerHeight;
      const maxRatio = Math.max(xRatio, yRatio);
      const newZoom = 1 / maxRatio;

      // fit <img> to container according to option
      if (defaultZoom === "shrink") {
        if (maxRatio > 1) zoom = newZoom;
      } else if (defaultZoom === "fit") zoom = newZoom;

      updateTransform();
    };

    // when mouse wheel is rolled anywhere in container
    const onContainerWheel = function (event) {
      if (!event) return;

      // let ctrl + mouse wheel to zoom behave as normal
      if (event.ctrlKey) return;

      // prevent normal scroll behavior
      event.preventDefault();
      event.stopPropagation();

      // point around which to scale img
      const viewRect = container.getBoundingClientRect();
      const viewX = (viewRect.left + viewRect.right) / 2;
      const viewY = (viewRect.top + viewRect.bottom) / 2;
      const originX = centerZoom === "true" ? viewX : mouseX;
      const originY = centerZoom === "true" ? viewY : mouseY;

      // get point on image under origin
      const oldRect = img.getBoundingClientRect();
      const oldPercentX = (originX - oldRect.left) / oldRect.width;
      const oldPercentY = (originY - oldRect.top) / oldRect.height;

      // increment/decrement zoom
      if (event.deltaY < 0) zoomIn();
      if (event.deltaY > 0) zoomOut();

      // get offset between previous image point and origin
      const newRect = img.getBoundingClientRect();
      const offsetX = originX - (newRect.left + newRect.width * oldPercentX);
      const offsetY = originY - (newRect.top + newRect.height * oldPercentY);

      // translate image to keep image point under origin
      translateX += offsetX;
      translateY += offsetY;

      // perform translate
      updateTransform();
    };

    // when container is clicked
    function onContainerClick(event) {
      // if container itself is target of click, and not other
      // element above it
      if (event.target === this) closeLightbox();
    }

    // when mouse button is pressed on image
    const onImageMouseDown = function (event) {
      // store original mouse position relative to image
      clickMouseX = window.mouseX;
      clickMouseY = window.mouseY;
      clickTranslateX = translateX;
      clickTranslateY = translateY;
      event.stopPropagation();
      event.preventDefault();
    };

    // when mouse button is released anywhere in window
    const onWindowMouseUp = function (event) {
      // reset original mouse position
      clickMouseX = undefined;
      clickMouseY = undefined;
      clickTranslateX = undefined;
      clickTranslateY = undefined;

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mouseup", onWindowMouseUp);
    };

    // when mouse is moved anywhere in window
    const onWindowMouseMove = function (event) {
      if (
        clickMouseX === undefined ||
        clickMouseY === undefined ||
        clickTranslateX === undefined ||
        clickTranslateY === undefined
      )
        return;

      // offset image based on original and current mouse position
      translateX = clickTranslateX + window.mouseX - clickMouseX;
      translateY = clickTranslateY + window.mouseY - clickMouseY;
      updateTransform();
      event.preventDefault();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mousemove", onWindowMouseMove);
    };

    // when window is resized
    const onWindowResize = function (event) {
      fit();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("resize", onWindowResize);
    };

    // attach the necessary event listeners
    img.addEventListener("dblclick", fit);
    img.addEventListener("mousedown", onImageMouseDown);
    container.addEventListener("wheel", onContainerWheel);
    container.addEventListener("mousedown", onContainerClick);
    container.addEventListener("touchstart", onContainerClick);
    window.addEventListener("mouseup", onWindowMouseUp);
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("resize", onWindowResize);

    // run fit() after lightbox atttached to document and <img> Loaded
    // so needed container and img dimensions available
    img.addEventListener("load", fit);

    return img;
  }

  // make caption
  function makeCaption(img) {
    const caption = document.createElement("div");
    caption.id = "lightbox_caption";
    const captionSource = img.nextElementSibling;
    if (captionSource.tagName.toLowerCase() === "figcaption") {
      const captionCopy = makeCopy(captionSource);
      caption.innerHTML = captionCopy.innerHTML;
    }

    caption.addEventListener("touchstart", function (event) {
      event.stopPropagation();
    });

    return caption;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // make button to jump to previous image in document
  function makePrevButton(img) {
    const prevButton = document.createElement("button");
    prevButton.id = "lightbox_prev_button";
    prevButton.title = "Jump to the previous image in the document [←]";
    prevButton.classList.add("icon_button", "lightbox_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;

    // attach click listeners to button
    prevButton.addEventListener("click", function () {
      getPrevImg(img).click();
    });

    return prevButton;
  }

  // make button to jump to next image in document
  function makeNextButton(img) {
    const nextButton = document.createElement("button");
    nextButton.id = "lightbox_next_button";
    nextButton.title = "Jump to the next image in the document [→]";
    nextButton.classList.add("icon_button", "lightbox_button");
    nextButton.innerHTML = document.querySelector(
      ".icon_caret_right"
    ).innerHTML;

    // attach click listeners to button
    nextButton.addEventListener("click", function () {
      getNextImg(img).click();
    });

    return nextButton;
  }

  // get previous image in document
  function getPrevImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if < 1
    if (index - 1 >= 0) index--;
    else index = imgs.length - 1;
    return imgs[index];
  }

  // get next image in document
  function getNextImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if > total
    if (index + 1 <= imgs.length - 1) index++;
    else index = 0;
    return imgs[index];
  }

  // close lightbox
  function closeLightbox() {
    focusBody();

    const lightbox = document.getElementById("lightbox_overlay");
    if (lightbox) lightbox.remove();
  }

  // make all elements behind lightbox non-focusable
  function blurBody(overlay) {
    const all = document.querySelectorAll("*");
    for (const element of all) element.tabIndex = -1;
    document.body.classList.add("body_no_scroll");
  }

  // make all elements focusable again
  function focusBody() {
    const all = document.querySelectorAll("*");
    for (const element of all) element.removeAttribute("tabIndex");
    document.body.classList.remove("body_no_scroll");
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* regular <img> in document when hovered */
    img.lightbox_document_img:hover {
      cursor: pointer;
    }

    .body_no_scroll {
      overflow: hidden !important;
    }

    /* screen overlay */
    #lightbox_overlay {
      display: flex;
      flex-direction: column;
      position: fixed;
      left: 0;
      top: 0;
      right: 0;
      bottom: 0;
      background: rgba(0, 0, 0, 0.75);
      z-index: 3;
    }

    /* middle area containing lightbox image */
    #lightbox_image_container {
      flex-grow: 1;
      display: flex;
      justify-content: center;
      align-items: center;
      overflow: hidden;
      position: relative;
      padding: 20px;
    }

    /* bottom area containing caption */
    #lightbox_bottom_container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100px;
      min-height: 100px;
      max-height: 100px;
      background: rgba(0, 0, 0, 0.5);
    }

    /* image number info text box */
    #lightbox_number_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      left: 2px;
      top: 0;
      z-index: 4;
    }

    /* zoom info text box */
    #lightbox_zoom_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      right: 2px;
      top: 0;
      z-index: 4;
    }

    /* copy of image caption */
    #lightbox_caption {
      box-sizing: border-box;
      display: inline-block;
      width: 100%;
      max-height: 100%;
      padding: 10px 0;
      text-align: center;
      overflow-y: auto;
      color: #ffffff;
    }

    /* navigation previous/next button */
    .lightbox_button {
      width: 100px;
      height: 100%;
      min-width: 100px;
      min-height: 100%;
      color: #ffffff;
    }

    /* navigation previous/next button when hovered */
    .lightbox_button:hover {
      background: none !important;
    }

    /* navigation button icon */
    .lightbox_button > svg {
      height: 25px;
    }

    /* figure auto-number */
    #lightbox_caption > span:first-of-type {
      font-weight: bold;
      margin-right: 5px;
    }

    /* lightbox image when hovered */
    #lightbox_img:hover {
      cursor: grab;
    }

    /* lightbox image when grabbed */
    #lightbox_img:active {
      cursor: grabbing;
    }
  }

  /* when on screen < 480px wide */
  @media only screen and (max-width: 480px) {
    /* make navigation buttons skinnier on small screens to make more room for caption text */
    .lightbox_button {
      width: 50px;
      min-width: 50px;
    }
  }

  /* always hide lightbox on print */
  @media only print {
    #lightbox_overlay {
      display: none;
    }
  }
</style>
<!-- 
  Link Highlight Plugin

  Makes it such that when a user hovers or focuses a link, other links that have
  the same target will be highlighted. It also makes it such that when clicking
  a link, the target of the link (eg reference, figure, table) is briefly
  highlighted.
-->

<script type="module">
  // whether to also highlight links that go to external urls
  const externalLinks = "false";
  // whether user must click off to unhighlight instead of just
  // un-hovering
  const clickUnhighlight = "false";
  // whether to also highlight links that are unique
  const highlightUnique = "true";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach mouse and focus listeners to link
      link.addEventListener("mouseenter", onLinkFocus);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("mouseleave", onLinkUnhover);
    }

    // attach click and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("hashchange", onHashChange);

    // run hash change on window load in case user has navigated
    // directly to hash
    onHashChange();
  }

  // when link is focused (tabbed to) or hovered
  function onLinkFocus() {
    highlight(this);
  }

  // when link is unhovered
  function onLinkUnhover() {
    if (clickUnhighlight !== "true") unhighlightAll();
  }

  // when the mouse is clicked anywhere in window
  function onClick(event) {
    unhighlightAll();
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) glowElement(target);
  }

  // start glow sequence on an element
  function glowElement(element) {
    const startGlow = function () {
      onGlowEnd();
      element.dataset.glow = "true";
      element.addEventListener("animationend", onGlowEnd);
    };
    const onGlowEnd = function () {
      element.removeAttribute("data-glow");
      element.removeEventListener("animationend", onGlowEnd);
    };
    startGlow();
  }

  // highlight link and all others with same target
  function highlight(link) {
    // force unhighlight all to start fresh
    unhighlightAll();

    // get links with same target
    if (!link) return;
    const sameLinks = getSameLinks(link);

    // if link unique and option is off, exit and don't highlight
    if (sameLinks.length <= 1 && highlightUnique !== "true") return;

    // highlight all same links, and "select" (special highlight) this
    // one
    for (const sameLink of sameLinks) {
      if (sameLink === link) sameLink.setAttribute("data-selected", "true");
      else sameLink.setAttribute("data-highlighted", "true");
    }
  }

  // unhighlight all links
  function unhighlightAll() {
    const links = getLinks();
    for (const link of links) {
      link.setAttribute("data-selected", "false");
      link.setAttribute("data-highlighted", "false");
    }
  }

  // get links with same target
  function getSameLinks(link) {
    const results = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        results.push(otherLink);
    }
    return results;
  }

  // get all links of types we wish to handle
  function getLinks() {
    let query = "a";
    if (externalLinks !== "true") query += '[href^="#"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelectorAll(query);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<style>
  @media only screen {
    /* anything with data-highlighted attribute true */
    [data-highlighted="true"] {
      background: #ffeb3b;
    }

    /* anything with data-selected attribute true */
    [data-selected="true"] {
      background: #ff8a65 !important;
    }

    /* animation definition for glow */
    @keyframes highlight_glow {
      0% {
        background: none;
      }
      10% {
        background: #bbdefb;
      }
      100% {
        background: none;
      }
    }

    /* anything with data-glow attribute true */
    [data-glow="true"] {
      animation: highlight_glow 2s;
    }
  }
</style>
<!--
  Table of Contents Plugin

  Provides a "table of contents" (toc) panel on the side of the document that
  allows the user to conveniently navigate between sections of the document.
-->

<script type="module">
  // which types of elements to add links for, in "document.querySelector" format
  const typesQuery = "h1, h2, h3";
  // whether toc starts open. use 'true' or 'false', or 'auto' to
  // use 'true' behavior when screen wide enough and 'false' when not
  const startOpen = "false";
  // whether toc closes when clicking on toc link. use 'true' or
  // 'false', or 'auto' to use 'false' behavior when screen wide
  // enough and 'true' when not
  const clickClose = "auto";
  // if list item is more than this many characters, text will be
  // truncated
  const charLimit = "50";
  // whether or not to show bullets next to each toc item
  const bullets = "false";

  // start script
  function start() {
    // make toc panel and populate with entries (links to document
    // sections)
    const panel = makePanel();
    if (!panel) return;
    makeEntries(panel);
    // attach panel to document after making entries, so 'toc' heading
    // in panel isn't included in toc
    document.body.insertBefore(panel, document.body.firstChild);

    // initial panel state
    if (startOpen === "true" || (startOpen === "auto" && !isSmallScreen()))
      openPanel();
    else closePanel();

    // attach click, scroll, and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("scroll", onScroll);
    window.addEventListener("hashchange", onScroll);
    window.addEventListener("keyup", onKeyUp);
    onScroll();

    // add class to push document body down out of way of toc button
    document.body.classList.add("toc_body_nudge");
  }

  // determine if screen wide enough to fit toc panel
  function isSmallScreen() {
    // in default theme:
    // 816px = 8.5in = width of "page" (<body>) element
    // 260px = min width of toc panel (*2 for both sides of <body>)
    return window.innerWidth < 816 + 260 * 2;
  }

  // when mouse is clicked anywhere in window
  function onClick() {
    if (isSmallScreen()) closePanel();
  }

  // when window is scrolled or hash changed
  function onScroll() {
    highlightViewed();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    // close on esc
    if (event.key === "Escape") closePanel();
  }

  // find entry of currently viewed document section in toc and highlight
  function highlightViewed() {
    const firstId = getFirstInView(typesQuery);

    // get toc entries (links), unhighlight all, then highlight viewed
    const list = document.getElementById("toc_list");
    if (!firstId || !list) return;
    const links = list.querySelectorAll("a");
    for (const link of links) link.dataset.viewing = "false";
    const link = list.querySelector('a[href="#' + firstId + '"]');
    if (!link) return;
    link.dataset.viewing = "true";
  }

  // get first or previous toc listed element in top half of view
  function getFirstInView(query) {
    // get all elements matching query and with id
    const elements = document.querySelectorAll(query);
    const elementsWithIds = [];
    for (const element of elements) {
      if (element.id) elementsWithIds.push(element);
    }

    // get first or previous element in top half of view
    for (let i = 0; i < elementsWithIds.length; i++) {
      const element = elementsWithIds[i];
      const prevElement = elementsWithIds[Math.max(0, i - 1)];
      if (element.getBoundingClientRect().top >= 0) {
        if (element.getBoundingClientRect().top < window.innerHeight / 2)
          return element.id;
        else return prevElement.id;
      }
    }
  }

  // make panel
  function makePanel() {
    // create panel
    const panel = document.createElement("div");
    panel.id = "toc_panel";
    if (bullets === "true") panel.dataset.bullets = "true";

    // create header
    const header = document.createElement("div");
    header.id = "toc_header";

    // create toc button
    const button = document.createElement("button");
    button.id = "toc_button";
    button.innerHTML = document.querySelector(".icon_th_list").innerHTML;
    button.title = "Table of Contents";
    button.classList.add("icon_button");

    // create header text
    const text = document.createElement("h4");
    text.innerHTML = "Table of Contents";

    // create container for toc list
    const list = document.createElement("div");
    list.id = "toc_list";

    // attach click listeners
    panel.addEventListener("click", onPanelClick);
    header.addEventListener("click", onHeaderClick);
    button.addEventListener("click", onButtonClick);

    // attach elements
    header.appendChild(button);
    header.appendChild(text);
    panel.appendChild(header);
    panel.appendChild(list);

    return panel;
  }

  // create toc entries (links) to each element of the specified types
  function makeEntries(panel) {
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) {
      // do not add link if element doesn't have assigned id
      if (!element.id) continue;

      // create link/list item
      const link = document.createElement("a");
      link.classList.add("toc_link");
      switch (element.tagName.toLowerCase()) {
        case "h1":
          link.dataset.level = "1";
          break;
        case "h2":
          link.dataset.level = "2";
          break;
        case "h3":
          link.dataset.level = "3";
          break;
        case "h4":
          link.dataset.level = "4";
          break;
      }
      link.title = element.innerText;
      let text = element.innerText;
      if (text.length > charLimit) text = text.slice(0, charLimit) + "...";
      link.innerHTML = text;
      link.href = "#" + element.id;
      link.addEventListener("click", onLinkClick);

      // attach link
      panel.querySelector("#toc_list").appendChild(link);
    }
  }

  // when panel is clicked
  function onPanelClick(event) {
    // stop click from propagating to window/document and closing panel
    event.stopPropagation();
  }

  // when header itself is clicked
  function onHeaderClick(event) {
    togglePanel();
  }

  // when button is clicked
  function onButtonClick(event) {
    togglePanel();
    // stop header underneath button from also being clicked
    event.stopPropagation();
  }

  // when link is clicked
  function onLinkClick(event) {
    if (clickClose === "true" || (clickClose === "auto" && isSmallScreen()))
      closePanel();
    else openPanel();
  }

  // open panel if closed, close if opened
  function togglePanel() {
    const panel = document.getElementById("toc_panel");
    if (!panel) return;

    if (panel.dataset.open === "true") closePanel();
    else openPanel();
  }

  // open panel
  function openPanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "true";
  }

  // close panel
  function closePanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "false";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- th list icon -->

<template class="icon_th_list">
  <!-- modified from: https://fontawesome.com/icons/th-list -->
  <svg width="16" height="16" viewBox="0 0 512 512" tabindex="-1">
    <path
      fill="currentColor"
      d="M96 96c0 26.51-21.49 48-48 48S0 122.51 0 96s21.49-48 48-48 48 21.49 48 48zM48 208c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm0 160c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm96-236h352c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* toc panel */
    #toc_panel {
      box-sizing: border-box;
      position: fixed;
      top: 0;
      left: 0;
      background: #ffffff;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      z-index: 2;
    }

    /* toc panel when closed */
    #toc_panel[data-open="false"] {
      min-width: 60px;
      width: 60px;
      height: 60px;
      border-right: solid 1px #bdbdbd;
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc panel when open */
    #toc_panel[data-open="true"] {
      min-width: 260px;
      max-width: 480px;
      /* keep panel edge consistent distance away from "page" edge */
      width: calc(((100vw - 8.5in) / 2) - 30px - 40px);
      bottom: 0;
      border-right: solid 1px #bdbdbd;
    }

    /* toc panel header */
    #toc_header {
      box-sizing: border-box;
      display: flex;
      flex-direction: row;
      align-items: center;
      height: 60px;
      margin: 0;
      padding: 20px;
    }

    /* toc panel header when hovered */
    #toc_header:hover {
      cursor: pointer;
    }

    /* toc panel header when panel open */
    #toc_panel[data-open="true"] > #toc_header {
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc open/close header button */
    #toc_button {
      margin-right: 20px;
    }

    /* hide toc list and header text when closed */
    #toc_panel[data-open="false"] > #toc_header > *:not(#toc_button),
    #toc_panel[data-open="false"] > #toc_list {
      display: none;
    }

    /* toc list of entries */
    #toc_list {
      box-sizing: border-box;
      width: 100%;
      padding: 20px;
      position: absolute;
      top: calc(60px + 1px);
      bottom: 0;
      overflow: auto;
    }

    /* toc entry, link to section in document */
    .toc_link {
      display: block;
      padding: 5px;
      position: relative;
      font-weight: 600;
      text-decoration: none;
    }

    /* toc entry when hovered or when "viewed" */
    .toc_link:hover,
    .toc_link[data-viewing="true"] {
      background: #f5f5f5;
    }

    /* toc entry, level 1 indentation */
    .toc_link[data-level="1"] {
      margin-left: 0;
    }

    /* toc entry, level 2 indentation */
    .toc_link[data-level="2"] {
      margin-left: 20px;
    }

    /* toc entry, level 3 indentation */
    .toc_link[data-level="3"] {
      margin-left: 40px;
    }

    /* toc entry, level 4 indentation */
    .toc_link[data-level="4"] {
      margin-left: 60px;
    }

    /* toc entry bullets */
    #toc_panel[data-bullets="true"] .toc_link[data-level]:before {
      position: absolute;
      left: -15px;
      top: -1px;
      font-size: 1.5em;
    }

    /* toc entry, level 2 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="2"]:before {
      content: "\2022";
    }

    /* toc entry, level 3 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="3"]:before {
      content: "\25AB";
    }

    /* toc entry, level 4 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="4"]:before {
      content: "-";
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* push <body> ("page") element down to make room for toc icon */
    .toc_body_nudge {
      padding-top: 60px;
    }

    /* toc icon when panel closed and not hovered */
    #toc_panel[data-open="false"]:not(:hover) {
      background: rgba(255, 255, 255, 0.75);
    }
  }

  /* always hide toc panel on print */
  @media only print {
    #toc_panel {
      display: none;
    }
  }
</style>
<!-- 
  Tooltips Plugin

  Makes it such that when the user hovers or focuses a link to a citation or
  figure, a tooltip appears with a preview of the reference content, along with
  arrows to navigate between instances of the same reference in the document.
-->

<script type="module">
  // whether user must click off to close tooltip instead of just un-hovering
  const clickClose = "false";
  // delay (in ms) between opening and closing tooltip
  const delay = "100";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach hover and focus listeners to link
      link.addEventListener("mouseover", onLinkHover);
      link.addEventListener("mouseleave", onLinkUnhover);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("touchend", onLinkTouch);
    }

    // attach mouse, key, and resize listeners to window
    window.addEventListener("mousedown", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("keyup", onKeyUp);
    window.addEventListener("resize", onResize);
  }

  // when link is hovered
  function onLinkHover() {
    // function to open tooltip
    const delayOpenTooltip = function () {
      openTooltip(this);
    }.bind(this);

    // run open function after delay
    this.openTooltipTimer = window.setTimeout(delayOpenTooltip, delay);
  }

  // when mouse leaves link
  function onLinkUnhover() {
    // cancel opening tooltip
    window.clearTimeout(this.openTooltipTimer);

    // don't close on unhover if option specifies
    if (clickClose === "true") return;

    // function to close tooltip
    const delayCloseTooltip = function () {
      // if tooltip open and if mouse isn't over tooltip, close
      const tooltip = document.getElementById("tooltip");
      if (tooltip && !tooltip.matches(":hover")) closeTooltip();
    };

    // run close function after delay
    this.closeTooltipTimer = window.setTimeout(delayCloseTooltip, delay);
  }

  // when link is focused (tabbed to)
  function onLinkFocus(event) {
    openTooltip(this);
  }

  // when link is touched on touch screen
  function onLinkTouch(event) {
    // attempt to force hover state on first tap always, and trigger
    // regular link click (and navigation) on second tap
    if (event.target === document.activeElement) event.target.click();
    else {
      document.activeElement.blur();
      event.target.focus();
    }
    if (event.cancelable) event.preventDefault();
    event.stopPropagation();
    return false;
  }

  // when mouse is clicked anywhere in window
  function onClick(event) {
    closeTooltip();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("tooltip_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("tooltip_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeTooltip();
        break;
    }
  }

  // when window is resized or zoomed
  function onResize() {
    closeTooltip();
  }

  // get all links of types we wish to handle
  function getLinks() {
    const queries = [];
    // exclude buttons, anchor links, toc links, etc
    const exclude =
      ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    queries.push('a[href^="#ref-"]' + exclude); // citation links
    queries.push('a[href^="#fig:"]' + exclude); // figure links
    const query = queries.join(", ");
    return document.querySelectorAll(query);
  }

  // get links with same target, get index of link in set, get total
  // same links
  function getSameLinks(link) {
    const sameLinks = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        sameLinks.push(otherLink);
    }

    return {
      elements: sameLinks,
      index: sameLinks.indexOf(link),
      total: sameLinks.length,
    };
  }

  // open tooltip
  function openTooltip(link) {
    // delete tooltip if it exists, start fresh
    closeTooltip();

    // make tooltip element
    const tooltip = makeTooltip(link);

    // if source couldn't be found and tooltip not made, exit
    if (!tooltip) return;

    // make navbar elements
    const navBar = makeNavBar(link);
    if (navBar) tooltip.firstElementChild.appendChild(navBar);

    // attach tooltip to page
    document.body.appendChild(tooltip);

    // position tooltip
    const position = function () {
      positionTooltip(link);
    };
    position();

    // if tooltip contains images, position again after they've loaded
    const imgs = tooltip.querySelectorAll("img");
    for (const img of imgs) img.addEventListener("load", position);
  }

  // close (delete) tooltip
  function closeTooltip() {
    const tooltip = document.getElementById("tooltip");
    if (tooltip) tooltip.remove();
  }

  // make tooltip
  function makeTooltip(link) {
    // get target element that link points to
    const source = getSource(link);

    // if source can't be found, exit
    if (!source) return;

    // create new tooltip
    const tooltip = document.createElement("div");
    tooltip.id = "tooltip";
    const tooltipContent = document.createElement("div");
    tooltipContent.id = "tooltip_content";
    tooltip.appendChild(tooltipContent);

    // make copy of source node and put in tooltip
    const sourceCopy = makeCopy(source);
    tooltipContent.appendChild(sourceCopy);

    // attach mouse event listeners
    tooltip.addEventListener("click", onTooltipClick);
    tooltip.addEventListener("mousedown", onTooltipClick);
    tooltip.addEventListener("touchstart", onTooltipClick);
    tooltip.addEventListener("mouseleave", onTooltipUnhover);

    // (for interaction with lightbox plugin)
    // transfer click on tooltip copied img to original img
    const sourceImg = source.querySelector("img");
    const sourceCopyImg = sourceCopy.querySelector("img");
    if (sourceImg && sourceCopyImg) {
      const clickImg = function () {
        sourceImg.click();
        closeTooltip();
      };
      sourceCopyImg.addEventListener("click", clickImg);
    }

    return tooltip;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
      "class",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // when tooltip is clicked
  function onTooltipClick(event) {
    // when user clicks on tooltip, stop click from transferring
    // outside of tooltip (eg, click off to close tooltip, or eg click
    // off to unhighlight same refs)
    event.stopPropagation();
  }

  // when tooltip is unhovered
  function onTooltipUnhover(event) {
    if (clickClose === "true") return;

    // make sure new mouse/touch/focus no longer over tooltip or any
    // element within it
    const tooltip = document.getElementById("tooltip");
    if (!tooltip) return;
    if (this.contains(event.relatedTarget)) return;

    closeTooltip();
  }

  // make nav bar to go betwen prev/next instances of same reference
  function makeNavBar(link) {
    // find other links to the same source
    const sameLinks = getSameLinks(link);

    // don't show nav bar when singular reference
    if (sameLinks.total <= 1) return;

    // find prev/next links with same target
    const prevLink = getPrevLink(link, sameLinks);
    const nextLink = getNextLink(link, sameLinks);

    // create nav bar
    const navBar = document.createElement("div");
    navBar.id = "tooltip_nav_bar";
    const text = sameLinks.index + 1 + " of " + sameLinks.total;

    // create nav bar prev/next buttons
    const prevButton = document.createElement("button");
    const nextButton = document.createElement("button");
    prevButton.id = "tooltip_prev_button";
    nextButton.id = "tooltip_next_button";
    prevButton.title =
      "Jump to the previous occurence of this item in the document [←]";
    nextButton.title =
      "Jump to the next occurence of this item in the document [→]";
    prevButton.classList.add("icon_button");
    nextButton.classList.add("icon_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;
    nextButton.innerHTML =
      document.querySelector(".icon_caret_right").innerHTML;
    navBar.appendChild(prevButton);
    navBar.appendChild(document.createTextNode(text));
    navBar.appendChild(nextButton);

    // attach click listeners to buttons
    prevButton.addEventListener("click", function () {
      onPrevNextClick(link, prevLink);
    });
    nextButton.addEventListener("click", function () {
      onPrevNextClick(link, nextLink);
    });

    return navBar;
  }

  // get previous link with same target
  function getPrevLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if < 1
    let index;
    if (sameLinks.index - 1 >= 0) index = sameLinks.index - 1;
    else index = sameLinks.total - 1;
    return sameLinks.elements[index];
  }

  // get next link with same target
  function getNextLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if > total
    let index;
    if (sameLinks.index + 1 <= sameLinks.total - 1) index = sameLinks.index + 1;
    else index = 0;
    return sameLinks.elements[index];
  }

  // get element that is target of link or url hash
  function getSource(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector('[id="' + id + '"]');
    if (!target) return;

    // if ref or figure, modify target to get expected element
    if (id.indexOf("ref-") === 0) target = target.querySelector(":nth-child(2)");
    else if (id.indexOf("fig:") === 0) target = target.querySelector("figure");

    return target;
  }

  // when prev/next arrow button is clicked
  function onPrevNextClick(link, prevNextLink) {
    if (link && prevNextLink)
      goToElement(prevNextLink, window.innerHeight * 0.5);
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);
    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // determine position to place tooltip based on link position in
  // viewport and tooltip size
  function positionTooltip(link, left, top) {
    const tooltipElement = document.getElementById("tooltip");
    if (!tooltipElement) return;

    // get convenient vars for position/dimensions of
    // link/tooltip/page/view
    link = getRectInPage(link);
    const tooltip = getRectInPage(tooltipElement);
    const view = getRectInPage();

    // horizontal positioning
    if (left)
      // use explicit value
      left = left;
    else if (link.left + tooltip.width < view.right)
      // fit tooltip to right of link
      left = link.left;
    else if (link.right - tooltip.width > view.left)
      // fit tooltip to left of link
      left = link.right - tooltip.width;
    // center tooltip in view
    else left = (view.right - view.left) / 2 - tooltip.width / 2;

    // vertical positioning
    if (top)
      // use explicit value
      top = top;
    else if (link.top - tooltip.height > view.top)
      // fit tooltip above link
      top = link.top - tooltip.height;
    else if (link.bottom + tooltip.height < view.bottom)
      // fit tooltip below link
      top = link.bottom;
    else {
      // center tooltip in view
      top = view.top + view.height / 2 - tooltip.height / 2;
      // nudge off of link to left/right if possible
      if (link.right + tooltip.width < view.right) left = link.right;
      else if (link.left - tooltip.width > view.left)
        left = link.left - tooltip.width;
    }

    tooltipElement.style.left = left + "px";
    tooltipElement.style.top = top + "px";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* tooltip container */
    #tooltip {
      position: absolute;
      width: 50%;
      min-width: 240px;
      max-width: 75%;
      z-index: 1;
    }

    /* tooltip content */
    #tooltip_content {
      margin-bottom: 5px;
      padding: 20px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
      overflow-wrap: break-word;
    }

    /* tooltip copy of paragraphs and figures */
    #tooltip_content > p,
    #tooltip_content > figure {
      margin: 0;
      max-height: 320px;
      overflow-y: auto;
    }

    /* tooltip copy of <img> */
    #tooltip_content > figure > img,
    #tooltip_content > figure > svg {
      max-height: 260px;
    }

    /* navigation bar */
    #tooltip_nav_bar {
      margin-top: 10px;
      text-align: center;
    }

    /* navigation bar previous/next buton */
    #tooltip_nav_bar > .icon_button {
      position: relative;
      top: 3px;
    }

    /* navigation bar previous button */
    #tooltip_nav_bar > .icon_button:first-of-type {
      margin-right: 5px;
    }

    /* navigation bar next button */
    #tooltip_nav_bar > .icon_button:last-of-type {
      margin-left: 5px;
    }
  }

  /* always hide tooltip on print */
  @media only print {
    #tooltip {
      display: none;
    }
  }
</style>
<!--
  Analytics Plugin (third-party) 
  
  Copy and paste code from Google Analytics or similar service here.
-->
<!-- 
  Annotations Plugin

  Allows public annotation of the  manuscript. See https://web.hypothes.is/.
-->

<script type="module">
  // configuration
  window.hypothesisConfig = function () {
    return {
      branding: {
        accentColor: "#2196f3",
        appBackgroundColor: "#f8f8f8",
        ctaBackgroundColor: "#f8f8f8",
        ctaTextColor: "#000000",
        selectionFontFamily: "Open Sans, Helvetica, sans serif",
        annotationFontFamily: "Open Sans, Helvetica, sans serif",
      },
    };
  };

  // hypothesis client script
  const embed = "https://hypothes.is/embed.js";
  // hypothesis annotation count query url
  const query = "https://api.hypothes.is/api/search?limit=0&url=";

  // start script
  function start() {
    const button = makeButton();
    document.body.insertBefore(button, document.body.firstChild);
    insertCount(button);
  }

  // make button
  function makeButton() {
    // create button
    const button = document.createElement("button");
    button.id = "hypothesis_button";
    button.innerHTML = document.querySelector(".icon_hypothesis").innerHTML;
    button.title = "Hypothesis annotations";
    button.classList.add("icon_button");

    function onClick(event) {
      onButtonClick(event, button);
    }

    // attach click listeners
    button.addEventListener("click", onClick);

    return button;
  }

  // insert annotations count
  async function insertCount(button) {
    // get annotation count from Hypothesis based on url
    let count = "-";
    try {
      const canonical = document.querySelector('link[rel="canonical"]');
      const location = window.location;
      const url = encodeURIComponent((canonical || location).href);
      const response = await fetch(query + url);
      const json = await response.json();
      count = json.total || "-";
    } catch (error) {
      console.log(error);
    }

    // put count into button
    const counter = document.createElement("span");
    counter.id = "hypothesis_count";
    counter.innerHTML = count;
    button.title = "View " + count + " Hypothesis annotations";
    button.append(counter);
  }

  // when button is clicked
  function onButtonClick(event, button) {
    const script = document.createElement("script");
    script.src = embed;
    document.body.append(script);
    button.remove();
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- hypothesis icon -->

<template class="icon_hypothesis">
  <!-- modified from: https://simpleicons.org/icons/hypothesis.svg / https://git.io/Jf1VB -->
  <svg width="16" height="16" viewBox="0 0 24 24" tabindex="-1">
    <path
      fill="currentColor"
      d="M3.43 0C2.5 0 1.72 .768 1.72 1.72V18.86C1.72 19.8 2.5 20.57 3.43 20.57H9.38L12 24L14.62 20.57H20.57C21.5 20.57 22.29 19.8 22.29 18.86V1.72C22.29 .77 21.5 0 20.57 0H3.43M5.14 3.43H7.72V9.43S8.58 7.72 10.28 7.72C12 7.72 13.74 8.57 13.74 11.24V17.14H11.16V12C11.16 10.61 10.28 10.07 9.43 10.29C8.57 10.5 7.72 11.41 7.72 13.29V17.14H5.14V3.43M18 13.72C18.95 13.72 19.72 14.5 19.72 15.42A1.71 1.71 0 0 1 18 17.13A1.71 1.71 0 0 1 16.29 15.42C16.29 14.5 17.05 13.71 18 13.71Z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  /* hypothesis activation button */
  #hypothesis_button {
    box-sizing: border-box;
    position: fixed;
    top: 0;
    right: 0;
    width: 60px;
    height: 60px;
    background: #ffffff;
    border-radius: 0;
    border-left: solid 1px #bdbdbd;
    border-bottom: solid 1px #bdbdbd;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
    z-index: 2;
  }

  /* hypothesis button svg */
  #hypothesis_button > svg {
    position: relative;
    top: -4px;
  }

  /* hypothesis annotation count */
  #hypothesis_count {
    position: absolute;
    left: 0;
    right: 0;
    bottom: 5px;
  }

  /* side panel */
  .annotator-frame {
    width: 280px !important;
  }

  /* match highlight color to rest of theme */
  .annotator-highlights-always-on .annotator-hl {
    background-color: #ffeb3b !important;
  }

  /* match focused color to rest of theme */
  .annotator-hl.annotator-hl-focused {
    background-color: #ff8a65 !important;
  }

  /* match bucket bar color to rest of theme */
  .annotator-bucket-bar {
    background: #f5f5f5 !important;
  }

  /* always hide button, toolbar, and tooltip on print */
  @media only print {
    #hypothesis_button {
      display: none;
    }

    .annotator-frame {
      display: none !important;
    }

    hypothesis-adder {
      display: none !important;
    }
  }
</style>
<!-- 
  Mathjax Plugin (third-party) 

  Allows the proper rendering of math/equations written in LaTeX.
  See https://www.mathjax.org/.
-->

<script type="text/x-mathjax-config">
  // configuration
  MathJax.Hub.Config({
    "CommonHTML": { linebreaks: { automatic: true } },
    "HTML-CSS": { linebreaks: { automatic: true } },
    "SVG": { linebreaks: { automatic: true } },
    "fast-preview": { disabled: true }
  });
</script>

<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A=="
  crossorigin="anonymous"
></script>

<style>
  /* mathjax containers */
  .math.display > span:not(.MathJax_Preview) {
    /* turn inline element (no dimensions) into block (allows fixed width and thus scrolling) */
    display: flex !important;
    overflow-x: auto !important;
    overflow-y: hidden !important;
    justify-content: center;
    align-items: center;
    margin: 0 !important;
  }

  /* right click menu */
  .MathJax_Menu {
    border-radius: 5px !important;
    border: solid 1px #bdbdbd !important;
    box-shadow: none !important;
  }

  /* equation auto-number */
  span[id^="eq:"] > span.math.display + span {
    font-weight: 600;
  }

  /* equation */
  span[id^="eq:"] > span.math.display > span {
    /* nudge to make room for equation auto-number and anchor */
    margin-right: 60px !important;
  }
</style>
</body>
</html>
